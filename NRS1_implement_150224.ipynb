{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\src'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:80% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper.mmax_reader import *\n",
    "from helper.etri_reader import *\n",
    "from helper.DepTreeForEuroParl import ReadCorpusFast as ReadConll\n",
    "from helper.misc import id2int, index_in_sent\n",
    "from pprint import pprint\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from IPython.display import display\n",
    "pd.options.display.max_columns = None\n",
    "import os\n",
    "from helper.transcriber import transcribe\n",
    "\n",
    "parsed_sentences = [None] + list(read_corpus(\"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\data\\\\etri_corpus\\\\one_sent_per_line.txt.dep_parse.out.mod_orig_add_na_2018.txt\"))\n",
    "# 원래 작업했던 txt 파일에 문제가 생겨서 ...2018년 원본 파일로 다시 돌려봄! 070822 이것만 작동, 이걸로 해야할 듯 \n",
    "\n",
    "annotation_dir = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\data\\\\finished_annotation\\\\\"\n",
    "annotation_dir_test = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\data\\\\finished_annotation_test\\\\\"\n",
    "project_dir = \"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\data\\\\finished_annotation\\\\etri{}\" # template for paths to project directories\n",
    "subj_predicates = pickle.load(open(\"C:\\\\Users\\\\ArumPark\\\\Desktop\\\\fsimonjetz-korean_anaphora-57be778a8ba3\\\\data\\\\subj_pred_words.pkl\", mode=\"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include all projects\n",
    "include_projects = [] # project numbers to include\n",
    "\n",
    "for f in os.listdir(annotation_dir): #test로 check하기 # 다시 원래 dir로\n",
    "    \n",
    "    if f.startswith(\".\"):\n",
    "        continue\n",
    "    \n",
    "    include_projects.append(int(f[4:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions\n",
    "import re\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_parse_info(markable):\n",
    "\n",
    "    #assert len(markable) == 1, \"item '{}' is longer than one token\".format(str(markable))\n",
    "    \n",
    "    #if len(markable) != 1:\n",
    "        #print(\"strange tokenization at\", markable.id)\n",
    "    \n",
    "    hostsent = markable.host(\"sentence\")\n",
    "    parse_tokens = parsed_sentences[int(hostsent.sentence_id)].tokens\n",
    "    offset = id2int(hostsent.span[0])\n",
    "    return [parse_tokens[id2int(i)-offset] for i in markable.span]\n",
    "\n",
    "def sent_morphs(markable):\n",
    "\n",
    "    hostsent = markable.host(\"sentence\")\n",
    "    parse_tokens = parsed_sentences[int(hostsent.sentence_id)].tokens\n",
    "    for t in parse_tokens:\n",
    "        sent_morphs = t.morphs\n",
    "    \n",
    "    return sent_morphs\n",
    "\n",
    "def sent_dep(markable):\n",
    "\n",
    "    hostsent = markable.host(\"sentence\")\n",
    "    parse_tokens = parsed_sentences[int(hostsent.sentence_id)].tokens\n",
    "    for t in parse_tokens:\n",
    "        sent_dep = t.dep\n",
    "    \n",
    "    return sent_dep\n",
    "    \n",
    "def get_base_morpheme(markable):\n",
    "    \n",
    "    assert len(markable) == 1, \"item '{}' is longer than one token\".format(str(markable))\n",
    "    \n",
    "    parse_info = get_parse_info(markable)[0]\n",
    "    \n",
    "    return parse_info.morphs[0]\n",
    "\n",
    "\n",
    "def has_participant_switch(sentence):\n",
    "    # return True if the previous sentence is by a different speaker\n",
    "    \n",
    "    if sentence.host(\"Utterance\").span[0] == sentence.span[0]:\n",
    "        return 1\n",
    "    \n",
    "    return 0\n",
    "    \n",
    "\n",
    "def re_morphmatch(morphlist, **kwargs):\n",
    "    # test if there is any morph for which all kwarg constraints\n",
    "    # hold; kwargs look like featurename=/regex/\n",
    "    \n",
    "    if not kwargs:\n",
    "        return False\n",
    "    \n",
    "    for m in morphlist:\n",
    "        \n",
    "        matched = set()\n",
    "        \n",
    "        for feature, regex in kwargs.items():\n",
    "            \n",
    "            #print(\"testing if\", regex, \"is compatible with\", feature, \"whose value is\",  m.get(feature, \"\"))\n",
    "            is_match = re.search(regex, m.get(feature, \"\"))\n",
    "            matched.add(bool(is_match))\n",
    "            \n",
    "        \n",
    "        if all(matched):\n",
    "            return 1\n",
    "        \n",
    "    return 0\n",
    "\n",
    "\n",
    "def sentence_noun_map(project):\n",
    "    # map {sentence_id:candidates}\n",
    "    # return a dictionary that gives a list of noun candidates for a sentence id\n",
    "    sent_nouns = defaultdict(set)\n",
    "    \n",
    "    for n in project.markables[\"Noun\"]:\n",
    "        sent_nouns[n.host(\"sentence\").id].add(n)\n",
    "        \n",
    "    return sent_nouns\n",
    "\n",
    "def get_candidates(sent, sent_map, n=3):\n",
    "    # extract candidates from up to n sentences to the left\n",
    "    \n",
    "    candidates = list()\n",
    "    \n",
    "    for i in range(n+1):\n",
    "        \n",
    "        for cand in sent_map.get(\"markable_{}\".format(id2int(sent.id)-i), []):\n",
    "            # TO DO: make sure that the sent and the cand are from the same dialog\n",
    "            if sent.host(\"Dialog\").id == cand.host(\"Dialog\").id:\n",
    "                candidates.append((i,cand))\n",
    "        \n",
    "    return candidates    \n",
    "\n",
    "\n",
    "def get_is_ana(zero):\n",
    "    \n",
    "    if zero.anaphora_type == \"extra\":\n",
    "        return 0\n",
    "    \n",
    "    elif zero.anaphora_type in [\"inter\", \"intra\"]:\n",
    "        return 1\n",
    "    \n",
    "    else:\n",
    "        return -1\n",
    "    \n",
    "    \n",
    "def is_head(cand_parse_info):\n",
    "    \n",
    "    if cand_parse_info.dep == \"modi\":\n",
    "        # should cover -와/과, -(이)랑, -(이)나, -하고\n",
    "        if re_morphmatch(cand_parse_info.morphs, pos=\"P\", subpos=\"coor\", morphpos=\"FJ\"):\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "    \n",
    "    else:\n",
    "        return 1\n",
    "    \n",
    "def connective_true(morphs):\n",
    "    if re_morphmatch(morphs, pos=\"E\", subpos=\"oend\", morphpos=\"EC\"):\n",
    "        return 1\n",
    "    else:\n",
    "        return 0    \n",
    "    \n",
    "def get_connective_pred(morphs):\n",
    "    \n",
    "    if re_morphmatch(morphs, morph=\"^(고|구|거나|고서|고나서|으?면서도?|으?며|으?려고|으?려|으?러|으?려면|느라고?|는바람에|기위해서?|[아어]서|서|어|아|라|어서그런지|다가|어?다)$\", subpos=\"^(oend|cend)$\"):\n",
    "        return \"class_A\"  \n",
    "    elif re_morphmatch(morphs, morph=\"^(길래|자|자마자)$\"):\n",
    "        return \"class_B\"\n",
    "#     elif re_morphmatch(morphs, morph=\"^[았었]더니$\"):\n",
    "#         return \"class_C\"\n",
    "    \n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_connective_pred_str(morphs):\n",
    "    \n",
    "    if re_morphmatch(morphs, morph=\"^(고|구|거나|고서|고나서|으?면서도?|으?며|으?려고|으?려|으?러|으?려면|느라고?|는바람에|기위해서?|[아어]서|서|어|아|라|어서그런지|다가|어?다)$\", subpos=\"^(oend|cend)$\"):\n",
    "        return morphs[1].get('morph')\n",
    "    else:\n",
    "        return \"N/A\"\n",
    "\n",
    "def get_referents(zero):\n",
    "    \n",
    "    #\"1sh+(저)\", \"2sh+(당신)\", \"1sh-(나)\", \"2sh-(너)\", \"1ph+(저희)\", \"1ph-(우리)\", \"3s(그/그녀/그것)\", \n",
    "    # \"3p(그들/그녀들/그것들)\", \"2ph+(당신들)\", \"2ph-(너희들)\", \"others\"\n",
    "    if zero.anaphora_type == \"extra\":\n",
    "        \n",
    "        if zero.referent_type == \"1sh+\":\n",
    "            return \"1sh+\"\n",
    "\n",
    "        elif zero.referent_type == \"2sh+\":\n",
    "            return \"2sh+\"\n",
    "\n",
    "        elif zero.referent_type == \"1sh-\":\n",
    "            return \"1sh-\"\n",
    "\n",
    "        elif zero.referent_type == \"2sh-\":\n",
    "            return \"2sh-\"\n",
    "\n",
    "        elif zero.referent_type == \"1ph+\":\n",
    "            return \"1ph+\"\n",
    "\n",
    "        elif zero.referent_type == \"1ph-\":\n",
    "            return \"1ph-\"\n",
    "\n",
    "        elif zero.referent_type == \"3s\":\n",
    "            return \"3s\"\n",
    "\n",
    "        elif zero.referent_type == \"3p\":\n",
    "            return \"3p\"\n",
    "\n",
    "        elif zero.referent_type == \"2ph+\":\n",
    "            return \"2ph+\"\n",
    "\n",
    "        elif zero.referent_type == \"2ph-\":\n",
    "            return \"2ph-\"\n",
    "\n",
    "        elif zero.referent_type == \"other\":\n",
    "            return zero.other_ref_comment\n",
    "\n",
    "        else: \n",
    "            return \"N/A\"\n",
    "        \n",
    "#extra에 대한 것만 referent_type이 있어서 그런거 아닐까?\n",
    "# extra를 조건으로 넣어보기 /  성공함\n",
    "# 나중에 같은지 아닌지 조건 넣고\n",
    "# 복문 규칙까지 넣어야 하는지 고민해 보기 (복문규칙은 이미 완성된 dataframe을 활용해야 하는 건가?)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "code_folding": [
     273
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.00%\r"
     ]
    }
   ],
   "source": [
    "# Actual data generation\n",
    "\n",
    "datapoints = list()\n",
    "maxn = len(include_projects)\n",
    "print(\"{:.2f}%\\r\".format(0/maxn), end=\"\")\n",
    "\n",
    "for i,pnum in enumerate(include_projects, start=1): \n",
    "    \n",
    "    filepath = os.path.join(project_dir.format(pnum), \"zero_pronouns.mmax\")\n",
    "    project = MMAXReader(filepath, unique_hosts=[\"sentence\", \"Dialog\", \"Utterance\"])\n",
    "    cand_map = sentence_noun_map(project)\n",
    "    \n",
    "    for zero in project.markables[\"Zero\"]: \n",
    "\n",
    "        zp_sentence = zero.host(\"sentence\")\n",
    "        \n",
    "        parse_tokens = parsed_sentences[int(zp_sentence.sentence_id)].tokens\n",
    "        ## ['이것을', '작성하아주시겠습니까', '?']\n",
    "    \n",
    "        sent_dep = list()\n",
    "        sent_morphs = list()\n",
    "        sent_head = list()\n",
    "        \n",
    "        for t in parse_tokens: \n",
    "            sent_dep.append(t.dep) \n",
    "            sent_morphs.append(t.morphs) \n",
    "            sent_head.append(t.head)\n",
    "        \n",
    "        candidates = get_candidates(zp_sentence, cand_map)\n",
    "        antecedents = set(a.id for a in zero.resolve(\"antecedent\"))\n",
    "        antecedents_list = [a for a in zero.resolve(\"antecedent\")]  \n",
    "        antecedents_str = ''.join(str(s) for s in antecedents_list)\n",
    "    \n",
    "        \n",
    "        zp_markable_ID = zero.id\n",
    "        \n",
    "        morphs = get_parse_info(zero)[0].morphs\n",
    "        sent_morphs = get_parse_info(zp_sentence)[0].morphs\n",
    "        \n",
    "        base_morpheme = morphs[0]\n",
    "        \n",
    "        # base_morpheme_second = get_connective_pred_str(morphs) \n",
    "                \n",
    "        zero_parse_info = get_parse_info(zero)[0]\n",
    "        \n",
    "        datapoint = dict()\n",
    "            \n",
    "        datapoint[\"etri_sent\"] = zp_sentence.sentence_id\n",
    "        datapoint[\"sent\"] = zp_sentence\n",
    "        datapoint[\"verb\"] = str(zero)\n",
    "        datapoint[\"sent_dep\"] = sent_dep\n",
    "        datapoint[\"sent_morphs\"] = sent_morphs\n",
    "        datapoint[\"connective\"] = get_connective_pred(morphs)\n",
    "        datapoint[\"base_morpheme\"] = base_morpheme.get(\"morph\", \"N/A\") # added\n",
    "        datapoint[\"base_morpheme_second\"] = get_connective_pred_str(morphs) # added\n",
    "        datapoint[\"connective_true\"] = connective_true(morphs)\n",
    "        datapoint[\"verb_roman\"] = transcribe(datapoint[\"verb\"], \"-\")\n",
    "        datapoint[\"zp_markable_ID\"] = zp_markable_ID\n",
    "        datapoint[\"zp_speaker\"] = zero.host(\"Utterance\").speaker\n",
    "        datapoint[\"tense_zp\"] = base_morpheme.get(\"tense\", \"N/A\")\n",
    "        datapoint[\"zp_mood\"] = base_morpheme.get(\"mood\", \"N/A\")\n",
    "        datapoint[\"zp_hono\"] = base_morpheme.get(\"hono\", \"N/A\")\n",
    "        datapoint[\"zero_type\"] = zero.anaphora_type\n",
    "        datapoint[\"verb_syn\"] = zero_parse_info.dep\n",
    "        datapoint[\"verb_head\"] = zero_parse_info.head\n",
    "        datapoint[\"zero_referents\"] = get_referents(zero)\n",
    "        datapoint[\"antecedent\"] = antecedents_str\n",
    "        datapoint[\"applied_rule\"] = \"\"\n",
    "        datapoint[\"pos\"] = base_morpheme.get(\"pos\", \"N/A\")\n",
    "        # objective pred\n",
    "#         datapoint[\"obj_pred\"] = base_morpheme.get(\"morph\", \"\") + \"다\" in subj_predicates[\"D\"]\n",
    "#         datapoint[\"obj_pred\"] = int(datapoint[\"obj_pred\"])\n",
    "\n",
    "            \n",
    "        if datapoint[\"zp_mood\"] == \"impe\":\n",
    "            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "            datapoint[\"group\"] = \"mood derivable\"\n",
    "            datapoint[\"rule\"] = \"impe\"\n",
    "            datapoint[\"rule_order\"] = \"1\"\n",
    "            datapoint[\"applied_rule\"] += \"impe was applied, \"\n",
    "        elif datapoint[\"zp_mood\"] == \"sugg\":\n",
    "            datapoint[\"subject_by_rule\"] = \"1ph-\"\n",
    "            datapoint[\"group\"] = \"mood derivable\"\n",
    "            datapoint[\"rule\"] = \"sugg\"\n",
    "            datapoint[\"rule_order\"] = \"2\"\n",
    "            datapoint[\"applied_rule\"] += \"sugg was applied, \"\n",
    "        elif re_morphmatch(morphs, morph=\"^(으?세요|으?세여|으?십시오|으?십시요)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "            datapoint[\"group\"] = \"mood derivable\"\n",
    "            datapoint[\"rule\"] = \"impe\"\n",
    "            datapoint[\"rule_order\"] = \"3\"\n",
    "            datapoint[\"applied_rule\"] += \"impe(decl) was applied, \"\n",
    "#         elif re_morphmatch(morphs, morph=\"^(ㅂ시다|자)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#             datapoint[\"subject_by_rule\"] = \"1ph-\"\n",
    "#             datapoint[\"group\"] = \"mood derivable\"\n",
    "#             datapoint[\"rule\"] = \"sugg\"\n",
    "#             datapoint[\"applied_rule\"] += \"sugg(decl) was applied, \"\n",
    "#             # nichts getroffen\n",
    "        elif re_morphmatch(morphs, morph=\"^(는게좋|면좋|는게낫|는것이좋|는편이좋)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "            datapoint[\"group\"] = \"suggestory formula\"\n",
    "            datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "            datapoint[\"rule_order\"] = \"4\"\n",
    "            datapoint[\"applied_rule\"] += \"suggestory(plus) was applied, \"\n",
    "        elif re_morphmatch(morphs, morph=\"^(는게좋|면좋|는게낫|는것이좋|는편이좋)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "            datapoint[\"group\"] = \"suggestory formula\"\n",
    "            datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "            datapoint[\"rule_order\"] = \"5\"\n",
    "            datapoint[\"applied_rule\"] += \"suggestory was applied, \"\n",
    "            \n",
    "        elif re_morphmatch(morphs, morph=\"^(을것같|ㄹ것같|는것같|ㄴ것같|은듯하)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "            datapoint[\"group\"] = \"guess\"\n",
    "            datapoint[\"rule\"] = \"guess periphrastic construction\"\n",
    "            datapoint[\"rule_order\"] = \"6\"\n",
    "            datapoint[\"applied_rule\"] += \"guess(plus) was applied, \"\n",
    "            \n",
    "#         elif datapoint[\"obj_pred\"] = base_morpheme.get(\"morph\", \"\") + \"다\" in subj_predicates[\"E\"]:\n",
    "#             datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#             datapoint[\"group\"] = \"exclude\"\n",
    "#             datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#             datapoint[\"applied_rule\"] += \"exclude_predicate1 was applied, \"\n",
    "            \n",
    "        else: \n",
    "            datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "            \n",
    "        # 2,601 getroffen (seems ok. 11.94% of 17,266)\n",
    "        \n",
    "# clau related rules      \n",
    "\n",
    "        try_str_list = [\"보니까\", \"보니깐\", \"보니까는\"]\n",
    "        performative_head_list = [\"부탁\", \"권유\"]\n",
    "        cognition_head_list = [\"예정\",\"생각\",\"기억\",\"확인\",\"깜박\",\"예상\",\"착각\",\"이해\",\"확실\",\"집중\",\"계획\",\"숙지\",\"오해\",\"파악\",\"동의\",\"사과\",\"깜빡\"]\n",
    "        sensation_head_list = [\"시음\",\"민감\",\"구경\",\"시원\"]\n",
    "        feeling_head_list = [\"실망\",\"공포\",\"영광\",\"장담\",\"한시름\",\"미안\",\"실례\",\"의아\",\"질색\",\"놀랍\",\"환영\"]\n",
    "        if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "            if re_morphmatch(morphs, morph=\"^(아?야하|어야하|아?야되|어야되|고계시|으?시|으셨)$\", pos=\"^(X|E)$\") and re_morphmatch(morphs, morph=\"^([ㄴ는]데|는데|는건데|은데|[ㄴ은는]건데)$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\" and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"7\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_neunde) was applied, \"\n",
    "                # 진짜 적용됐는지 확인하기\n",
    "            elif re_morphmatch(morphs, morph=\"^(으?시|아계시|고계시|[ㄹ을]줄알|가시|주무시|으셨)$\", pos=\"^(E|V)$\") and re_morphmatch(morphs, morph=\"^(려?면|으면|다보면)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\": \n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_myeon\"\n",
    "                datapoint[\"rule_order\"] = \"8\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_myeon) was applied, \"\n",
    "                # 진짜 적용됐는지 확인하기\n",
    "                #datapoint[\"rule_order\"] = \"\"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^었$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^으면$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_myeon\"\n",
    "                datapoint[\"rule_order\"] = \"9\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_myeon) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^주무시$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^고$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\": \n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_go\"\n",
    "                datapoint[\"rule_order\"] = \"10\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_go) was applied, \"\n",
    "                # nichts getroffen\n",
    "            elif re_morphmatch(morphs, morph=\"^(으?시|[아어]계시)$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^다가?$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_daga\"\n",
    "                datapoint[\"rule_order\"] = \"11\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_daga) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^으?면서$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_myeonseo\"\n",
    "                datapoint[\"rule_order\"] = \"12\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_myeonseo) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^어서$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_eoseo\"\n",
    "                datapoint[\"rule_order\"] = \"13\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_eoseo) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^ㄴ후에*$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_nhu\"\n",
    "                datapoint[\"rule_order\"] = \"14\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_nhu) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^기전에$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_gijeon\"\n",
    "                datapoint[\"rule_order\"] = \"15\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_gijeon) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^(ㄴ다음에*|ㄴ뒤에*|ㄴ뒤에도)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_ndaum\"\n",
    "                datapoint[\"rule_order\"] = \"16\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_ndaum) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^(ㄹ때|ㄹ때에?는|ㄹ때에?도)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_lttae\"\n",
    "                datapoint[\"rule_order\"] = \"17\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_lttae) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^다니$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_dani\"\n",
    "                datapoint[\"rule_order\"] = \"18\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_dani) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^어야지?$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_eoya\"\n",
    "                datapoint[\"rule_order\"] = \"19\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_eoya) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^더라도?$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_deolado\"\n",
    "                datapoint[\"rule_order\"] = \"20\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_deolado) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^으?시$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^ㄴ?다면$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_ndamyeon\"\n",
    "                datapoint[\"rule_order\"] = \"21\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_ndamyeo) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(아?야하|어야하|으?려하|[ㄹ을]터이|을테|[아어]보|[ㄹ을]것이)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(니|니까|니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\" and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\" # 1sh+ 였던걸 2sh+로 바꿈 270723\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_nikka\"\n",
    "                datapoint[\"rule_order\"] = \"22\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus_nikka) was applied, \"\n",
    "                # nichts getroffen\n",
    "            elif datapoint[\"zp_hono\"] == \"plus\" and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_plus\"\n",
    "                datapoint[\"rule_order\"] = \"23\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(plus, at the end) was applied, \"\n",
    "                \n",
    "            elif re_morphmatch(morphs, morph=\"^(ㄹ수있|을수있|ㄹ수없|ㄹ수도없|을수없)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^으?니$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_euni\"\n",
    "                datapoint[\"rule_order\"] = \"24\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(euni_possibility) was applied, \"\n",
    "                # coverage and precision low\n",
    "#             elif re_morphmatch(morphs, morph=\"^(걸리|멀쩡하|멀|나오)$\", pos=\"^(J|V)$\") and re_morphmatch(morphs, morph=\"^(는데)$\", pos=\"^E$\"):\n",
    "#                 datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                 datapoint[\"group\"] = \"exclude\"\n",
    "#                 datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                 datapoint[\"rule_order\"] = \"25\"\n",
    "#                 datapoint[\"applied_rule\"] += \"exclude before clau(neunde1) was applied,\"\n",
    "            elif re_morphmatch(morphs, morph=\"^(겠|았|었|아?야하|어야하|아?야되|어야되|ㄹ려고?하|으?려고?하|고자하|ㄹ까하|[아어]?드리|고싶|[ㄹ을]생각이|[ㄹ을]지도모르)$\", pos=\"^(E|X)$\") and re_morphmatch(morphs, morph=\"^([ㄴ는]데|는데|는건데|은데|[ㄴ은는]건데)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"26\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(neunde1) was applied, \"\n",
    "                # 진짜 적용됐는지 확인하기\n",
    "            elif re_morphmatch(morphs, morph=\"^([ㄹ을]건데|려는데)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"27\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(neunde2) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(고있|어서그러|던중이|고있는중이|ㄴ것같|은것같|은듯하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"28\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(neunde3) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(았었|았|었|었었)$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^는데$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"29\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(neunde4) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(은건데|는건데)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_neunde\"\n",
    "                datapoint[\"rule_order\"] = \"30\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(neunde5) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^고싶$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^지만$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_jiman\"\n",
    "                datapoint[\"rule_order\"] = \"31\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(jiman1) was applied, \"\n",
    "                # coverage low\n",
    "            elif re_morphmatch(morphs, morph=\"^.*보$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^았$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^지만$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_jiman\"\n",
    "                datapoint[\"rule_order\"] = \"32\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(jiman2) was applied, \"\n",
    "                # coverage low\n",
    "            elif re_morphmatch(morphs, morph=\"^고싶$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^으니까?$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_nikka\"\n",
    "                datapoint[\"rule_order\"] = \"33\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(nikka1) was applied, \"\n",
    "                # eins getroffen\n",
    "            elif re_morphmatch(morphs, morph=\"^려니?$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_nikka\"\n",
    "                datapoint[\"rule_order\"] = \"34\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(nikka2) was applied, \"\n",
    "                # eins getroffen\n",
    "            elif re_morphmatch(morphs, morph=\"^(아?야하|어야하|으?려하|[ㄹ을]터이|을테|[아어]보|[ㄹ을]것이)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(니|니까|니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_nikka\"\n",
    "                datapoint[\"rule_order\"] = \"35\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(nikka3) was applied, \"\n",
    "                \n",
    "            elif re_morphmatch(morphs, morph=\"^(아드리|어드리|드리|축하드리)$\", pos=\"^(X|V)$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_various\"\n",
    "                datapoint[\"rule_order\"] = \"36\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(various_deuli) was applied, \"                \n",
    "           \n",
    "            elif re_morphmatch(morphs, morph=\"^지말$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^고$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_go\"\n",
    "                datapoint[\"rule_order\"] = \"37\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(go_variant) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(ㄹ까보|게되|고싶)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(아|어|어서|아서)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_aseo\"\n",
    "                datapoint[\"rule_order\"] = \"38\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(aseo_variant) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(게되|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(다니|ㄴ다니)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_dani\"\n",
    "                datapoint[\"rule_order\"] = \"39\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(dani_variant) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^려고$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^보$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^(니|니까)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_euni\"\n",
    "                datapoint[\"rule_order\"] = \"40\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(euni_variant1) was applied, \"\n",
    "            elif [x for x in try_str_list if x in datapoint[\"verb\"]] and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_euni\"\n",
    "                datapoint[\"rule_order\"] = \"41\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(euni_variant2) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^고있$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(으니|으니까|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_euni\"\n",
    "                datapoint[\"rule_order\"] = \"42\"\n",
    "                datapoint[\"applied_rule\"] += \"clau(euni_variant3) was applied, \"\n",
    "                \n",
    "            elif re_morphmatch(morphs, morph=\"^(는줄알|ㄴ줄알|을줄알|ㄹ줄알|ㄹ줄아|잘알|알|아|알게되|잘아|모르|는줄모르|을줄모르|ㄹ줄모르|어야할지모르|는지모르|ㄴ지모르|을지모르|ㄹ지모르|을수있을지모르|알아들|궁금하|고집하)$\", pos=\"^(X|V|J)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"43\"\n",
    "                datapoint[\"applied_rule\"] += \"0.1 clau(cognition1) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(잊|잊어버리|믿|알아듣|외우|헷갈리)$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"44\"\n",
    "                datapoint[\"applied_rule\"] += \"0.2 clau(cognition2) was applied, \"\n",
    "                # nichts getroffen\n",
    "            elif [x for x in cognition_head_list if x in datapoint[\"verb\"]] and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"45\"\n",
    "                datapoint[\"applied_rule\"] += \"0.3 clau(cognition3) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(보|못보|살펴보|둘러보|보러오|듣|들어서|들|들어보|맛보|배고프|싱거우|간지럽)$\", pos=\"^(V|J)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"46\"\n",
    "                datapoint[\"applied_rule\"] += \"0.4 clau(cognition4) was applied, \"\n",
    "            elif [x for x in sensation_head_list if x in datapoint[\"verb\"]] and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"47\"\n",
    "                datapoint[\"applied_rule\"] += \"0.5 clau(cognition5) was applied, \"\n",
    "                # nichts getroffen\n",
    "            elif re_morphmatch(morphs, morph=\"^(춥|따뜻하|덥|후텁지근하|찝찝하|아프|쓰라리|만지|만져보|느끼)$\", pos=\"^(J|V)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"48\"\n",
    "                datapoint[\"applied_rule\"] += \"0.6 clau(cognition6) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(괜찮|고맙|불편하|피곤하|지루하|흥미롭|곤란하|불쾌하|거북하|불안하|힘들|힘드|기힘들|좋|어쩔수없|별수없)$\", pos=\"^(J|X)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"49\"\n",
    "                datapoint[\"applied_rule\"] += \"0.7 clau(cognition7) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(좋아하|부담되|기대되|마음에들|기대하|걱정되|걱정하|고민되|고민하|놀라|당황하|놀래|놀랍|행복하|미안하|죄송하|죄송|답답하|부담스럽|의아하|질색이)$\", pos=\"^(V|N)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"50\"\n",
    "                datapoint[\"applied_rule\"] += \"0.8 clau(cognition8) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(후회하|감사하|감사드리|감탄하|횡재하|들뜨|떨리|만족하|만족스럽|망설여지|망설이|싫|싫어하)$\", pos=\"^(V|J)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"51\"\n",
    "                datapoint[\"applied_rule\"] += \"0.9 clau(cognition9) was applied, \"\n",
    "                # nichts getroffen\n",
    "            elif [x for x in feeling_head_list if x in datapoint[\"verb\"]] and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"52\"\n",
    "                datapoint[\"applied_rule\"] += \"0.10 clau(cognition10) was applied, \"\n",
    "            elif re_morphmatch(morphs, morph=\"^(익숙하|자부하|망설여지|망설이|아쉽|상관없|즐겁|재미있|재미없|즐거우|무섭|기쁘|행복하|난감하|반갑|환영하|신기하|당황스럽|막막하|안타깝|부럽)$\", pos=\"^(J|V)$\") and re_morphmatch(morphs, morph=\"^(ㄴ데|는데|은데|지만|다가|니까|으니까|니깐|으니깐)$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                datapoint[\"group\"] = \"clau\"\n",
    "                datapoint[\"rule\"] = \"clau_cognition\"\n",
    "                datapoint[\"rule_order\"] = \"53\"\n",
    "                datapoint[\"applied_rule\"] += \"0.11 clau(cognition11) was applied, \"\n",
    "                \n",
    "#             elif re_morphmatch(morphs, morph=\"^으?니$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "#                 datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                 datapoint[\"group\"] = \"clau\"\n",
    "#                 datapoint[\"rule\"] = \"clau_euni\"\n",
    "#                 datapoint[\"applied_rule\"] += \"clau(euni) was applied, \"\n",
    "\n",
    "            else: \n",
    "                datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "                \n",
    "                # 2,601 + 1,204 = 3,805 (22.1% of 17,266)\n",
    "            \n",
    "            indirect_str_list = [\"주시겠\",\"주실수\",\"주시면안\",\"시면안\",\"주겠\",\"줄수\",\"주면안\"]\n",
    "            want_str_list = [\"시겠습\", \"시겠어\", \"시려고\"]\n",
    "#             want_suffix_list = [\"을게요\",\"ㄹ게요\",\"게요\",\"을께요\",\"ㄹ께요\",\"ㄹ게\",\"ㄹ께\",\"ㄹ건데요\",\"ㄹ건가\",\"ㄹ거라\",\"을건데요\",\"는건가요\",\"은건가요\",\"ㄹ겁니까\"]\n",
    "\n",
    "            if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                if re_morphmatch(morphs, morph=\"^ㄴ적이?있|ㄴ적이?없$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"experience\"\n",
    "                    datapoint[\"rule\"] = \"experience periphrastic construction\"\n",
    "                    datapoint[\"rule_order\"] = \"54\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.1 experience(plus) was applied, \"\n",
    "                elif [x for x in indirect_str_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"indirect speech act\"\n",
    "                    datapoint[\"rule\"] = \"request\"\n",
    "                    datapoint[\"rule_order\"] = \"55\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.2 indirect(ques) was applied, \"\n",
    "                    # coverage and precision very high\n",
    "                elif [x for x in want_str_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"want statement\"\n",
    "                    datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                    datapoint[\"rule_order\"] = \"56\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.3. want(ques_plus) was applied, \"\n",
    "                    # coverage and precision very high\n",
    "#                 elif re_morphmatch(morphs, morph=\"^(는게좋|는편이좋|지않)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(을것이|을듯싶|는게좋|겠)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                     datapoint[\"group\"] = \"suggestory formula\"\n",
    "#                     datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "#                     datapoint[\"applied_rule\"] += \"5. suggestory(decl) was applied, \"\n",
    "                # 5는 나중에 적용, 정확도가 높지가 않음\n",
    "                elif re_morphmatch(morphs, morph=\"^으?면되$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(겠|지않)$\", pos=\"^(E|X)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"suggestory formula\"\n",
    "                    datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "                    datapoint[\"rule_order\"] = \"57\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.4 suggestory(plus) was applied, \"\n",
    "                elif re_morphmatch(morphs, morph=\"^(아?야하|[어여]야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^았$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"suggestory formula\"\n",
    "                    datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "                    datapoint[\"rule_order\"] = \"58\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.5 suggestory(plus) was applied, \"\n",
    "                    # coverage 진짜 낮음 (어야하 plus 관련된 obligation 다 위로 올라와도 됨)\n",
    "                elif re_morphmatch(morphs, morph=\"^(았|었)$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^어야지요?$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"suggestory formula\"\n",
    "                    datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "                    datapoint[\"rule_order\"] = \"59\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.6 suggestory(plus) was applied, \"\n",
    "                    # coverage 진짜 낮음\n",
    "                elif re_morphmatch(morphs, morph=\"^(고[싶지?않?])$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"want statement\"\n",
    "                    datapoint[\"rule\"] = \"want periphrastic construction/plus\"\n",
    "                    datapoint[\"rule_order\"] = \"60\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.7 want(plus) was applied, \"\n",
    "                elif re_morphmatch(morphs, morph=\"^[ㄹ을]건가요?$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"want statement\"\n",
    "                    datapoint[\"rule\"] = \"want periphrastic construction/plus\"\n",
    "                    datapoint[\"rule_order\"] = \"61\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.8 want(plus) was applied, \"\n",
    "                    # coverage 진짜 낮음\n",
    "                elif re_morphmatch(morphs, morph=\"^[ㄹ을]것이$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^ㄴ가요$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                    datapoint[\"group\"] = \"want statement\"\n",
    "                    datapoint[\"rule\"] = \"want periphrastic construction/plus\"\n",
    "                    datapoint[\"rule_order\"] = \"62\"\n",
    "                    datapoint[\"applied_rule\"] += \"1.9 want(plus) was applied, \"\n",
    "                    # coverage and precision high\n",
    "#                 elif re_morphmatch(morphs, morph=\"^(을게요|ㄹ게요|게요|을께요|ㄹ께요|ㄹ게|ㄹ께|ㄹ건데요|ㄹ건가|ㄹ거라|을건데요|는건가요|은건가요|ㄹ겁니까)$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "#                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                     datapoint[\"group\"] = \"want statement\"\n",
    "#                     datapoint[\"rule\"] = \"want verbal suffix/plus\"\n",
    "#                     datapoint[\"applied_rule\"] += \"10. want(plus) was applied, \"\n",
    "                # 말씀하시는건가요 이런거 섞여있음 나중에 다시 적용 \n",
    "                #그럼 이걸로 하실겁니까 ? 얼마 충전하실겁니까 ? 혹시 체크카드로 현금인출을하려고하시는건가요 ? (이런거 cover하고 싶었던 듯)\n",
    "#                 elif re_morphmatch(morphs, morph=\"^ㄹ것이$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "#                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                     datapoint[\"group\"] = \"want statement\"\n",
    "#                     datapoint[\"rule\"] = \"want verbal suffix/plus\"\n",
    "#                     datapoint[\"applied_rule\"] += \"11. want(plus) was applied, \"\n",
    "                # 이것저것 다 섞여있음 나중에 다시 적용\n",
    "                    \n",
    "                else: \n",
    "                    datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "                \n",
    "                if datapoint[\"subject_by_rule\"] == \"N/A\": \n",
    "                    if re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^습니다$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"63\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.1 want(plus) was applied, \"\n",
    "                    elif re_morphmatch(morphs, morph=\"^(겠|겟)$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^습니다$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"64\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.2 want was applied, \"\n",
    "                        # coverage and precision very high\n",
    "                        # -드리겠-도 이미 포함\n",
    "                        \n",
    "                    elif re_morphmatch(morphs, morph=\"^(려고하|으려고하|사려하|려|려하|으려하|으려고|려고|어보려고하|려고그러|려고요|으려고요|ㄹ려고요|려는것이|고자하|ㄹ려고하|기로하|ㄹ려구요|려구요|사려하)$\", pos=\"^(X|E|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"65\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.2.1 want(plus) was applied, \"\n",
    "                    elif re_morphmatch(morphs, morph=\"^(려고하|으려고하|사려하|려|려하|으려하|으려고|려고|어보려고하|려고그러|려고요|으려고요|ㄹ려고요|려는것이|고자하|ㄹ려고하|기로하|ㄹ려구요|려구요|사려하)$\", pos=\"^(X|E|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"66\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.2.2 want was applied, \"\n",
    "                        # coverage and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(려고하|으려고하|사려하|려|려하|으려하|으려고|려고|어보려고하|려고그러|려고요|으려고요|ㄹ려고요|려는것이|고자하|ㄹ려고하|기로하|ㄹ려구요|려구요|사려하)$\", pos=\"^(X|E|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"67\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.2.3 want was applied, \"\n",
    "                        \n",
    "                    elif re_morphmatch(morphs, morph=\"^[ㄹ을으]려고요?$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"68\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.3 want was applied, \"\n",
    "                        # coverage not that high\n",
    "                    elif re_morphmatch(morphs, morph=\"^아가$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^되$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"permission\"\n",
    "                        datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"69\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.4 permission was applied, \"\n",
    "                        # coverage really low\n",
    "                    elif re_morphmatch(morphs, morph=\"^(아보|어보|보|가보|해보|써보|타보|찾아보|재보|맛보)$\", pos=\"^(X|V)$\") and re_morphmatch(morphs, morph=\"^(아도되|아도돼|ㄹ수있|면안되)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"permission\"\n",
    "                        datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"70\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.5 permission was applied, \"\n",
    "                        # coverage and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(아도되|어도되|도되)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"permission\"\n",
    "                        datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"71\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.6 permission was applied, \"\n",
    "                        # coverage and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(면되|으면되)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"72\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.7 want was applied, \"\n",
    "                    elif re_morphmatch(morphs, morph=\"^(았|었)$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^어야지요?$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"73\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.8 want was applied, \"\n",
    "                         # nichs getroffen 원래 의도는 20884 곤충은 검역대상인데 신고를 하셨어야지요.\n",
    "\n",
    "                    elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                        datapoint[\"rule\"] = \"obligation periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"74\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.9 obligation(plus) was applied, \"\n",
    "                        # covearge and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^ㄹ것같$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"75\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.10 want was applied, \"\n",
    "                        # coverage not that high, one wrong case (ambiguous even for human)\n",
    "                    elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                        datapoint[\"rule\"] = \"obligation periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"76\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.11 obligation was applied, \"\n",
    "                        # covearge and precision high\n",
    "                        \n",
    "                    elif re_morphmatch(morphs, morph=\"^(고싶|고싶지않)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"77\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.12 want was applied, \"\n",
    "                        # coverage very low, two cases\n",
    "                    elif re_morphmatch(morphs, morph=\"^을건가요$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"ques\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"78\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.13 want was applied, \"\n",
    "                        # coverage not that high\n",
    "                    elif re_morphmatch(morphs, morph=\"^[ㄹ을]것이$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^ㄴ가요$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"ques\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"79\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.14 want was applied, \"\n",
    "                        # coverage not that high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(을게요|ㄹ게요|게요|을께요|ㄹ께요|ㄹ게|ㄹ께|ㄹ건데요|ㄹ건가|ㄹ거라|을건데요|는건데요|은건데요)?$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"80\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.15 want was applied, \"\n",
    "                        # coverage and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(을래요|ㄹ래요)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"81\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.16 want was applied, \"\n",
    "                        # coverage not that high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(을래요|ㄹ래요|ㄹ겁니까)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"82\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.17 want was applied, \"\n",
    "                        # coverage and precision high\n",
    "                    elif re_morphmatch(morphs, morph=\"^ㄹ것이$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want verbal suffix\"\n",
    "                        datapoint[\"rule_order\"] = \"83\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.18 want was applied, \"\n",
    "                        # nichs getroffen (ㄹ것이 plus에서 이미 다 cover되는 듯)\n",
    "                    elif re_morphmatch(morphs, morph=\"^(고싶|ㄹ것이|고싶지않)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"84\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.19.1 want(plus) was applied, \"\n",
    "                        # definition of subject is still the problem e.g. 통로를 걸어가셔서 문을 열고 왼쪽으로 꺾으시면 바로 보이실겁니다 . (화장실? 당신?)\n",
    "#                     elif re_morphmatch(morphs, morph=\"^(생기|나오|멀|멀쩡하|정교하|기품있|편리하|덜컹거리|싱싱하|저렴하|부드럽|아슬하|저렴하|잠기|지나|사라지|조용하|나뉘|멀쩡하|딱맞|유효하|독하|생기|개운하|고급스러|독특하|좋아보이|뿌옇|특이하|가동하|붐비|저렴|유치하|잘어울리|거대하|부풀어오르|특별하|한산하|훌륭하|열|견고하|터지|동일하|알록달록하|달|어둡|저렴하|대단하|상영하|유명하|저렴하|기품있|경쾌하|튼튼하|화려하|많|밀리|열리|작동하|들어있|닫|간단하|담백하|적당하|남|빠듯하|돌|차|늦|복잡하|기|짧|머|저렴하|아담하|가벼우|어둡|밝|쏠리|부드럽|달|다르|크|작|적|활기차|비리|고소하|저렴하|달|싱싱하|맑|유명하|굳|운치있|혼잡하|독특하|똑같|조용하|한산하|매콤하|새콤하|꼬불꼬불하|다르|징그럽|모자르|시끄럽|심하|초과하|위치하|심해지|높|어울리|고장나|남성스럽|상하|익혀지|멋지|딸리|흔들리|작|싱겁|바뀌|깨지|끝나|이륙하|귀엽|험하|무섭|문제없|번거롭|어렵|쉽|아름답|예쁘|답답하|더럽|만만하|신기하|나|재밌|재미있|지루하|벗겨지|무겁|깨|튀|마르|위험하|이색적이|가볍|느끼하|맛있|불편하|시원하|덥|춥|편하|불편하|출발하|편안하|아프|관계없|싸|비싸|가능하|불가능하|걸리|고쳐지|깨끗하|나오|내리|다니|줄어들|되|들어오|마련되|나아지|맞|매우|맵|멀|가깝|보이|비슷하|비치되|시원하|시작하|없|아니|이|이기|이색적이|있|잘되|충분하|\\w+되|\\w+어지|\\w+히)$\", pos=\"^(J|V|N)$\"):\n",
    "#                         datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                         datapoint[\"group\"] = \"exclude\"\n",
    "#                         datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                         datapoint[\"rule_order\"] = \"85\"\n",
    "#                         datapoint[\"applied_rule\"] += \"2.19.2 exclude_predicate1 was applied, \"\n",
    "                        # many predicates are excluded already here\n",
    "#                     elif re_morphmatch(morphs, morph=\"^(어있|아있|아지|어지|지|ㄹ만하|게되어있)$\", pos=\"^X$\"):\n",
    "#                         datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                         datapoint[\"group\"] = \"exclude\"\n",
    "#                         datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                         datapoint[\"rule_order\"] = \"86\"\n",
    "#                         datapoint[\"applied_rule\"] += \"2.19.3 exclude_predicate2 was applied, \"\n",
    "                    elif re_morphmatch(morphs, morph=\"^(고싶|고싶지않)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"87\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.19.4 want was applied, \"\n",
    "                        # coverage and precion high\n",
    "                    elif re_morphmatch(morphs, morph=\"^(ㄹ것이|ㄹ거라)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"88\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.19.5 want was applied, \"\n",
    "#                     elif re_morphmatch(morphs, morph=\"^(\\w+다고하|\\w+라고하|다고하)$\", pos=\"^X$\"):\n",
    "#                         datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                         datapoint[\"group\"] = \"exclude\"\n",
    "#                         datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                         datapoint[\"rule_order\"] = \"89\"\n",
    "#                         datapoint[\"applied_rule\"] += \"2.19.6 exclude_hearsay was applied, \"\n",
    "              \n",
    "                        \n",
    "                    elif re_morphmatch(morphs, morph=\"^고도$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^싶$\", pos=\"^J$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                        datapoint[\"group\"] = \"want statement\"\n",
    "                        datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                        datapoint[\"rule_order\"] = \"90\"\n",
    "                        datapoint[\"applied_rule\"] += \"2.20 want was applied, \"\n",
    "                        # coverage really low, one case\n",
    "\n",
    "                    else: \n",
    "                        datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "#                     # 9777 getroffen (56.7% of 17,266)\n",
    "\n",
    "                    if datapoint[\"subject_by_rule\"] == \"N/A\": \n",
    "                        if re_morphmatch(morphs, morph=\"^(을까요|ㄹ까요|을까|ㄹ까)$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":  \n",
    "                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                            datapoint[\"group\"] = \"suggestory formula\"\n",
    "                            datapoint[\"rule\"] = \"suggestory verbal suffix\"\n",
    "                            datapoint[\"rule_order\"] = \"91\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.1.1 suggestory(plus) was applied, \"\n",
    "                        elif re_morphmatch(morphs, morph=\"^(을까요|ㄹ까요|을까|ㄹ까)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"ques\":  \n",
    "                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                            datapoint[\"group\"] = \"suggestory formula\"\n",
    "                            datapoint[\"rule\"] = \"suggestory verbal suffix\"\n",
    "                            datapoint[\"rule_order\"] = \"92\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.1.2 suggestory was applied, \"\n",
    "                            # 10991, 11546은 나중에 exclude list로 제외돼야 함! e.g. 죄송하지만 조금만 빨리 가주을까요 ? 원래 데이터 수정해도 반영이 안됨\n",
    "#                         elif re_morphmatch(morphs, morph=\"^(는게좋|는게낫|는것이좋|면좋)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                             datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                             datapoint[\"group\"] = \"suggestory formula\"\n",
    "#                             datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "#                             datapoint[\"applied_rule\"] += \"3.2 suggestory was applied, \"\n",
    "                            # mixed subjects 일단 보류\n",
    "#                         elif re_morphmatch(morphs, morph=\"^(는게좋|는게낫|는것이좋|면좋)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "#                             datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                             datapoint[\"group\"] = \"suggestory formula\"\n",
    "#                             datapoint[\"rule\"] = \"suggestory periphrastic construction\"\n",
    "#                             datapoint[\"applied_rule\"] += \"3.2.1 suggestory was applied, \"\n",
    "                            # nichts getroffen\n",
    "                        elif re_morphmatch(morphs, morph=\"^(ㄴ적이?있|ㄴ적이?없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                            datapoint[\"group\"] = \"experience\"\n",
    "                            datapoint[\"rule\"] = \"experience periphrastic construction\"\n",
    "                            datapoint[\"rule_order\"] = \"93\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.3 experience was applied, \"\n",
    "                            # coverage not that high\n",
    "                        elif re_morphmatch(morphs, morph=\"^(ㄴ적이?있|ㄴ적이?없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                            datapoint[\"group\"] = \"experience\"\n",
    "                            datapoint[\"rule\"] = \"experience periphrastic construction\"\n",
    "                            datapoint[\"rule_order\"] = \"94\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.4 experience was applied, \"\n",
    "                            # nichts getroffen (covered by plus)\n",
    "                        elif re_morphmatch(morphs, morph=\"^더니$\", pos=\"^E$\") and datapoint[\"tense_zp\"] == \"past\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                            datapoint[\"group\"] = \"experience\"\n",
    "                            datapoint[\"rule\"] = \"experience periphrastic construction\"\n",
    "                            datapoint[\"rule_order\"] = \"95\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.5 experience was applied, \"\n",
    "                            # coverage not that high\n",
    "                        elif re_morphmatch(morphs, morph=\"^([아어]드리)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                            datapoint[\"group\"] = \"politeness\"\n",
    "                            datapoint[\"rule\"] = \"politeness auxiliary verb\"\n",
    "                            datapoint[\"rule_order\"] = \"96\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.6 politeness(plus) was applied, \"\n",
    "                            # nichts getroffen\n",
    "                        elif re_morphmatch(morphs, morph=\"^(아버리|어버리|잃어버리)$\", pos=\"^(X|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                            datapoint[\"group\"] = \"subjectivication\"\n",
    "                            datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                            datapoint[\"rule_order\"] = \"97\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.7 subjectivication(plus) was applied, \"\n",
    "                            # coverage not that high\n",
    "                        elif re_morphmatch(morphs, morph=\"^(아보|어보|가보|해보|써보|타보|찾아보|재보)$\", pos=\"^(X|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                            datapoint[\"group\"] = \"try\"\n",
    "                            datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                            datapoint[\"rule_order\"] = \"98\"\n",
    "                            datapoint[\"applied_rule\"] += \"3.8 try(plus) was applied, \"\n",
    "                            # clau도 포함됨 e.g.써보시고 \n",
    "\n",
    "                        else: \n",
    "                            datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "                            \n",
    "#                         performative_head_list = [\"부탁\", \"권유\"]\n",
    "#                         cognition_head_list = [\"예정\",\"생각\",\"기억\",\"확인\",\"깜박\",\"예상\",\"착각\",\"이해\",\"확실\",\"집중\",\"계획\",\"숙지\",\"오해\",\"파악\",\"동의\",\"사과\",\"깜빡\"]\n",
    "#                         sensation_head_list = [\"시음\",\"민감\",\"구경\",\"시원\"]\n",
    "#                         feeling_head_list = [\"실망\",\"공포\",\"영광\",\"장담\",\"한시름\",\"미안\",\"실례\"]\n",
    "                                               \n",
    "                        if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                            if [x for x in performative_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                datapoint[\"group\"] = \"performative\"\n",
    "                                datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"99\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.1 performative(plus) was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^부탁하$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^(면되|아도되|ㄹ수있)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"permission\"\n",
    "                                datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"100\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.2 permission was applied, \"\n",
    "                                # coverage really low 모닝콜 서비스를 부탁해도 되겠습니까? 이미 위에서 커버됨 2.5+6 permission was applied, / one case\n",
    "                            elif re_morphmatch(morphs, morph=\"^(기?원하|바라|기바라|추천하|필요하|필요없|ㄹ필요는없|빌|권하|지않아도되)$\", pos=\"^(V|X|J)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"performative\"\n",
    "                                datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"101\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.3 performative(plus) was applied, \"\n",
    "                                # coverage and precision very high\n",
    "                                # 나중에 zero referents 틀린 case 찾아볼 때는 \"subject_error_analysis_080822.xlxs\" 참고\n",
    "                            elif re_morphmatch(morphs, morph=\"^(는줄알|ㄴ줄알|을줄알|ㄹ줄알|ㄹ줄아|잘알|알|아|알게되|잘아|모르|는줄모르|을줄모르|ㄹ줄모르|어야할지모르|는지모르|ㄴ지모르|을지모르|ㄹ지모르|을수있을지모르|알아들|궁금하|고집하)$\", pos=\"^(X|V|J)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"102\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.4 cognition(plus) was applied, \"\n",
    "                                # coverage and precision high\n",
    "                            elif re_morphmatch(morphs, morph=\"^(잊|잊어버리|믿|알아듣|외우|헷갈리)$\", pos=\"^V$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"103\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.5 cognition(plus) was applied, \"\n",
    "                                # coverage low, only 3 cases\n",
    "                            elif [x for x in cognition_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"104\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.6 cognition(plus) was applied, \"\n",
    "                                # \"예정이다\", '예정하다'\n",
    "                                # coverage ok\n",
    "                            elif re_morphmatch(morphs, morph=\"^(보|못보|살펴보|둘러보|보러오|듣|들어서|들|들어보|맛보|배고프|싱거우|간지럽)$\", pos=\"^(V|J)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"105\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.7 sensation(plus) was applied, \"\n",
    "                                # coverage ok\n",
    "                            elif [x for x in sensation_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"106\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.8 sensation(plus) was applied, \"\n",
    "                                # coverage low, only 2 cases\n",
    "                            elif re_morphmatch(morphs, morph=\"^(춥|따뜻하|덥|후텁지근하|찝찝하|아프|쓰라리|만지|만져보|느끼)$\", pos=\"^(J|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"107\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.9 sensation(plus) was applied, \"\n",
    "                                # coverage low, only 3 cases\n",
    "                            elif re_morphmatch(morphs, morph=\"^(괜찮|고맙|불편하|피곤하|지루하|흥미롭|곤란하|불쾌하|거북하|불안하|힘들|힘드|기힘들|좋|어쩔수없|별수없)$\", pos=\"^(J|X)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate/plus\"\n",
    "                                datapoint[\"rule_order\"] = \"108\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.10 feeling(plus) was applied, \"\n",
    "                                # coverage ok\n",
    "                            elif re_morphmatch(morphs, morph=\"^(좋아하|부담되|기대되|마음에들|기대하|걱정되|걱정하|고민되|고민하|놀라|당황하|놀래|놀랍|행복하|미안하|죄송하|죄송|답답하|부담스럽|의아하|질색이)$\", pos=\"^(V|N)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"109\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.11 feeling(plus) was applied, \"\n",
    "                                # coverage not that high, 6 cases\n",
    "                            elif re_morphmatch(morphs, morph=\"^(후회하|감사하|감사드리|감탄하|횡재하|들뜨|떨리|만족하|만족스럽|망설여지|망설이|싫|싫어하)$\", pos=\"^(V|J)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"110\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.12 feeling(plus) was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif [x for x in feeling_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"111\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.13 feeling(plus) was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^(익숙하|자부하|망설여지|망설이|아쉽|상관없|즐겁|재미있|재미없|즐거우|무섭|기쁘|행복하|난감하|반갑|환영하|신기하|당황스럽|막막하|안타깝|부럽)$\", pos=\"^(J|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"112\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.14 feeling(plus) was applied, \"\n",
    "                                # coverage low, only 2 cases\n",
    "                            elif re_morphmatch(morphs, morph=\"^(드시|안드|계시|말씀하|고계시)$\", pos=\"^(V|X)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"politeness\"\n",
    "                                datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"113\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.15.1 politeness(plus) was applied, \"\n",
    "                                # coverage very high, some cases lower the precision\n",
    "                            elif re_morphmatch(morphs, morph=\"^(드시|안드|계시|말씀하|고계시)$\", pos=\"^(V|X)$\"):\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"politeness\"\n",
    "                                datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"114\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.15.2 politeness was applied, \"\n",
    "                                # coverage ok, two cases off\n",
    "                            elif re_morphmatch(morphs, morph=\"^말씀하$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^아드리$\", pos=\"^X$\"):\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"politeness\"\n",
    "                                datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"115\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.16.1 politeness was applied, \"\n",
    "                            elif re_morphmatch(morphs, morph=\"^축하드리$\", pos=\"^V$\"):\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"politeness\"\n",
    "                                datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"116\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.16.2 politeness was applied, \"\n",
    "                                \n",
    "                                # nichts getroffen\n",
    "#                             elif re_morphmatch(morphs, morph=\"^(귀엽|험하|무섭|문제없|번거롭|어렵|쉽|아름답|예쁘|답답하|더럽|만만하|신기하|재밌|재미있|지루하|무겁|이색적이|가볍|느끼하|맛있|불편하|시원하|덥|춥|편하|불편하|편안하|아프|관계없|싸|비싸|가능하|불가능하|걸리|고쳐지|깨끗하|나오|내리|다니|되|들어오|마련되|맞|매우|맵|멀|가깝|보이|비슷하|비치되|시원하|시작하|없|아니|이|이기|이색적이|있|잘되|충분하|\\w+되|\\w+어지|\\w+히)$\", pos=\"^(J|V)$\"):\n",
    "#                                 datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                                 datapoint[\"group\"] = \"exclude\"\n",
    "#                                 datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                                 datapoint[\"applied_rule\"] += \"4.17.1 exclude_predicate1 was applied, \"\n",
    "#                             elif re_morphmatch(morphs, morph=\"^(어있|아있|아지|어지)$\", pos=\"^X$\"):\n",
    "#                                 datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                                 datapoint[\"group\"] = \"exclude\"\n",
    "#                                 datapoint[\"rule\"] = \"exclude predicate\"\n",
    "#                                 datapoint[\"applied_rule\"] += \"4.17.2 exclude_predicate2 was applied, \"\n",
    "                            elif re_morphmatch(morphs, morph=\"^(을것이|ㄹ것이|을거이|ㄹ거이)$\", pos=\"^(X|E)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"want statement\"\n",
    "                                datapoint[\"rule\"] = \"want(plus) periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"117\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.17.1 want/guess1 was applied, \"\n",
    "                            # coverage ok\n",
    "                            # definition of subject lower the precision, two cases e.g. 쑥 내려가실거예요 ./ 노랫소리처럼 들리실거예요 .\n",
    "                            elif re_morphmatch(morphs, morph=\"^(을것이|ㄹ것이|을거이|ㄹ거이)$\", pos=\"^(X|E)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"want statement\"\n",
    "                                datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"118\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.17.2 want/guess1 was applied, \"\n",
    "                                # coverage ok but precision is really low because of the ambiguity between the first and second person\n",
    "                            elif re_morphmatch(morphs, morph=\"^(을것이|ㄹ것이|을거이|ㄹ거이)$\", pos=\"^(X|E)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                datapoint[\"group\"] = \"want statement\"\n",
    "                                datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"119\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.17.3 want/guess2 was applied, \"   \n",
    "                            elif re_morphmatch(morphs, morph=\"^지않을것이$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"guess\"\n",
    "                                datapoint[\"rule\"] = \"guess periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"120\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.17.4 guess1 was applied, \"\n",
    "                                # nichts getroffen \n",
    "                            elif re_morphmatch(morphs, morph=\"^진않$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^을거이$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"guess\"\n",
    "                                datapoint[\"rule\"] = \"guess periphrastic construction\"\n",
    "                                datapoint[\"rule_order\"] = \"121\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.17.5 guess2 was applied, \"\n",
    "                                # coverage and precision very low\n",
    "\n",
    "                            elif [x for x in cognition_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"122\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.18 cognition was applied, \"\n",
    "                                # \"확인\"이 조금 문제 , 1sh+를 만듦 \n",
    "                                # coverage ok but precision low because of segments with \"확인\"\n",
    "                            elif [x for x in performative_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"performative\"\n",
    "                                datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"123\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.19 performative was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^(기?원하|바라|기바라|추천하|필요하|필요없|ㄹ필요는없|지않아도되|빌|권하)$\", pos=\"^(V|X|J)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"performative\"\n",
    "                                datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"124\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.20 performative was applied, \"\n",
    "                                # coverage not that high, one case of with \"필요하다\", hard even for human\n",
    "                            elif re_morphmatch(morphs, morph=\"^(는줄알|ㄴ줄알|을줄알|ㄹ줄알|ㄹ줄아|잘알|알|아|알게되|잘아|모르|는줄모르|ㄴ즐모르|을줄모르|ㄹ줄모르|어야할지모르|는지모르|을지모르|ㄹ지모르|을수있을지모르|ㄹ수있을지모르|알아들|궁금하|고집하)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"125\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.21 cognition was applied, \"\n",
    "                            elif [x for x in cognition_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"126\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.22 cognition was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^(잊|잊어버리|믿|알아듣|외우|헷갈리)$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"127\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.23 cognition was applied, \"\n",
    "                                # coverage not that high\n",
    "                            elif re_morphmatch(morphs, morph=\"^(보|못보|살펴보|둘러보|보러오|듣|들어서|들|들어보|맛보|배고프|싱거우|간지럽)$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"128\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.24 sensation was applied, \"\n",
    "                                # coverage and precision very low\n",
    "                            elif [x for x in sensation_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"129\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.25 sensation was applied, \"\n",
    "                                # coverage and precision very low, ambiguous cases include between discourse participants and general people e.g. A 코스는 어디를 구경합니까 ?\n",
    "                            elif re_morphmatch(morphs, morph=\"^(춥|따뜻하|덥|후텁지근하|찝찝하|아프|쓰라리|만지|만져보|느끼)$\", pos=\"^(J|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"130\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.26 sensation was applied, \"\n",
    "                                # coverage not that high and segmentation problem 25303 이제 안전벨트 풀어도 괜찮은거죠 ? exception일수도\n",
    "                            elif re_morphmatch(morphs, morph=\"^(고맙|불편하|피곤하|지루하|흥미롭|곤란하|불쾌하|거북하|불안하|힘들|힘드|기힘들)$\", pos=\"^(J|X)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"131\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.27 feeling was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^(좋아하|부담되|기대되|마음에들|기대하|걱정되|걱정하|고민되|고만하|놀라|당황하|놀래|놀랍|행복하|미안하|죄송하|죄송|답답하|부담스럽|의아하|질색이)$\", pos=\"^(V|J|N)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"132\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.28 feeling was applied, \"\n",
    "                                # coverage very low\n",
    "                            elif re_morphmatch(morphs, morph=\"^(후회하|감사하|감사드리|감탄하|횡재하|들뜨|떨리|만족하|만족스럽|망설여지|망설이|싫|싫어하)$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"133\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.29 feeling was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif [x for x in feeling_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"134\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.30 feeling was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            elif re_morphmatch(morphs, morph=\"^(익숙하|자부하|망설여지|망설이|아쉽|상관없|즐겁|재미있|재미없|즐거우|무섭|기쁘|행복하|난감하|반갑|환영하|신기하|당황스럽|막막하|안타깝|부럽)$\", pos=\"^J$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                datapoint[\"group\"] = \"experience\"\n",
    "                                datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                datapoint[\"rule_order\"] = \"135\"\n",
    "                                datapoint[\"applied_rule\"] += \"4.31 feeling was applied, \"\n",
    "                                # nichts getroffen\n",
    "                            else: \n",
    "                                datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                            if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                if [x for x in cognition_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"136\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.1 coginition was applied, \"\n",
    "                                    # coverage high,some ambiguous cases lower the precision\n",
    "                                elif re_morphmatch(morphs, morph=\"^(면되|으면되|기만하면되|아도되|어도되|아가|도되|지않아도되|아야하|어야하|여야하|야하|어야할것이|아야할것이|어야할것같|아야되|어야되|야되|아야죠|어야죠|야지요|으면안되|면안되)$\", pos=\"^(X|E)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                    datapoint[\"group\"] = \"permission\"\n",
    "                                    datapoint[\"rule\"] = \"permission periphrastic construction/plus\"\n",
    "                                    datapoint[\"rule_order\"] = \"137\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.2.1 permission(plus) was applied, \"\n",
    "                                    # coverage and precision very high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아야겠|어야겠)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"want statement\"\n",
    "                                    datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"138\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.2.2 want was applied, \"\n",
    "                                    # coverage ok\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"want statement\"\n",
    "                                    datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"139\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.2.3 want was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif [x for x in performative_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"performative\"\n",
    "                                    datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"140\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.2.4 performative was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(기?원하|바라|기바라|추천하|필요하|필요없|ㄹ필요는없|빌|권하)$\", pos=\"^(V|X|J)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"performative\"\n",
    "                                    datapoint[\"rule\"] = \"performative predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"141\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.3 performative was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(는줄알|ㄴ줄알|을줄알|ㄹ줄알|ㄹ줄아|잘알|알|아|알게되|잘아|모르|는줄모르|을줄모르|ㄹ줄모르|어야할지모르|는지모르|ㄴ지모르|을지모르|ㄹ지모르|을수있을지모르|알아들|궁금하|고집하)$\", pos=\"^(X|V|J)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"142\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.4 cognition was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(잊|잊어버리|믿|알아듣|외우|헷갈리)$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"cognition predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"143\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.5 cognition was applied, \"\n",
    "                                    # coverage not that high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(보|못보|살펴보|둘러보|보러오|듣|들어서|들|들어보|맛보|배고프|싱거우|간지럽)$\", pos=\"^(V|J)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"144\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.6 sensation was applied, \"\n",
    "                                    # coverage high, some cases are off \n",
    "                                elif [x for x in sensation_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"145\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.7 sensation was applied, \"\n",
    "                                    # coverage and precision very low\n",
    "                                elif re_morphmatch(morphs, morph=\"^(춥|따뜻하|덥|후텁지근하|찝찝하|아프|쓰라리|만지|만져보|느끼)$\", pos=\"^(J|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"sensation predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"146\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.8 sensation was applied, \"\n",
    "                                    # coverage and precision low, somce cases have the issue with the definition of subject e.g. 따뜻합니다 ./ 네, 손바닥이 부어오르고 쓰라리네요 .\n",
    "                                elif re_morphmatch(morphs, morph=\"^(괜찮|고맙|불편하|피곤하|지루하|흥미롭|곤란하|불쾌하|거북하|불안하|힘들|힘드|기힘들|좋|어쩔수없|별수없)$\", pos=\"^(J|X)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"147\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.9 feeling was applied, \"\n",
    "                                    # 괜찮다, 좋다의 다의성으로 인해 off되는 case 많음\n",
    "                                elif re_morphmatch(morphs, morph=\"^(좋아하|부담되|기대되|마음에들|기대하|걱정되|걱정하|고민되|고민하|놀라|당황하|놀래|놀랍|행복하|미안하|죄송하|죄송|답답하|부담스럽|의아하|질색이)$\", pos=\"^(V|N)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"148\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.10 feeling was applied, \"\n",
    "                                    # some cases with \"마음에 들다\" lower the precision and also related to the definition of subject issue\n",
    "                                elif re_morphmatch(morphs, morph=\"^(후회하|감사하|감사드리|감탄하|횡재하|들뜨|떨리|만족하|만족스럽|망설여지|망설이|싫|싫어하)$\", pos=\"^(V|J)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"149\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.11 feeling was applied, \"\n",
    "                                    # coverage ok\n",
    "                                elif [x for x in feeling_head_list if x in datapoint[\"verb\"]] and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"150\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.12 feeling was applied, \"\n",
    "                                    # coverage not that high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(익숙하|자부하|망설여지|망설이|아쉽|상관없|즐겁|재미있|재미없|즐거우|무섭|기쁘|행복하|난감하|반갑|신기하|당황스럽|막막하|안타깝|부럽)$\", pos=\"^(J|V)$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\"):\n",
    "                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"151\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.13.1 feeling was applied, \"\n",
    "                                    # coverage very low, one case\n",
    "                                elif re_morphmatch(morphs, morph=\"^(익숙하|자부하|망설여지|망설이|아쉽|상관없|즐겁|재미있|재미없|즐거우|무섭|기쁘|행복하|난감하|반갑|신기하|당황스럽|막막하|안타깝|부럽)$\", pos=\"^(J|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"experience\"\n",
    "                                    datapoint[\"rule\"] = \"feeling predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"152\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.13.2 feeling was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(드리)$\", pos=\"^(V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"politeness\"\n",
    "                                    datapoint[\"rule\"] = \"politeness predicate/auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"153\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.14.1 politeness was applied, \"\n",
    "                                    # coverage ok\n",
    "                                elif re_morphmatch(morphs, morph=\"^드리$\", pos=\"^V$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"politeness\"\n",
    "                                    datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"154\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.14.2 politeness was applied, \"\n",
    "                                    # coverage low, 3 cases\n",
    "                                elif re_morphmatch(morphs, morph=\"^드리$\", pos=\"^V$\") and datapoint[\"verb_syn\"] == \"clau\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"politeness\"\n",
    "                                    datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"155\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.14.3 politeness was applied, \"\n",
    "                                    # nichts getroffen\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아드리|어드리)$\", pos=\"^X$\"):\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"politeness\"\n",
    "                                    datapoint[\"rule\"] = \"politeness auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"156\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.14.4 politeness was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(가져다드리|알려드리|보여드리|해드리|연락드리|도와드리|돌려드리|전해드리|전화드리|갖다드리)$\", pos=\"^V$\"):\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"politeness\"\n",
    "                                    datapoint[\"rule\"] = \"politeness predicate\"\n",
    "                                    datapoint[\"rule_order\"] = \"157\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.15 politeness was applied, \"\n",
    "                                    # coverage and precision high\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아버리|어버리|잃어버리)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"subjectification\"\n",
    "                                    datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"158\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.16.1 subjectification was applied, \"\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아버리|어버리|잃어버리)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                    datapoint[\"group\"] = \"subjectification\"\n",
    "                                    datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"159\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.16.2 subjectification was applied, \"\n",
    "                                    # coverage very low, one case\n",
    "                                elif re_morphmatch(morphs, morph=\"^(ㄹ수밖에없|는수밖에없)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"160\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.17.1 possibility was applied, \"\n",
    "                                    # coverage really low, one case\n",
    "                                elif re_morphmatch(morphs, morph=\"^(ㄹ수밖에없|는수밖에없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"161\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.17.2 possibility was applied, \"\n",
    "                                    # coverage low, four cases\n",
    "#                                 elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "#                                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                                     datapoint[\"group\"] = \"want statement\"\n",
    "#                                     datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "#                                     datapoint[\"applied_rule\"] += \"5.18.1 want(plus) was applied, \"\n",
    "#                                 elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "#                                     datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                                     datapoint[\"group\"] = \"want statement\"\n",
    "#                                     datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "#                                     datapoint[\"applied_rule\"] += \"5.18.2 want was applied, \"\n",
    "                                elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하|야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"verb_syn\"] == \"clau\": \n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"want statement\"\n",
    "                                    datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"162\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.18 want was applied, \"\n",
    "                                    # nichts getroffen\n",
    "                                elif re_morphmatch(morphs, morph=\"^야하$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^는데요?$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"obligation statement\"\n",
    "                                    datapoint[\"rule\"] = \"obligation periphrastic construction\"\n",
    "                                    datapoint[\"rule_order\"] = \"163\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.19 obligation was applied, \"\n",
    "                                    # coverage low, 4 cases\n",
    "                                elif re_morphmatch(morphs, morph=\"^어보$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^려고요$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                    datapoint[\"group\"] = \"try\"\n",
    "                                    datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"164\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.20 try was applied, \"\n",
    "                                    # nichts getroffen\n",
    "                                elif re_morphmatch(morphs, morph=\"^어보$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^려고요$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                    datapoint[\"group\"] = \"try\"\n",
    "                                    datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                    datapoint[\"rule_order\"] = \"165\"\n",
    "                                    datapoint[\"applied_rule\"] += \"5.21 try was applied, \"\n",
    "                                    # nichts getroffen\n",
    "                                    \n",
    "                                else: \n",
    "                                    datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "\n",
    "                                if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                    if re_morphmatch(morphs, morph=\"^(아야겠|어야겠|어야|야하|아야)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                                        datapoint[\"rule\"] = \"obligation periphrastic construction/plus\"\n",
    "                                        datapoint[\"rule_order\"] = \"166\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.1 obligation(plus) was applied, \"\n",
    "                                        # coverage really low, one case\n",
    "                                    elif re_morphmatch(morphs, morph=\"^(려고하|으려고하|사려하|려|려하|으려하|으려고|려고|어보려고하|려고그러|려고요|으려고요|ㄹ려고요|려는것이|고자하|ㄹ려고하|기로하|ㄹ려구요|려구요|을것이|ㄹ것이|을거이|ㄹ거이)$\", pos=\"^(X|E)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"want statement\"\n",
    "                                        datapoint[\"rule\"] = \"want periphrastic construction/plus\"\n",
    "                                        datapoint[\"rule_order\"] = \"167\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.2 obligation(plus) was applied, \"\n",
    "                                        # nichts getroffen\n",
    "                                    elif re_morphmatch(morphs, morph=\"^(아야하|어야하|여야하)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                                        datapoint[\"rule\"] = \"obligation periphrastic construction/plus\"\n",
    "                                        datapoint[\"rule_order\"] = \"168\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.3 obligation(plus) was applied, \"\n",
    "                                        # nichts getroffen\n",
    "                                    elif re_morphmatch(morphs, morph=\"^가시$\", pos=\"^V$\") and re_morphmatch(morphs, morph=\"^(어야하|면되)$\", pos=\"^X$\"): \n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                                        datapoint[\"rule\"] = \"obligation periphrastic construction/plus\"\n",
    "                                        datapoint[\"rule_order\"] = \"169\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.4 obligation(plus) was applied, \"\n",
    "                                        # coverage really low 동사 하나만 cover하기 때문, 3 cases\n",
    "                                    elif re_morphmatch(morphs, morph=\"^으면하$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\": \n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                        datapoint[\"group\"] = \"want statement\"\n",
    "                                        datapoint[\"rule\"] = \"want periphrastic construction/plus\"\n",
    "                                        datapoint[\"rule_order\"] = \"170\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.5 want(plus) was applied, \"\n",
    "                                        # coverage ok\n",
    "                                    elif re_morphmatch(morphs, morph=\"^(으면안되|면안되)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                        datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                        datapoint[\"group\"] = \"permission\"\n",
    "                                        datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                                        datapoint[\"rule_order\"] = \"171\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.6 permission was applied, \"\n",
    "                                        # coverage low, 3 cases\n",
    "                                    elif re_morphmatch(morphs, morph=\"^(으면안되|면안되)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"prohibition\"\n",
    "                                        datapoint[\"rule\"] = \"prohibition periphrastic construction\"\n",
    "                                        datapoint[\"rule_order\"] = \"172\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.7 prohibition was applied, \"\n",
    "                                        # coverage low, one case off about 3rd. person\n",
    "                                    elif re_morphmatch(morphs, morph=\"^야하$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(ㅂ니다|아요)$\", pos=\"^E$\"): \n",
    "                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                                        datapoint[\"rule\"] = \"obligation periphrastic construction\"\n",
    "                                        datapoint[\"rule_order\"] = \"173\"\n",
    "                                        datapoint[\"applied_rule\"] += \"6.8 obligation was applied, \"\n",
    "                                        # coverage ok, some 1sh+ lower the precision, hard even for human\n",
    "                                        \n",
    "                                    else: \n",
    "                                        datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                                    if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "\n",
    "#                                         if re_morphmatch(morphs, morph=\"^(아야겠|어야겠)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "#                                             datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                                             datapoint[\"group\"] = \"want statement\"\n",
    "#                                             datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "#                                             datapoint[\"applied_rule\"] += \"7.1 want was applied, \"\n",
    "                                        if re_morphmatch(morphs, morph=\"^(아야겠|어야겠)$\", pos=\"^X$\") and datapoint[\"verb_syn\"] == \"clau\": \n",
    "                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                            datapoint[\"group\"] = \"want statement\"\n",
    "                                            datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                            datapoint[\"rule_order\"] = \"174\"\n",
    "                                            datapoint[\"applied_rule\"] += \"7.1 want was applied, \"\n",
    "                                            # nichts getroffen\n",
    "                                        elif re_morphmatch(morphs, morph=\"^(으면하|ㄹ까하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                            datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                                            datapoint[\"group\"] = \"want statement\"\n",
    "                                            datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                            datapoint[\"rule_order\"] = \"175\"\n",
    "                                            datapoint[\"applied_rule\"] += \"7.2 want was applied, \"\n",
    "                                            # coverage low, 8 cases\n",
    "                                        elif re_morphmatch(morphs, morph=\"^(을뿐이|ㄹ뿐이|는것뿐이|ㄴ것뿐이)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                            datapoint[\"subject_by_rule\"] = \"1sh+\"  \n",
    "                                            datapoint[\"group\"] = \"experience\"\n",
    "                                            datapoint[\"rule\"] = \"judgement periphrastic construction\"\n",
    "                                            datapoint[\"rule_order\"] = \"176\"\n",
    "                                            datapoint[\"applied_rule\"] += \"7.3 judgement was applied, \"\n",
    "                                            # coverage really low, 2 cases, one off\n",
    "                                        else: \n",
    "                                             datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                                        if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "#                                             if re_morphmatch(morphs, morph=\"^(면되|으면되|기만하면되|아도되|어도되|아가|도되|지않아도되|아야하|어야하|여야하|야하|어야할것이|아야할것이|어야할것같|아야되|어야되|야되|아야죠|어야죠|야지요|으면안되|면안되)$\", pos=\"^(X|E)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "#                                                 datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                                                 datapoint[\"group\"] = \"permission\"\n",
    "#                                                 datapoint[\"rule\"] = \"permission periphrastic construction/plus\"\n",
    "#                                                 datapoint[\"applied_rule\"] += \"8.1 permission(plus) was applied, \"\n",
    "#                                                 # coverage and precision very high\n",
    "                                            if re_morphmatch(morphs, morph=\"^(아보|어보|보|가보|해보|써보|타보|찾아보|재보|맛보)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                datapoint[\"group\"] = \"try\"\n",
    "                                                datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                                datapoint[\"rule_order\"] = \"177\"\n",
    "                                                datapoint[\"applied_rule\"] += \"8.1 try was applied, \"\n",
    "                                                # coverage ok, two cases off\n",
    "                                            elif re_morphmatch(morphs, morph=\"^(아보|어보|보|가보|해보|써보|타보|찾아보|재보|맛보)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                datapoint[\"group\"] = \"try\"\n",
    "                                                datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                                datapoint[\"rule_order\"] = \"178\"\n",
    "                                                datapoint[\"applied_rule\"] += \"8.2 try was applied, \"\n",
    "                                                # coverage really low, one case\n",
    "#                                             elif re_morphmatch(morphs, morph=\"^(면되|으면되|기만하면되|아도되|어도되|아가|도되|지않아도되|아야하|어야하|여야하|야하|어야할것이|아야할것이|어야할것같|아야되|어야되|야되|아야죠|어야죠|야지요|으면안되|면안되)$\", pos=\"^(X|E)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                                                 datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                                                 datapoint[\"group\"] = \"permission\"\n",
    "#                                                 datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "#                                                 datapoint[\"applied_rule\"] += \"8.4 permission was applied, \"\n",
    "#                                                 # precision not that high (사람도 context 없이 어려움)\n",
    "                                            elif re_morphmatch(morphs, morph=\"^(어야|아야|야하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\": \n",
    "                                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                datapoint[\"group\"] = \"want statement\"\n",
    "                                                datapoint[\"rule\"] = \"want periphrastic construction\"\n",
    "                                                datapoint[\"rule_order\"] = \"179\"\n",
    "                                                datapoint[\"applied_rule\"] += \"8.3 want was applied, \"\n",
    "                                                # coverage really low, one case\n",
    "                                            elif re_morphmatch(morphs, morph=\"^(면되|으면되|기만하면되|아도되|어도되|아가|도되|지않아도되|아야하|어야하|여야하|야하|어야할것이|아야할것이|어야할것같|아야되|어야되|야되|아야죠|어야죠|야지요|으면안되|면안되)$\", pos=\"^(X|E)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                datapoint[\"group\"] = \"permission\"\n",
    "                                                datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                                                datapoint[\"rule_order\"] = \"180\"\n",
    "                                                datapoint[\"applied_rule\"] += \"8.4 permission was applied, \"\n",
    "                                                # precision not that high (사람도 context 없이 어려움)/ 도메인 특성상 상대방에게 permission을 말하는 경우가 더 많을 것 같아서 2인칭으로\n",
    "                                                # 1인칭 30, 2인칭 69 \n",
    "                                            elif re_morphmatch(morphs, morph=\"^(면되|으면되|기만하면되|아도되|어도되|아가|도되|지않아도되|아야하|어야하|여야하|야하|어야할것이|아야할것이|어야할것같|아야되|어야되|야되|아야죠|어야죠|야지요|으면안되|면안되)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                datapoint[\"group\"] = \"permission\"\n",
    "                                                datapoint[\"rule\"] = \"permission periphrastic construction\"\n",
    "                                                datapoint[\"rule_order\"] = \"181\"\n",
    "                                                datapoint[\"applied_rule\"] += \"8.5 permission was applied, \"\n",
    "                                               # coverage and precision quite high\n",
    "                                                \n",
    "                                            else: \n",
    "                                                 datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                                            if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                                if re_morphmatch(morphs, morph=\"^(ㄹ수있|을수있|ㄹ수는있|을수는있|ㄹ수도있|을수도있|ㄹ수가있|을수가있|ㄹ수없|ㄹ수도없|을수없|ㄹ수는없|을수는없|ㄹ수밖에없|을수밖에없|ㄴ수밖에없|는수밖에없|ㄹ수가없|을수가없|ㄹ리가없|을리가없|ㄹ필요는없|을필요는없|ㄹ필요가없|ㄹ필요없|지않아도되|을필요없|ㄹ필요가있|을필요가있|ㄹ뻔하|을뻔하)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction/plus\"\n",
    "                                                    datapoint[\"rule_order\"] = \"182\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.1 possibility(plus,total) was applied, \"\n",
    "                                                    # coverage and precision high\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(나하|ㄹ뻔하|을뻔하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"183\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.2.1 possibility was applied, \"\n",
    "                                                    # coverage low, 4 cases\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수있|을수있|ㄹ수는있|을수는있|ㄹ수도있|을수도있|ㄹ수가있|을수가있)$\", pos=\"^X$\") and re_morphmatch(morphs, morph=\"^(겠)$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"184\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.2.2 possibility was applied, \"\n",
    "                                                    # nichts getroffen\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수있|을수있|ㄹ수는있|을수는있|ㄹ수도있|을수도있|ㄹ수가있|을수가있)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"185\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.2.3 possibility was applied, \"\n",
    "                                                    # coverage ok\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수없|ㄹ수도없|을수없|ㄹ수는없|을수는없|ㄹ수밖에없|을수밖에없|ㄴ수밖에없|는수밖에없|ㄹ수가없|을수가없|ㄹ리가없|을리가없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                    datapoint[\"group\"] = \"impossibility\"\n",
    "                                                    datapoint[\"rule\"] = \"impossibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"186\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.3 impossibility was applied, \"\n",
    "                                                    # coverage ok, some cases are off because of the ambiguity between first and second person, hard even for human\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수없|ㄹ수도없|을수없|ㄹ수는없|을수는없|ㄹ수밖에없|을수밖에없|ㄴ수밖에없|는수밖에없|ㄹ수가없|을수가없|ㄹ리가없|을리가없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                    datapoint[\"group\"] = \"unnecessary\"\n",
    "                                                    datapoint[\"rule\"] = \"unnecessary periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"187\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.4 Unnecessary was applied, \"\n",
    "                                                    # nichts getroffen \n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수있을것같|을수있을것같|ㄹ수있다는것이|을수있다는것이|ㄹ수있을것이|을수있을것이)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"188\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.5.1 possibility+guess(plus) was applied, \"\n",
    "                                                    # coverage ok\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수있을것같|을수있을것같|ㄹ수있다는것이|을수있다는것이|ㄹ수있을것이|을수있을것이)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "                                                    datapoint[\"rule_order\"] = \"189\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.5.2 possibility+guess was applied, \"\n",
    "                                                    # coverage and precision low, 9 cases\n",
    "                                                elif re_morphmatch(morphs, morph=\"^(ㄹ수있|을수있|ㄹ수는있|을수는있|ㄹ수도있|을수도있|ㄹ수가있|을수가있|ㄹ수없|ㄹ수도없|을수없|ㄹ수는없|을수는없|ㄹ수밖에없|을수밖에없|ㄴ수밖에없|는수밖에없|ㄹ수가없|을수가없|ㄹ리가없|을리가없|ㄹ필요는없|을필요는없|ㄹ필요가없|ㄹ필요없|을필요없|지않아도되|ㄹ필요가있|을필요가있|ㄹ뻔하|을뻔하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                    datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                    datapoint[\"group\"] = \"possibility\"\n",
    "                                                    datapoint[\"rule\"] = \"possibility periphrastic construction/plus\"\n",
    "                                                    datapoint[\"rule_order\"] = \"190\"\n",
    "                                                    datapoint[\"applied_rule\"] += \"9.6 possibility was applied, \"\n",
    "                                                    # coverage quite high, some cases are off because of the ambiguity between first and second person, hard even for human\n",
    "                                                    \n",
    "#                                                 if re_morphmatch(morphs, morph=\"^(을수있|을수가있|을수도있|을수는있|ㄹ수는있|ㄹ수있을것같|ㄹ수가있|ㄹ수도있|ㄹ수는있|ㄹ수있다는것이|을수있을것이|ㄹ수있을것이|을수없|을수가없|을수는없|ㄹ수없|ㄹ수가없|을수가없|ㄹ수는없|는수밖에없|ㄹ수밖에없|을뻔하|ㄹ뻔하)$\", pos=\"^X$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "#                                                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                                                     datapoint[\"group\"] = \"possibility\"\n",
    "#                                                     datapoint[\"rule\"] = \"possibility periphrastic construction/plus\"\n",
    "#                                                     datapoint[\"applied_rule\"] += \"9.1 possibility(plus) was applied, \"\n",
    "#                                                 elif re_morphmatch(morphs, morph=\"^(을뻔하|ㄹ뻔하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                                                     datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                                                     datapoint[\"group\"] = \"possibility\"\n",
    "#                                                     datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "#                                                     datapoint[\"applied_rule\"] += \"9.2 possibility was applied, \"\n",
    "#                                                 elif re_morphmatch(morphs, morph=\"^(을수있|을수도있|을수는있|ㄹ수는있|ㄹ수있을것같|ㄹ수도있|ㄹ수는있|ㄹ수있다는것이|을수있을것이|ㄹ수있을것이|ㄹ수있을것이|을수없|을수는없|ㄹ수없|ㄹ수는없|는수밖에없|ㄹ수밖에없|을뻔하|ㄹ뻔하)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                                                     datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "#                                                     datapoint[\"group\"] = \"possibility\"\n",
    "#                                                     datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "#                                                     datapoint[\"applied_rule\"] += \"9.3 possibility was applied, \"\n",
    "#                                                     # precision not that high (사람도 context 없이 어려움), 을수가/ㄹ수가 를 여기 rule에서 제외해보기 1sh+의 특징일 수도\n",
    "#                                                 elif re_morphmatch(morphs, morph=\"^(을수있|을수가있|을수도있|을수는있|ㄹ수는있|ㄹ수있을것같|ㄹ수가있|ㄹ수도있|ㄹ수는있|ㄹ수있다는것이|을수있을것이|ㄹ수있을것이|ㄹ수있을것이|을수없|을수가없|을수는없|ㄹ수없|ㄹ수가없|을수가없|ㄹ수는없|는수밖에없|ㄹ수밖에없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "#                                                     datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                                                     datapoint[\"group\"] = \"possibility\"\n",
    "#                                                     datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "#                                                     datapoint[\"applied_rule\"] += \"9.4 possibility was applied, \"\n",
    "#                                                 elif re_morphmatch(morphs, morph=\"^(을수있|을수가있|을수도있|을수는있|ㄹ수는있|ㄹ수있을것같|ㄹ수가있|ㄹ수도있|ㄹ수는있|ㄹ수있다는것이|을수있을것이|ㄹ수있을것이|ㄹ수있을것이|을수없|을수가없|을수는없|ㄹ수없|ㄹ수가없|을수가없|ㄹ수는없|는수밖에없|ㄹ수밖에없)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "#                                                     datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "#                                                     datapoint[\"group\"] = \"possibility\"\n",
    "#                                                     datapoint[\"rule\"] = \"possibility periphrastic construction\"\n",
    "#                                                     datapoint[\"applied_rule\"] += \"9.5 possibility was applied, \"\n",
    "                                                    \n",
    "                                                else: \n",
    "                                                    datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                                                if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                                    if re_morphmatch(morphs, morph=\"^(아주|어주|주|가져다주|찍어주|안해주|틀어주|도와주|넣어주|거슬러주)$\", pos=\"^(X|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"benefactive\"\n",
    "                                                        datapoint[\"rule\"] = \"modality auxiliary verb/plus\"\n",
    "                                                        datapoint[\"rule_order\"] = \"191\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.1 benefactive(plus) was applied, \"\n",
    "                                                        # coverage and precision high\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아놓|어놓|열어놓|정해놓|걸어놓)$\", pos=\"^(X|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb/plus\"\n",
    "                                                        datapoint[\"rule_order\"] = \"192\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.2 subjectivication(plus) was applied, \"\n",
    "                                                        # coverage low, two cases\n",
    "                                                    elif \"다놓\" in datapoint[\"verb\"] and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb/plus\"\n",
    "                                                        datapoint[\"rule_order\"] = \"193\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.3 subjectivication(plus) was applied, \"\n",
    "                                                        # nichts getroffen\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아두|어두|걸어두|세워두|놓아두|맡겨두|넣어두)$\", pos=\"^(X|V)$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb/plus\"\n",
    "                                                        datapoint[\"rule_order\"] = \"194\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.4 subjectivication(plus) was applied, \"\n",
    "                                                        # coverage low, 3 cases\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아놓|어놓|열어놓|정해놓|걸어놓)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                        datapoint[\"rule_order\"] = \"195\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.5 subjectivication(plus) was applied, \"\n",
    "                                                        # coverage very low, 1 case\n",
    "                                                    elif \"다놓\" in datapoint[\"verb\"] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                        datapoint[\"rule_order\"] = \"196\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.6 subjectivication was applied, \"\n",
    "                                                        # nichts getroffen\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아두|어두|걸어두|세워두|놓아두|맡겨두|넣어두)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                        datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                        datapoint[\"rule_order\"] = \"197\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.7 subjectivication was applied, \"\n",
    "                                                        # coverage very low, one case, even off\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아주|어주|주|가져다주|찍어주|안해주|틀어주|도와주|넣어주|거슬러주)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"benefactive\"\n",
    "                                                        datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                                        datapoint[\"rule_order\"] = \"198\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.8 benefactive was applied, \"\n",
    "                                                        # coverage ok, some cases are off because of the ambiguity between second person and general e.g. 기름은 넣어주나요 ?\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아주|어주|주|가져다주|찍어주)$\", pos=\"^(X|V)$\") and re_morphmatch(morphs, morph=\"^어야하$\", pos=\"^X$\"):\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"obligation statement\"\n",
    "                                                        datapoint[\"rule\"] = \"obligation periphrastic construction\"\n",
    "                                                        datapoint[\"rule_order\"] = \"199\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.9 benefactive was applied, \"\n",
    "                                                        # nichts getroffen\n",
    "                                                    elif re_morphmatch(morphs, morph=\"^(아보|어보|가보|해보|써보|타보|찾아보|재보)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                        datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                        datapoint[\"group\"] = \"try\"\n",
    "                                                        datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                                        datapoint[\"rule_order\"] = \"200\"\n",
    "                                                        datapoint[\"applied_rule\"] += \"10.10 try was applied, \"\n",
    "                                                        # nichts getroffen\n",
    "                                                    else: \n",
    "                                                        datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "\n",
    "                                                    if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                                        if re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                                            datapoint[\"group\"] = \"guess/future/will\"\n",
    "                                                            datapoint[\"rule\"] = \"guess/future/will infix\"\n",
    "                                                            datapoint[\"rule_order\"] = \"201\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.1 guess/future/will(plus) was applied, \"\n",
    "                                                            # coverage ok\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^겠$\", pos=\"^E$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                                                            datapoint[\"group\"] = \"guess/future/will\"\n",
    "                                                            datapoint[\"rule\"] = \"guess/future/will infix\"\n",
    "                                                            datapoint[\"rule_order\"] = \"202\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.2 guess/future/will was applied, \"\n",
    "                                                            # coverage ok, some cases off because of the issue with the definition of subject e.g. 정말 매력적이겠네요 . \n",
    "                                                        elif \"겠습니까\" in datapoint[\"verb\"] and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"2sh+\"\n",
    "                                                            datapoint[\"group\"] = \"will\"\n",
    "                                                            datapoint[\"rule\"] = \"will infix\"\n",
    "                                                            datapoint[\"rule_order\"] = \"203\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.3 will was applied, \"\n",
    "                                                            # coverage low, 4 cases, even one off\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^(아놓|어놓|열어놓|정해놓|걸어놓)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                            datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                            datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                            datapoint[\"rule_order\"] = \"204\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.4 subjectivication was applied, \"\n",
    "                                                            # coverage ok, some cases off because of the general subject e.g. 네, 입구 쪽에 테이블을 쭉 펼쳐놨어요 .\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^(아두|어두|걸어두|세워두|놓아두|맡겨두|넣어두)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                            datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                            datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                            datapoint[\"rule_order\"] = \"205\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.5 subjectivication was applied, \"\n",
    "                                                            # coverage ok\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^(아주|어주|주|가져다주|찍어주|안해주|틀어주|도와주|넣어주|거슬러주)$\", pos=\"^(X|V)$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                            datapoint[\"group\"] = \"benefactive\"\n",
    "                                                            datapoint[\"rule\"] = \"modality auxiliary verb\"\n",
    "                                                            datapoint[\"rule_order\"] = \"206\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.6 guess was applied, \"\n",
    "                                                            # coverage low, precision very low\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^어다$\", pos=\"^E$\") and re_morphmatch(morphs, morph=\"^놓$\", pos=\"^V$\"):\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                            datapoint[\"group\"] = \"subjectivication\"\n",
    "                                                            datapoint[\"rule\"] = \"aspect auxiliary verb\"\n",
    "                                                            datapoint[\"rule_order\"] = \"207\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.7 subjectivication was applied, \"\n",
    "                                                            # nichts getroffen\n",
    "                                                        elif re_morphmatch(morphs, morph=\"^(을것같|ㄹ것같|는것같|ㄴ것같|은것같|은듯하|ㄴ거같|을거같|ㄹ거같|는거같|던것같)$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                            datapoint[\"subject_by_rule\"] = \"1sh+\"\n",
    "                                                            datapoint[\"group\"] = \"guess\"\n",
    "                                                            datapoint[\"rule\"] = \"guess periphrastic construction\"\n",
    "                                                            datapoint[\"rule_order\"] = \"208\"\n",
    "                                                            datapoint[\"applied_rule\"] += \"11.8 guess was applied, \"\n",
    "                                                            # coverage는 높지만 여전히 subject가 무엇인지의 정의에 따라 정답률이 달라질 수 있음\n",
    "                                                                                                                        \n",
    "                                                        else: \n",
    "                                                            datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "                                                        \n",
    "                                                        exclude_list = [\"dobj\", \"modi\", \"scom\", \"subj\"]\n",
    "                                                        if datapoint[\"subject_by_rule\"] == \"N/A\":\n",
    "                                                            if re_morphmatch(morphs, morph=\"^고있$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                                                                datapoint[\"group\"] = \"progress\"\n",
    "                                                                datapoint[\"rule\"] = \"progress aspect\"\n",
    "                                                                datapoint[\"rule_order\"] = \"209\"\n",
    "                                                                datapoint[\"applied_rule\"] += \"12.1 progress was applied, \"\n",
    "                                                                # coverage quite high, but some off cases because of general subejct and third person\n",
    "                                                            elif re_morphmatch(morphs, morph=\"^고있$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"ques\":\n",
    "                                                                datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                                                datapoint[\"group\"] = \"progress\"\n",
    "                                                                datapoint[\"rule\"] = \"progress aspect\"\n",
    "                                                                datapoint[\"rule_order\"] = \"210\"\n",
    "                                                                datapoint[\"applied_rule\"] += \"12.2 progress was applied, \"\n",
    "                                                                # coverage ok, but some off cases because of general subejct and third person\n",
    "                                                            elif re_morphmatch(morphs, morph=\"^게되$\", pos=\"^X$\") and datapoint[\"zp_mood\"] == \"decl\":\n",
    "                                                                datapoint[\"subject_by_rule\"] = \"1sh+\" \n",
    "                                                                datapoint[\"group\"] = \"passive\"\n",
    "                                                                datapoint[\"rule\"] = \"passive aspect\"\n",
    "                                                                datapoint[\"rule_order\"] = \"211\"\n",
    "                                                                datapoint[\"applied_rule\"] += \"12.3 passive was applied, \"\n",
    "                                                                # coverage ok\n",
    "                                                            elif datapoint[\"zp_hono\"] == \"plus\":\n",
    "                                                                datapoint[\"subject_by_rule\"] = \"2sh+\" \n",
    "                                                                datapoint[\"group\"] = \"honorification\"\n",
    "                                                                datapoint[\"rule\"] = \"hono total\"\n",
    "                                                                datapoint[\"rule_order\"] = \"212\"\n",
    "                                                                datapoint[\"applied_rule\"] += \"12.4 hono(at the end) was applied, \"\n",
    "#                                                             elif datapoint[\"verb_syn\"] in exclude_list:\n",
    "#                                                                 datapoint[\"subject_by_rule\"] = \"none\"\n",
    "#                                                                 datapoint[\"group\"] = \"exclude_verb_syn\"\n",
    "#                                                                 datapoint[\"rule\"] = \"exclude\"\n",
    "#                                                                 datapoint[\"rule_order\"] = \"213\"\n",
    "#                                                                 datapoint[\"applied_rule\"] += \"exclude was applied, \" \n",
    "                                                            else: \n",
    "                                                                datapoint[\"subject_by_rule\"] = \"N/A\" \n",
    "                                                            \n",
    "\n",
    "\n",
    "        datapoints.append(datapoint) \n",
    "        \n",
    "        print(\"{:.2f}%\\r\".format(100*i/maxn), end=\"\")  \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(17266, 22)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etri_sent</th>\n",
       "      <th>sent</th>\n",
       "      <th>verb</th>\n",
       "      <th>applied_rule</th>\n",
       "      <th>subject_by_rule</th>\n",
       "      <th>zero_referents</th>\n",
       "      <th>antecedent</th>\n",
       "      <th>group</th>\n",
       "      <th>rule_order</th>\n",
       "      <th>rule</th>\n",
       "      <th>connective</th>\n",
       "      <th>base_morpheme</th>\n",
       "      <th>base_morpheme_second</th>\n",
       "      <th>connective_true</th>\n",
       "      <th>zp_hono</th>\n",
       "      <th>zp_mood</th>\n",
       "      <th>sent_dep</th>\n",
       "      <th>zp_markable_ID</th>\n",
       "      <th>zero_type</th>\n",
       "      <th>tense_zp</th>\n",
       "      <th>verb_syn</th>\n",
       "      <th>verb_head</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>121</td>\n",
       "      <td>산은 너무 멀고, 이쪽 길로 가면 숲을 지나서 돌아올수있어요 .</td>\n",
       "      <td>지나서</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>2sh+</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>지나</td>\n",
       "      <td>서</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[subj, modi, clau, modi, modi, clau, dobj, cla...</td>\n",
       "      <td>markable_5010</td>\n",
       "      <td>extra</td>\n",
       "      <td>N/A</td>\n",
       "      <td>clau</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>253</td>\n",
       "      <td>갇혀있으니 너무 무서워요 .</td>\n",
       "      <td>무서워요</td>\n",
       "      <td>5.13.2 feeling was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>experience</td>\n",
       "      <td>152</td>\n",
       "      <td>feeling predicate</td>\n",
       "      <td>N/A</td>\n",
       "      <td>무섭</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>decl</td>\n",
       "      <td>[clau, modi, root, punc]</td>\n",
       "      <td>markable_5131</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>396</td>\n",
       "      <td>현재 치료 중이 아니시면 괜찮으시구요, 귀마개를 착용하시는 편이 더 좋겠네요 .</td>\n",
       "      <td>괜찮으시구요,</td>\n",
       "      <td>clau(plus, at the end) was applied,</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>2sh+</td>\n",
       "      <td></td>\n",
       "      <td>clau</td>\n",
       "      <td>23</td>\n",
       "      <td>clau_plus</td>\n",
       "      <td>N/A</td>\n",
       "      <td>괜찮</td>\n",
       "      <td>N/A</td>\n",
       "      <td>1</td>\n",
       "      <td>plus</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[modi, modi, scom, clau, clau, dobj, modi, sub...</td>\n",
       "      <td>markable_5252</td>\n",
       "      <td>extra</td>\n",
       "      <td>N/A</td>\n",
       "      <td>clau</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>586</td>\n",
       "      <td>그럼 담요를 준비해드릴까요 ?</td>\n",
       "      <td>준비해드릴까요</td>\n",
       "      <td>3.1.2 suggestory was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>suggestory formula</td>\n",
       "      <td>92</td>\n",
       "      <td>suggestory verbal suffix</td>\n",
       "      <td>N/A</td>\n",
       "      <td>준비하</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>ques</td>\n",
       "      <td>[modi, dobj, root, punc]</td>\n",
       "      <td>markable_5373</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>784</td>\n",
       "      <td>준비해드리겠습니다 .</td>\n",
       "      <td>준비해드리겠습니다</td>\n",
       "      <td>2.2 want was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>want statement</td>\n",
       "      <td>64</td>\n",
       "      <td>want verbal suffix</td>\n",
       "      <td>N/A</td>\n",
       "      <td>준비하</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>decl</td>\n",
       "      <td>[root, punc]</td>\n",
       "      <td>markable_5494</td>\n",
       "      <td>extra</td>\n",
       "      <td>futu</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17261</th>\n",
       "      <td>8455</td>\n",
       "      <td>불편을 끼쳐드려 죄송합니다 .</td>\n",
       "      <td>끼쳐드려</td>\n",
       "      <td>clau(various_deuli) was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1ph+</td>\n",
       "      <td></td>\n",
       "      <td>clau</td>\n",
       "      <td>36</td>\n",
       "      <td>clau_various</td>\n",
       "      <td>class_A</td>\n",
       "      <td>끼치</td>\n",
       "      <td>어드리</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[dobj, clau, root, punc]</td>\n",
       "      <td>markable_5080</td>\n",
       "      <td>extra</td>\n",
       "      <td>N/A</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17262</th>\n",
       "      <td>8458</td>\n",
       "      <td>영화채널도 있는지 궁금합니다 .</td>\n",
       "      <td>궁금합니다</td>\n",
       "      <td>5.4 cognition was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>experience</td>\n",
       "      <td>142</td>\n",
       "      <td>cognition predicate</td>\n",
       "      <td>N/A</td>\n",
       "      <td>궁금하</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>decl</td>\n",
       "      <td>[subj, clau, root, punc]</td>\n",
       "      <td>markable_5083</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17263</th>\n",
       "      <td>8460</td>\n",
       "      <td>유료채널을 보려면 어떻게 해야합니까 ?</td>\n",
       "      <td>보려면</td>\n",
       "      <td></td>\n",
       "      <td>N/A</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>보</td>\n",
       "      <td>려면</td>\n",
       "      <td>1</td>\n",
       "      <td>N/A</td>\n",
       "      <td>N/A</td>\n",
       "      <td>[dobj, clau, modi, root, punc]</td>\n",
       "      <td>markable_5084</td>\n",
       "      <td>extra</td>\n",
       "      <td>N/A</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17264</th>\n",
       "      <td>8456</td>\n",
       "      <td>룸에 비치된 TV로 외국 채널을 볼수있을까요 ?</td>\n",
       "      <td>볼수있을까요</td>\n",
       "      <td>2.5 permission was applied,</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>1sh+</td>\n",
       "      <td></td>\n",
       "      <td>permission</td>\n",
       "      <td>70</td>\n",
       "      <td>permission periphrastic construction</td>\n",
       "      <td>N/A</td>\n",
       "      <td>보</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>N/A</td>\n",
       "      <td>ques</td>\n",
       "      <td>[modi, modi, modi, modi, dobj, root, punc]</td>\n",
       "      <td>markable_5081</td>\n",
       "      <td>extra</td>\n",
       "      <td>futu</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17265</th>\n",
       "      <td>8457</td>\n",
       "      <td>예, CCTV - 4, CNN, BBC 등을 보실수있습니다 .</td>\n",
       "      <td>보실수있습니다</td>\n",
       "      <td>4.7 sensation(plus) was applied,</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>2sh+</td>\n",
       "      <td></td>\n",
       "      <td>experience</td>\n",
       "      <td>105</td>\n",
       "      <td>sensation predicate</td>\n",
       "      <td>N/A</td>\n",
       "      <td>보</td>\n",
       "      <td>N/A</td>\n",
       "      <td>0</td>\n",
       "      <td>plus</td>\n",
       "      <td>decl</td>\n",
       "      <td>[modi, none, modi, modi, modi, modi, dobj, roo...</td>\n",
       "      <td>markable_5082</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>root</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17266 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      etri_sent                                          sent       verb  \\\n",
       "0           121           산은 너무 멀고, 이쪽 길로 가면 숲을 지나서 돌아올수있어요 .        지나서   \n",
       "1           253                               갇혀있으니 너무 무서워요 .       무서워요   \n",
       "2           396  현재 치료 중이 아니시면 괜찮으시구요, 귀마개를 착용하시는 편이 더 좋겠네요 .    괜찮으시구요,   \n",
       "3           586                              그럼 담요를 준비해드릴까요 ?    준비해드릴까요   \n",
       "4           784                                   준비해드리겠습니다 .  준비해드리겠습니다   \n",
       "...         ...                                           ...        ...   \n",
       "17261      8455                              불편을 끼쳐드려 죄송합니다 .       끼쳐드려   \n",
       "17262      8458                             영화채널도 있는지 궁금합니다 .      궁금합니다   \n",
       "17263      8460                         유료채널을 보려면 어떻게 해야합니까 ?        보려면   \n",
       "17264      8456                    룸에 비치된 TV로 외국 채널을 볼수있을까요 ?     볼수있을까요   \n",
       "17265      8457            예, CCTV - 4, CNN, BBC 등을 보실수있습니다 .    보실수있습니다   \n",
       "\n",
       "                               applied_rule subject_by_rule zero_referents  \\\n",
       "0                                                       N/A           2sh+   \n",
       "1              5.13.2 feeling was applied,             1sh+           1sh+   \n",
       "2      clau(plus, at the end) was applied,             2sh+           2sh+   \n",
       "3            3.1.2 suggestory was applied,             1sh+           1sh+   \n",
       "4                    2.2 want was applied,             1sh+           1sh+   \n",
       "...                                     ...             ...            ...   \n",
       "17261     clau(various_deuli) was applied,             1sh+           1ph+   \n",
       "17262           5.4 cognition was applied,             1sh+           1sh+   \n",
       "17263                                                   N/A           1sh+   \n",
       "17264          2.5 permission was applied,             1sh+           1sh+   \n",
       "17265     4.7 sensation(plus) was applied,             2sh+           2sh+   \n",
       "\n",
       "      antecedent               group rule_order  \\\n",
       "0                                NaN        NaN   \n",
       "1                         experience        152   \n",
       "2                               clau         23   \n",
       "3                 suggestory formula         92   \n",
       "4                     want statement         64   \n",
       "...          ...                 ...        ...   \n",
       "17261                           clau         36   \n",
       "17262                     experience        142   \n",
       "17263                            NaN        NaN   \n",
       "17264                     permission         70   \n",
       "17265                     experience        105   \n",
       "\n",
       "                                       rule connective base_morpheme  \\\n",
       "0                                       NaN    class_A            지나   \n",
       "1                         feeling predicate        N/A            무섭   \n",
       "2                                 clau_plus        N/A            괜찮   \n",
       "3                  suggestory verbal suffix        N/A           준비하   \n",
       "4                        want verbal suffix        N/A           준비하   \n",
       "...                                     ...        ...           ...   \n",
       "17261                          clau_various    class_A            끼치   \n",
       "17262                   cognition predicate        N/A           궁금하   \n",
       "17263                                   NaN    class_A             보   \n",
       "17264  permission periphrastic construction        N/A             보   \n",
       "17265                   sensation predicate        N/A             보   \n",
       "\n",
       "      base_morpheme_second  connective_true zp_hono zp_mood  \\\n",
       "0                        서                1     N/A     N/A   \n",
       "1                      N/A                0     N/A    decl   \n",
       "2                      N/A                1    plus     N/A   \n",
       "3                      N/A                0     N/A    ques   \n",
       "4                      N/A                0     N/A    decl   \n",
       "...                    ...              ...     ...     ...   \n",
       "17261                  어드리                1     N/A     N/A   \n",
       "17262                  N/A                0     N/A    decl   \n",
       "17263                   려면                1     N/A     N/A   \n",
       "17264                  N/A                0     N/A    ques   \n",
       "17265                  N/A                0    plus    decl   \n",
       "\n",
       "                                                sent_dep zp_markable_ID  \\\n",
       "0      [subj, modi, clau, modi, modi, clau, dobj, cla...  markable_5010   \n",
       "1                               [clau, modi, root, punc]  markable_5131   \n",
       "2      [modi, modi, scom, clau, clau, dobj, modi, sub...  markable_5252   \n",
       "3                               [modi, dobj, root, punc]  markable_5373   \n",
       "4                                           [root, punc]  markable_5494   \n",
       "...                                                  ...            ...   \n",
       "17261                           [dobj, clau, root, punc]  markable_5080   \n",
       "17262                           [subj, clau, root, punc]  markable_5083   \n",
       "17263                     [dobj, clau, modi, root, punc]  markable_5084   \n",
       "17264         [modi, modi, modi, modi, dobj, root, punc]  markable_5081   \n",
       "17265  [modi, none, modi, modi, modi, modi, dobj, roo...  markable_5082   \n",
       "\n",
       "      zero_type tense_zp verb_syn  verb_head  \n",
       "0         extra      N/A     clau          9  \n",
       "1         extra     pres     root          0  \n",
       "2         extra      N/A     clau          7  \n",
       "3         extra     pres     root          0  \n",
       "4         extra     futu     root          0  \n",
       "...         ...      ...      ...        ...  \n",
       "17261     extra      N/A     clau          3  \n",
       "17262     extra     pres     root          0  \n",
       "17263     extra      N/A     clau          4  \n",
       "17264     extra     futu     root          0  \n",
       "17265     extra     pres     root          0  \n",
       "\n",
       "[17266 rows x 22 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(datapoints) \n",
    "\n",
    "exclude_items = ['is_ana', 'verb_roman', 'zp_speaker','sent_morphs', \"pos\"]\n",
    "# 'is_ana' 왜인지 inter인 것의 value가 1\n",
    "\n",
    "#rearrange columns\n",
    "columns = ['etri_sent',\n",
    "           'sent',\n",
    "           'verb',\n",
    "           'applied_rule',\n",
    "           'subject_by_rule',\n",
    "           'zero_referents',\n",
    "           'antecedent',\n",
    "           'group',\n",
    "           'rule_order',\n",
    "           'rule',\n",
    "           'connective',\n",
    "           'base_morpheme',\n",
    "           'base_morpheme_second',\n",
    "           'connective_true',\n",
    "           'zp_hono',\n",
    "           'zp_mood',\n",
    "           'sent_dep',\n",
    "           'sent_morphs',\n",
    "           'verb_roman',\n",
    "           'zp_markable_ID',\n",
    "           'is_ana',\n",
    "           'zero_type',\n",
    "           'zp_speaker',\n",
    "           'tense_zp',\n",
    "           'verb_syn',\n",
    "           'verb_head']\n",
    "\n",
    "columns = [c for c in columns if not c in exclude_items]\n",
    "\n",
    "for c in list(df2):\n",
    "    if not c in columns+exclude_items:\n",
    "        print(\"Warning: did you miss column '{}'?\".format(c))\n",
    "        \n",
    "        \n",
    "df2 = df2[columns]\n",
    "#df2.to_csv(\"df2_connective_test_230323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "#print(df2.shape) # (17266, 13)\n",
    "#df.head()\n",
    "freq = df2.groupby(['group']).count() \n",
    "#print(freq)\n",
    "\n",
    "print(df2.shape) # (17266, 20)\n",
    "\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 281023\n",
    "\n",
    "#df2.to_csv(\"proposed_conjunction_change_281023.csv\", encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17266, 23)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. exclude 제거\n",
    "# 2. error_category 제거\n",
    "# 3. subject error 찾아서 group1 final 만들기\n",
    "# 4. group 2 적용\n",
    "# 5. error analysis 만들기\n",
    "\n",
    "# 1. exclude 제거\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# df2_exclude에 Filter 열 추가 \n",
    "conditionlist = [\n",
    "    (df2['subject_by_rule'] != 'N/A') & (df2['verb_syn'] == 'root'),\n",
    "    (df2['subject_by_rule'] != 'N/A') & (df2['verb_syn'] != 'root'),\n",
    "    (df2['subject_by_rule'] == 'N/A')] \n",
    "choicelist = ['A', 'B', 'C']\n",
    "\n",
    "df2['Filter'] = np.select(conditionlist, choicelist, default='Not Specified')\n",
    "\n",
    "df2.shape # (17266, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2555\n",
      "1384\n"
     ]
    }
   ],
   "source": [
    "# Filter 추가한 df2에서 각 Filter별로 제외돼야 하는 case 먼저 제외 -> df2_exclude 만들기\n",
    "\n",
    "#exclude_A\n",
    "\n",
    "exclude_A = pd.read_excel('Filter A_null_count_2_to_1_Error_27042022.xlsx') \n",
    "exclude_A_idx = exclude_A['idx']\n",
    "exclude_A_idx_val = exclude_A_idx.values \n",
    "exclude_A_idx_val_list = exclude_A_idx_val.tolist() \n",
    "\n",
    "exclude_A_rest = pd.read_excel('Filter A_Error_rest_28042022.xlsx') \n",
    "exclude_A_rest_idx = exclude_A_rest['idx']\n",
    "exclude_A_rest_idx_val = exclude_A_rest_idx.values \n",
    "exclude_A_rest_idx_val_list = exclude_A_rest_idx_val.tolist() # 86개의 제외될 predicate의 idx의 list\n",
    "\n",
    "#exclude_B\n",
    "\n",
    "exclude_B = pd.read_excel('Filter B_Error_28042022.xlsx') \n",
    "exclude_B_idx = exclude_B['idx']\n",
    "exclude_B_idx_val = exclude_B_idx.values \n",
    "exclude_B_idx_val_list = exclude_B_idx_val.tolist() # 111개의 제외될 predicate의 idx의 list\n",
    "\n",
    "#exclude_C\n",
    "\n",
    "exclude_C = pd.read_excel('Filter C_Error_28042022.xlsx') \n",
    "exclude_C_idx = exclude_C['idx']\n",
    "exclude_C_idx_val = exclude_C_idx.values \n",
    "exclude_C_idx_val_list = exclude_C_idx_val.tolist() # 647 \n",
    "\n",
    "#exclude_add\n",
    "\n",
    "exclude_add = pd.read_excel('case2A_B_C_02052022.xlsx', sheet_name = 'exclude') \n",
    "exclude_add_idx = exclude_add['idx']\n",
    "exclude_add_idx_val = exclude_add_idx.values \n",
    "exclude_add_idx_val_list = exclude_add_idx_val.tolist()\n",
    "\n",
    "#exclude_group2\n",
    "exclude_group2 = pd.read_excel('rule_group2_analysis.xlsx', sheet_name = 'exclude') \n",
    "exclude_group2_idx = exclude_group2['idx']\n",
    "exclude_group2_idx_val = exclude_group2_idx.values \n",
    "exclude_group2_idx_val_list = exclude_group2_idx_val.tolist() # 230522 더 추가함!!\n",
    "\n",
    "# print(len(exclude_group2_idx_val_list)) # 217\n",
    "\n",
    "# exclude_total\n",
    "exclude_total = pd.read_csv('exclude_total_excel_06052022.csv') # exclude가 반영이 안됨!\n",
    "exclude_total_idx = exclude_total['idx']\n",
    "exclude_total_idx_val = exclude_total_idx.values \n",
    "exclude_total_idx_val_list = exclude_total_idx_val.tolist() # 1,152\n",
    "#print(len(set(exclude_total_idx_val_list))) # 1,145\n",
    "\n",
    "exclude_total_idx_list = exclude_A_idx_val_list + exclude_A_rest_idx_val_list + exclude_B_idx_val_list + exclude_C_idx_val_list + exclude_add_idx_val_list + exclude_total_idx_val_list + exclude_group2_idx_val_list\n",
    "exclude_total_idx_list_set = set(exclude_total_idx_list)\n",
    "\n",
    "print(len(exclude_total_idx_list)) # 2555\n",
    "print(len(exclude_total_idx_list_set)) # 1384 \n",
    "\n",
    "df2_exclude = df2[df2.index.isin(exclude_total_idx_list_set) == False]\n",
    "df2_exclude.shape #(15883, 21) (17266-15883 = 1383)\n",
    "\n",
    "# df2_exclude.to_csv(\"NRS1_implement_150224.csv\", encoding = \"utf-8-sig\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2555, 3)\n",
      "1383\n"
     ]
    }
   ],
   "source": [
    "# exclude_total_idx_list_set index와 df2_exclude_True index의 difference를 찾아보기\n",
    "df2_exclude_True = df2[df2.index.isin(exclude_total_idx_list_set) == True]\n",
    "# print(df2_exclude_True.shape) # (1383, 21)\n",
    "\n",
    "# 위의 seperate된 exclude 관련 파일을 하나로 모아서, df2_exclude_True의 index만 지닌 애들을 뽑아서 exclude_total_idx_list_set과 비교하면 \n",
    "# 2개의 차이가 무엇인지도 알 수 있고, exclude에 대한 error 내기도 더 편함\n",
    "# \"exclude_error_compilation_130922.xlsx\"\n",
    "\n",
    "exclude_all = pd.read_excel(\"exclude_error_compilation_130922.xlsx\", sheet_name = None, index_col=0)\n",
    "\n",
    "exclude_all_df = pd.concat(exclude_all)\n",
    "\n",
    "print(exclude_all_df.shape) #(2555, 4) duplicates 제거 해야 함\n",
    "# exclude_all_df.to_csv(\"exclude_all_df_130922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# \"exclude_all_df_dedupilicate_130922.csv\" 1383개 1개의 차이가 뭔지 index 비교해보기 (굳이 필요 없음)\n",
    "\n",
    "exclude_all_dedup_review_df = pd.read_csv(\"exclude_all_df_review_140922.csv\")\n",
    "\n",
    "exclude_all_dedup_review_df_idx = exclude_all_dedup_review_df.index.tolist()\n",
    "print(len(exclude_all_dedup_review_df_idx)) # 1383\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1383\n"
     ]
    }
   ],
   "source": [
    "# exclude_all_dedup_df\"를 error를 기준으로 freq 찾고, 표로 만들고 예문 추가\n",
    "error_freq = exclude_all_dedup_review_df['error'].value_counts() \n",
    "print(exclude_all_dedup_review_df['error'].size) # 1383 richtig\n",
    "\n",
    "# error_freq.to_csv(\"error_frequence_140922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2sh+    6739\n",
      "1sh+    5386\n",
      "N/A     2045\n",
      "none    1669\n",
      "1ph-      44\n",
      "Name: subject_by_rule, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# df2_exclude (15883) 우선 csv로 만들기! \n",
    "# df2_exclude.to_csv(\"df2_exclude_061022.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_exclude.to_csv(\"df2_exclude_230322.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 전체 15883\n",
    "subject_by_rule_freq = df2_exclude['subject_by_rule'].value_counts() \n",
    "print(subject_by_rule_freq)\n",
    "\n",
    "\n",
    "subject_by_rule_group_by_freq = df2_exclude.groupby(['subject_by_rule','antecedent']).count() \n",
    "\n",
    "# subject_by_rule_freq.to_csv(\"subject_by_rule_freq_180922.csv\", encoding = \"utf-8-sig\") \n",
    "# subject_by_rule_group_by_freq.to_csv(\"subject_by_rule_group_by_freq_051022.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://wotres.tistory.com/entry/%EB%B6%84%EB%A5%98-%EC%84%B1%EB%8A%A5-%EC%B8%A1%EC%A0%95%ED%95%98%EB%8A%94%EB%B2%95-Accuracy-Precision-Recall-F1-score-ROC-AUC-in-python\n",
    "\n",
    "# Precision (실제 양성수 (할당한 주어가 실제 맞는 경우)/ 양성이라고 판단한 수 (특정 주어라고 판단한 수)) 모델이 얼마나 정밀한가, 진짜 양성만을 잘 고르는가?\n",
    "# TP / TP+FP\n",
    "\n",
    "df2_zero_referent_count = df2_exclude.groupby([\"subject_by_rule\",\"zero_referents\"])[\"zero_referents\"].count()\n",
    "df2_antecedent_count = df2_exclude.groupby([\"subject_by_rule\",\"antecedent\"])[\"antecedent\"].count()\n",
    "# df2_zero_referent_antecedent_count = df2_exclude.groupby([\"subject_by_rule\",\"zero_referents\",\"antecedent\"])[\"zero_referents\",\"antecedent\"].count()\n",
    "\n",
    "\n",
    "# df2_antecedent = df2_exclude[\"antecedent\"].groupby(df2_exclude[\"subject_by_rule\"])\n",
    "# df2_antecedent_dict = dict(list(df2_antecedent))\n",
    "\n",
    "# df2_antecedent\n",
    "\n",
    "# df2_zero_referent_count.to_csv(\"df2_zero_referent_count_051022.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_antecedent_count.to_csv(\"df2_antecedent_count_051022.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11423\n",
      "(11423, 23)\n",
      "(5386, 23)\n",
      "(6739, 23)\n",
      "(195, 23)\n",
      "(39, 23)\n",
      "195\n",
      "39\n",
      "11657\n",
      "(4226, 23)\n",
      "134\n",
      "(134, 23)\n",
      "11791\n"
     ]
    }
   ],
   "source": [
    "# df2_exclude에서 바로 규칙 정확도 계산해보기 180922 이걸로 다시 표 만들기 \n",
    "# Accuracy: 전체 샘플 중 맞게 예측한 샘플 수 (TP+TN / Total)\n",
    "\n",
    "# 1. simple string match (case1)\n",
    "\n",
    "df2_case1 = df2_exclude[df2_exclude.subject_by_rule == df2_exclude.zero_referents] \n",
    "df2_case1_idx = df2_case1.index.tolist()\n",
    "print(len(df2_case1_idx)) # 11422\n",
    "\n",
    "print(df2_case1.shape) # (11422, 21)\n",
    "\n",
    "# df2_case1.to_csv(\"df2_case1_180922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 2.  singular <-> plural, person match (case2) \n",
    "\n",
    "# df2_not_case1 = df2_total_group1_for_analysis[df2_total_group1_for_analysis.subject_by_rule != df2_total_group1_for_analysis.zero_referents] \n",
    "# print(df2_not_case1.shape) # (4470, 23) 15892 - 11422 = 4470 richtig\n",
    "\n",
    "# df2_not_case1_idx = df2_not_case1.index.tolist()\n",
    "# # print(len(df2_not_case1_idx)) # 4470\n",
    "\n",
    "df2_first_person = df2_exclude[df2_exclude['subject_by_rule'] == '1sh+'] \n",
    "df2_second_person = df2_exclude[df2_exclude['subject_by_rule'] == '2sh+'] \n",
    "\n",
    "print(df2_first_person.shape) #(5382, 21)\n",
    "print(df2_second_person.shape) #(6739, 21)\n",
    "\n",
    "df2_first_person_match = df2_first_person[(df2_first_person['zero_referents'] == '1ph-') | (df2_first_person['zero_referents'] == '1ph+')] \n",
    "df2_second_person_match = df2_second_person[(df2_second_person['zero_referents'] == '2ph+') | (df2_second_person['zero_referents'] == '2sh-')]\n",
    "\n",
    "print(df2_first_person_match.shape) # (195, 21)\n",
    "print(df2_second_person_match.shape) # (39, 21)\n",
    "\n",
    "df2_first_person_match_idx = df2_first_person_match.index.tolist()\n",
    "df2_second_person_match_idx = df2_second_person_match.index.tolist()\n",
    "\n",
    "print(len(df2_first_person_match_idx)) # 195\n",
    "print(len(df2_second_person_match_idx)) # 39\n",
    "\n",
    "# df2_first_person_match.to_csv(\"df2_first_person_match_180922.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_second_person_match.to_csv(\"df2_second_person_match_180922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 3. subject_by_rule과 antecedent 같은 경우 (case3) \n",
    "\n",
    "df2_referent_match_idx = df2_first_person_match_idx + df2_second_person_match_idx + df2_case1_idx\n",
    "print(len(df2_referent_match_idx)) # 11656 (195+39+11422 = 11656 richtig)\n",
    "\n",
    "df2_case3_check = df2_exclude[df2_exclude.index.isin(df2_referent_match_idx) == False]\n",
    "\n",
    "print(df2_case3_check.shape) # (4227, 21) 15883-11656 = 4227 richtig\n",
    "# df2_case3_check.to_csv(\"df2_case3_check_180922.csv\", encoding = \"utf-8-sig\") \n",
    "# 여기에서 N/A none 3718 (2049+1669 = 3718 richtig)\n",
    "# 실제로 체크해야 하는 case 509 richtig -> \"df2_case3_check_filter_for_referent_180922.csv\" (manuell 만듦)\n",
    "\n",
    "# # df2_case3_check 에서 3개의 subcases \n",
    "# # 3.1 subject_by_rule \"1sh+\" & antecedent \"{저}\", \"{저는}\", \"{저도}\", \"{저희가}\", \"{저희들도}\", \"{전}\", \"{제가}\" (total: 131)\n",
    "# # 3.2 subject_by_rule \"1ph-\" & antecedent \"{우리}\" (total: 1)\n",
    "# # 3.3 subject_by_rule \"2sh+\" # no match (total: 0)\n",
    "\n",
    "# \"df2_case3_index_180922.xlsx\" (총 134개의 index가 저장되어 있음)\n",
    "\n",
    "df2_case3 = pd.read_excel('df2_case3_index_180922.xlsx') \n",
    "df2_case3_idx = df2_case3['idx']\n",
    "df2_case3_idx_val = df2_case3_idx.values \n",
    "df2_case3_idx_val_list = df2_case3_idx_val.tolist() \n",
    "\n",
    "print(len(df2_case3_idx_val_list)) # 134 richtig\n",
    "\n",
    "\n",
    "df2_case3_antecedent_match = df2_case3_check[df2_case3_check.index.isin(df2_case3_idx_val_list) == True]\n",
    "print(df2_case3_antecedent_match.shape) # (134, 21) richtig\n",
    "\n",
    "# df2_case3_antecedent_match.to_csv(\"df2_case3_antecedent_match_260922.csv\", encoding = \"utf-8-sig\") \n",
    "referent_match_all_idx = df2_case1_idx + df2_first_person_match_idx + df2_second_person_match_idx + df2_case3_idx_val_list\n",
    "print(len(referent_match_all_idx)) # 11790 richtig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True Positive Case by class (적용된 rule 등에 대해서 분석할 때 용이하게 하기 위해서 311022)\n",
    "# df2_exclude (15883, 21)\n",
    "\n",
    "# 1. \"subject_by_rule\": 1ph-, \"zero_referents\": 1ph- (+ person matching)\n",
    "\n",
    "class1_TP_string = df2_exclude[(df2_exclude['subject_by_rule'] == '1ph-') & (df2_exclude['zero_referents'] == '1ph-')] # (42, 21)\n",
    "class1_TP_antecedent = df2_exclude[(df2_exclude['subject_by_rule'] == '1ph-') & (df2_exclude['antecedent'] == '우리')] # (1, 21)\n",
    "class1_TP = pd.concat([class1_TP_string, class1_TP_antecedent]) # (43, 21) richtig\n",
    "\n",
    "\n",
    "# 2. \"subject_by_rule\": 1sh+, \"zero_referents\": 1sh+ (+ person matching, antecedent matching, from false negative)\n",
    "\n",
    "class2_TP_string = df2_exclude[(df2_exclude['subject_by_rule'] == '1sh+') & (df2_exclude['zero_referents'] == '1sh+')] # (4853, 21)\n",
    "\n",
    "class2_antecedent_list = [\"저\", \"저는\", \"저도\", \"저희가\", \"저희는\", \"저희들도\", \"전\", \"제가\"]\n",
    "class2 = df2_exclude[(df2_exclude['subject_by_rule'] == '1sh+')] # (5382, 21)\n",
    "class2_TP_antecedent = df2_exclude[(df2_exclude['subject_by_rule'] == '1sh+') & df2_exclude['antecedent'].isin(class2_antecedent_list)] # (131, 21)\n",
    "class2_TP_person_matching = class2[(class2['zero_referents'] == \"1ph-\") | (class2['zero_referents'] == \"1ph+\")] # (195, 21)\n",
    "class2_FN_idx_list = [2513, 15475, 15484]\n",
    "class2_TP_from_FN = class2[class2.index.isin(class2_FN_idx_list) == True] # (3, 21)\n",
    "\n",
    "class2_TP = pd.concat([class2_TP_string, class2_TP_antecedent, class2_TP_person_matching, class2_TP_from_FN]) # (5182, 21) richtig\n",
    "\n",
    "# 3. \"subject_by_rule\": 2sh+, \"zero_referents\": 2sh+ (+ person matching, from false negative)\n",
    "\n",
    "class3_TP_string = df2_exclude[(df2_exclude['subject_by_rule'] == '2sh+') & (df2_exclude['zero_referents'] == '2sh+')] # (6527, 21)\n",
    "class3 = df2_exclude[(df2_exclude['subject_by_rule'] == '2sh+')] # (6739, 21)\n",
    "class3_TP_person_matching = class3[(class3['zero_referents'] == \"2ph+\") | (class3['zero_referents'] == \"2sh-\")] # (39, 21)\n",
    "class3_FN_idx_list = [4797, 2525]\n",
    "class3_TP_from_FN = class3[class3.index.isin(class3_FN_idx_list) == True] # (2, 21)\n",
    "\n",
    "class3_TP = pd.concat([class3_TP_string, class3_TP_person_matching, class3_TP_from_FN]) # (6568, 21) richtig\n",
    "                                 \n",
    "\n",
    "class_1_2_3_TP = pd.concat([class1_TP, class2_TP, class3_TP]) # (11793, 21)\n",
    "    \n",
    "#4. \"subject_by_rule\": 1sh+ or \"subject_by_rule\": 2sh+ / others (except \"none\" 1669 or \"N/A\" 2049) should be 142, 106 (should be total: 248, 5 exception)\n",
    "\n",
    "df2_exclude_none = df2_exclude[(df2_exclude['subject_by_rule'] == 'none')] # (1669, 21)\n",
    "df2_exclude_NA = df2_exclude[(df2_exclude['subject_by_rule'] == 'N/A')] # (2049, 21)\n",
    "\n",
    "df2_exclude_none_NA = pd.concat([df2_exclude_none, df2_exclude_NA]) # (3718, 21) richtig\n",
    "\n",
    "# not TP and not none & NA (what left)\n",
    "\n",
    "# DF=DF1[~DF1.isin(DF2)].dropna(how = 'all')\n",
    "\n",
    "left_case1 = df2_exclude[~df2_exclude.isin(df2_exclude_none_NA)].dropna(how = 'all') # (12165, 21) 15883-3718 = 12165 richtig\n",
    "left_case2 = left_case1[~left_case1.isin(class_1_2_3_TP)].dropna(how = 'all') # (372, 21) 12165-11793 = 372 richtig\n",
    "\n",
    "# left_case2.to_csv(\"left_case2_311022.csv\", encoding = \"utf-8-sig\") \n",
    "# class 4는 총 248개로 계산되는데 왜 남아 있는 것은 372개인지 확인해야 함 (124개의 차이)\n",
    "\n",
    "# 1ph-\t2sh+ (1)\n",
    "# 1sh+\t2sh+ (58)\n",
    "# 1sh+\tzero_referents_string (54) e.g. 사람들\n",
    "# 1sh+\tantecedent_string (88) \n",
    "# 2sh+\t1ph-,1ph+,1sh+ (65)\n",
    "# 2sh+\tzero_referents_string (46)\n",
    "# 2sh+\tantecedent_string (60)\n",
    "\n",
    "# total: 372 richtig\n",
    "\n",
    "# 아마 precision, recall, f-measure 다시 계산해야 할 듯!\n",
    "\n",
    "\n",
    "# class1_TP.to_csv(\"class1_TP_011122.csv\", encoding = \"utf-8-sig\") \n",
    "# class2_TP.to_csv(\"class2_TP_011122.csv\", encoding = \"utf-8-sig\") \n",
    "# class3_TP.to_csv(\"class3_TP_011122.csv\", encoding = \"utf-8-sig\") \n",
    "# left_case2.to_csv(\"left_case2_011122.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n",
      "5183\n",
      "6568\n",
      "375\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스 별로 통계\n",
    "\n",
    "class1_TP_rule_freq = class1_TP['group'].value_counts() \n",
    "print(class1_TP['group'].size) # 43 richtig\n",
    "\n",
    "class2_TP_rule_freq = class2_TP['group'].value_counts() \n",
    "print(class2_TP['group'].size) # 5182 richtig\n",
    "\n",
    "class3_TP_rule_freq = class3_TP['group'].value_counts() \n",
    "print(class3_TP['group'].size) # 6568 richtig\n",
    "\n",
    "\n",
    "left_case2_rule_freq = left_case2['group'].value_counts() \n",
    "print(left_case2['group'].size) # 372 richtig\n",
    "\n",
    "#class1_TP_rule_freq.to_csv(\"class1_TP_rule_freq_011122.csv\", encoding = \"utf-8-sig\") \n",
    "#class2_TP_rule_freq.to_csv(\"class2_TP_rule_freq_011122.csv\", encoding = \"utf-8-sig\") \n",
    "#class3_TP_rule_freq.to_csv(\"class3_TP_rule_freq_011122.csv\", encoding = \"utf-8-sig\") \n",
    "#left_case2_rule_freq.to_csv(\"left_case2_rule_freq_011122.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2 관련 richtig check 120923\n",
    "\n",
    "class1_TP_rule_order = class1_TP['rule_order'].value_counts() \n",
    "class2_TP_rule_order = class2_TP['rule_order'].value_counts() \n",
    "class3_TP_rule_order = class3_TP['rule_order'].value_counts() \n",
    "\n",
    "#class1_TP_rule_order.to_csv(\"class1_TP_rule_order_120923.csv\", encoding = \"utf-8-sig\") \n",
    "# class2_TP_rule_order.to_csv(\"class2_TP_rule_order_120923.csv\", encoding = \"utf-8-sig\") \n",
    "# class3_TP_rule_order.to_csv(\"class3_TP_rule_order_120923.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.0,\n",
       " 10.0,\n",
       " 22.0,\n",
       " 30.0,\n",
       " 44.0,\n",
       " 47.0,\n",
       " 50.0,\n",
       " 51.0,\n",
       " 52.0,\n",
       " 73.0,\n",
       " 83.0,\n",
       " 94.0,\n",
       " 96.0,\n",
       " 99.0,\n",
       " 110.0,\n",
       " 111.0,\n",
       " 115.0,\n",
       " 116.0,\n",
       " 119.0,\n",
       " 121.0,\n",
       " 123.0,\n",
       " 126.0,\n",
       " 131.0,\n",
       " 133.0,\n",
       " 134.0,\n",
       " 135.0,\n",
       " 155.0,\n",
       " 162.0,\n",
       " 164.0,\n",
       " 165.0,\n",
       " 167.0,\n",
       " 168.0,\n",
       " 174.0,\n",
       " 187.0,\n",
       " 193.0,\n",
       " 196.0,\n",
       " 199.0,\n",
       " 200.0,\n",
       " 207.0]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# getroffen이 0인 rule_order (rule 이름 가져오기_120923)\n",
    "\n",
    "rule_order_stat_csv = pd.read_csv(\"df2_exclude_rule_subject_value_count_120923.csv\", index_col=0)\n",
    "\n",
    "rule_order_empty = rule_order_stat_csv['empty']\n",
    "rule_order_empty_dropna = rule_order_empty.dropna(axis=0)\n",
    "rule_order_empty_dropna_val = rule_order_empty_dropna.values \n",
    "rule_order_empty_dropna_val_list = rule_order_empty_dropna_val.tolist() # 932 richtig\n",
    "\n",
    "rule_order_empty_dropna_val_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_exclude = pd.read_csv(\"df2_exclude_230322.csv\", index_col=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "etri_sent                 int64\n",
       "sent                     object\n",
       "verb                     object\n",
       "applied_rule             object\n",
       "subject_by_rule          object\n",
       "zero_referents           object\n",
       "antecedent               object\n",
       "group                    object\n",
       "rule_order              float64\n",
       "rule                     object\n",
       "connective               object\n",
       "base_morpheme            object\n",
       "base_morpheme_second     object\n",
       "connective_true           int64\n",
       "zp_hono                  object\n",
       "zp_mood                  object\n",
       "sent_dep                 object\n",
       "zp_markable_ID           object\n",
       "zero_type                object\n",
       "tense_zp                 object\n",
       "verb_syn                 object\n",
       "verb_head                 int64\n",
       "Filter                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_exclude_empty_rule = df2_exclude[df2_exclude.rule_order.isin(rule_order_empty_dropna_val_list) == True]\n",
    "\n",
    "df2_exclude.dtypes # 당연히 df2에는 정보가 없음. 적용되지 않았으므로 \n",
    "# before_exclude_annotation_error_verb_syn = before_exclude_annotation_error['verb_syn'].value_counts() \n",
    "\n",
    "# before_exclude_annotation_error_verb_syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(372, 22)\n",
      "(1, 22)\n",
      "(58, 22)\n",
      "(65, 22)\n",
      "(142, 22)\n",
      "(106, 22)\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스 별로 통계 (Falsche Vorhersagen)\n",
    "\n",
    "FV = pd.read_csv(\"Falsche_Vorhersagen_271222.csv\", index_col=0)\n",
    "print(FV.shape) # (372, 22) richtig\n",
    "\n",
    "classA_FV = FV[FV[\"Klasse\"] == \"A\"]\n",
    "print(classA_FV.shape) # (1, 22) richtig\n",
    "\n",
    "classB_FV = FV[FV[\"Klasse\"] == \"B\"]\n",
    "print(classB_FV.shape) # (58, 22) richtig\n",
    "\n",
    "classC_FV = FV[FV[\"Klasse\"] == \"C\"]\n",
    "print(classC_FV.shape) # (65, 22) richtig\n",
    "\n",
    "classDB_FV = FV[FV[\"Klasse\"] == \"D-B\"]\n",
    "print(classDB_FV.shape) # (142, 22) richtig\n",
    "\n",
    "classDC_FV = FV[FV[\"Klasse\"] == \"D-C\"]\n",
    "print(classDC_FV.shape) # (106, 22) richtig\n",
    "\n",
    "classA_FV_rule_freq = classA_FV['group'].value_counts() \n",
    "classB_FV_rule_freq = classB_FV['group'].value_counts() \n",
    "classC_FV_rule_freq = classC_FV['group'].value_counts() \n",
    "classDB_FV_rule_freq = classDB_FV['group'].value_counts() \n",
    "classDC_FV_rule_freq = classDC_FV['group'].value_counts() \n",
    "\n",
    "#classA_FV_rule_freq.to_csv(\"classA_FV_rule_freq_271222.csv\", encoding = \"utf-8-sig\") \n",
    "#classB_FV_rule_freq.to_csv(\"classB_FV_rule_freq_271222.csv\", encoding = \"utf-8-sig\") \n",
    "#classC_FV_rule_freq.to_csv(\"classC_FV_rule_freq_271222.csv\", encoding = \"utf-8-sig\") \n",
    "#classDB_FV_rule_freq.to_csv(\"classDB_FV_rule_freq_271222.csv\", encoding = \"utf-8-sig\") \n",
    "#classDC_FV_rule_freq.to_csv(\"classDC_FV_rule_freq_271222.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(37, 22)\n",
      "(44, 22)\n",
      "(90, 22)\n",
      "(68, 22)\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스의 (class2-3) ranking 1-3 Regelgruppe의 실제 regel (Falsche Vorhersagen)\n",
    "\n",
    "# Class B: group value_counts (ranking 1-3)\n",
    "\n",
    "classB_FV_first_three_ranking = ['want statement', 'experience', 'possibility']\n",
    "classB_FV_rules = classB_FV[classB_FV.group.isin(classB_FV_first_three_ranking)] \n",
    "print(classB_FV_rules.shape) # (37, 22) richtig\n",
    "\n",
    "classB_FV_rules_value_count = classB_FV_rules[\"rule\"].value_counts() \n",
    "# classB_FV_rules_value_count.to_csv(\"classB_FV_rules_value_count_271222.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# Class C: group value_counts (ranking 1-2)\n",
    "\n",
    "classC_FV_first_two_ranking = ['permission', 'possibility']\n",
    "classC_FV_rules = classC_FV[classC_FV.group.isin(classC_FV_first_two_ranking)] \n",
    "print(classC_FV_rules.shape) # (44, 22) richtig\n",
    "\n",
    "classC_FV_rules_value_count = classC_FV_rules[\"rule\"].value_counts() \n",
    "#classC_FV_rules_value_count.to_csv(\"classC_FV_rules_value_count_271222.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# Class DB: group value_counts (ranking 1-3)\n",
    "\n",
    "classDB_FV_first_three_ranking = ['experience', 'progress', 'want statement']\n",
    "classDB_FV_rules = classDB_FV[classDB_FV.group.isin(classDB_FV_first_three_ranking)] \n",
    "print(classDB_FV_rules.shape) # (90, 22) richtig\n",
    "\n",
    "classDB_FV_rules_value_count = classDB_FV_rules[\"rule\"].value_counts() \n",
    "#classDB_FV_rules_value_count.to_csv(\"classDB_FV_rules_value_count_271222.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# Class DC: group value_counts (ranking 1-5)\n",
    "\n",
    "classDC_FV_first_five_ranking = ['clau', 'honorification', 'permission', 'want statement', 'mood derivable']\n",
    "classDC_FV_rules = classDC_FV[classDC_FV.group.isin(classDC_FV_first_five_ranking)] \n",
    "print(classDC_FV_rules.shape) # (68, 22) richtig\n",
    "\n",
    "classDC_FV_rules_value_count = classDC_FV_rules[\"rule\"].value_counts() \n",
    "#classDC_FV_rules_value_count.to_csv(\"classDC_FV_rules_value_count_271222.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classA_FV_referents: 2sh+    1\n",
      "Name: zero_referents, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "classA_FV_referents = classA_FV['zero_referents'].value_counts()  \n",
    "print(\"classA_FV_referents:\", classA_FV_referents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class1_TP_referents: 1ph-    42\n",
      "Name: zero_referents, dtype: int64\n",
      "class1_TP_antecedent:         5049\n",
      "제가        79\n",
      "저희가       18\n",
      "저도        13\n",
      "저는         9\n",
      "전          8\n",
      "우리는        2\n",
      "저희는        2\n",
      "저          1\n",
      "저희들도       1\n",
      "Name: antecedent, dtype: int64\n",
      "class2_TP_referents: 1sh+    4853\n",
      "1ph+     153\n",
      "1ph-      42\n",
      "2sh+       1\n",
      "Name: zero_referents, dtype: int64\n",
      "class2_TP_antecedent:         5049\n",
      "제가        79\n",
      "저희가       18\n",
      "저도        13\n",
      "저는         9\n",
      "전          8\n",
      "우리는        2\n",
      "저희는        2\n",
      "저          1\n",
      "저희들도       1\n",
      "Name: antecedent, dtype: int64\n",
      "class3_TP_referents: 2sh+    6527\n",
      "2ph+      37\n",
      "2sh-       2\n",
      "Name: zero_referents, dtype: int64\n",
      "class2_TP_antecedent:          6566\n",
      "관광객으로       1\n",
      "분은          1\n",
      "Name: antecedent, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 각 클래스 별로 referents & antecedents 통계\n",
    "\n",
    "class1_TP_referents = class1_TP['zero_referents'].value_counts() \n",
    "print(\"class1_TP_referents:\", class1_TP_referents)\n",
    "class1_TP_antecedent = class2_TP['antecedent'].value_counts() \n",
    "print(\"class1_TP_antecedent:\", class1_TP_antecedent)\n",
    "\n",
    "class2_TP_referents = class2_TP['zero_referents'].value_counts() \n",
    "print(\"class2_TP_referents:\", class2_TP_referents)\n",
    "class2_TP_antecedent = class2_TP['antecedent'].value_counts() \n",
    "print(\"class2_TP_antecedent:\", class2_TP_antecedent)\n",
    "\n",
    "class3_TP_referents = class3_TP['zero_referents'].value_counts() \n",
    "print(\"class3_TP_referents:\", class3_TP_referents)\n",
    "class3_TP_antecedent = class3_TP['antecedent'].value_counts() \n",
    "print(\"class2_TP_antecedent:\", class3_TP_antecedent)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 각 클래스의 (class2-3) ranking 1-3 Regelgruppe의 실제 regel\n",
    "\n",
    "# Class 2: group value_counts (ranking 1-3)\n",
    "\n",
    "class2_TP_first_three_ranking = ['want statement', 'suggestory formula', 'clau']\n",
    "class2_TP_rules = class2_TP[class2_TP.group.isin(class2_TP_first_three_ranking)] # (3270, 21) richtig\n",
    "class2_TP_rules_value_count = class2_TP_rules[\"rule\"].value_counts() \n",
    "\n",
    "# class2_TP_rules_value_count.to_csv(\"class2_TP_rules_value_count_011122.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# Class 3: group value_counts (ranking 1-3)\n",
    "\n",
    "class3_TP_first_three_ranking = ['mood derivable', 'clau', 'want statement']\n",
    "class3_TP_rules = class3_TP[class3_TP.group.isin(class3_TP_first_three_ranking)] # (4056, 21) richtig\n",
    "class3_TP_rules.shape\n",
    "class3_TP_rules_value_count = class3_TP_rules[\"rule\"].value_counts() \n",
    "\n",
    "# class3_TP_rules_value_count.to_csv(\"class3_TP_rules_value_count_011122.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 주어가 틀린 경우 (class 4) error analysis\n",
    "\n",
    "# classDB_FV\n",
    "\n",
    "classDB_FV_referents = classDB_FV['zero_referents'].value_counts() \n",
    "#print(\"classDB_FV_referents:\", classDB_FV_referents)\n",
    "\n",
    "classDB_FV_antecedent = classDB_FV['antecedent'].value_counts() \n",
    "#print(\"classDB_FV_antecedent:\", classDB_FV_antecedent)\n",
    "\n",
    "# classDC_FV\n",
    "\n",
    "classDC_FV_referents = classDC_FV['zero_referents'].value_counts() \n",
    "#print(\"classDC_FV_referents:\", classDC_FV_referents)\n",
    "\n",
    "classDC_FV_antecedent = classDC_FV['antecedent'].value_counts() \n",
    "#print(\"classDC_FV_antecedent:\", classDC_FV_antecedent)\n",
    "\n",
    "#classDB_FV_referents.to_csv(\"classDB_FV_referents_value_count_010223.csv\", encoding = \"utf-8-sig\") \n",
    "#classDB_FV_antecedent.to_csv(\"classDB_FV_antecedent_value_count_010223.csv\", encoding = \"utf-8-sig\") \n",
    "#classDC_FV_referents.to_csv(\"classDC_FV_referents_value_count_010223.csv\", encoding = \"utf-8-sig\") \n",
    "#classDC_FV_antecedent.to_csv(\"classDC_FV_antecedent_value_count_010223.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11422\n",
      "195\n",
      "39\n",
      "134\n"
     ]
    }
   ],
   "source": [
    "# 주어가 맞는 경우 rule group count\n",
    "\n",
    "rule_freq_case1 = df2_case1['group'].value_counts() \n",
    "print(df2_case1['group'].size) # 381 richtig\n",
    "\n",
    "rule_freq_first_person_match = df2_first_person_match['group'].value_counts() \n",
    "print(df2_first_person_match['group'].size) # 67 richtig\n",
    "\n",
    "rule_freq_second_person_match = df2_second_person_match['group'].value_counts() \n",
    "print(df2_second_person_match['group'].size) # 67 richtig\n",
    "\n",
    "rule_freq_case3 = df2_case3_antecedent_match['group'].value_counts() \n",
    "print(df2_case3_antecedent_match['group'].size) # 233 richtig\n",
    "\n",
    "# rule_freq_case1.to_csv(\"rule_freq_case1_260922.csv\", encoding = \"utf-8-sig\") \n",
    "# rule_freq_first_person_match.to_csv(\"rule_freq_first_person_match_260922.csv\", encoding = \"utf-8-sig\") \n",
    "# rule_freq_second_person_match.to_csv(\"rule_freq_second_person_match_260922.csv\", encoding = \"utf-8-sig\") \n",
    "# rule_freq_case3.to_csv(\"rule_freq_case3_260922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(59, 23)\n",
      "(275, 23)\n",
      "(54, 23)\n",
      "(5382, 23)\n",
      "(221, 23)\n",
      "(90, 23)\n",
      "(65, 23)\n",
      "(108, 23)\n",
      "(46, 23)\n",
      "(6739, 23)\n",
      "(62, 23)\n",
      "(108, 23)\n",
      "(46, 23)\n"
     ]
    }
   ],
   "source": [
    "# Error Analysis 061022\n",
    "\n",
    "#1.1ph- as 2sh+ (count: 1)\n",
    "#2.1sh+ as 2sh+ (count: 59)\n",
    "df2_exclude_error_2 = df2_exclude[(df2_exclude['subject_by_rule'] == \"1sh+\") & (df2_exclude['zero_referents'] == \"2sh+\")]\n",
    "print(df2_exclude_error_2.shape) # (59, 21)\n",
    "\n",
    "df2_exclude_error_2_group = df2_exclude_error_2[\"group\"].value_counts()\n",
    "# print(df2_exclude_error_2_group)\n",
    "\n",
    "# df2_exclude_error_2_group.to_csv(\"df2_exclude_error_2_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "#3.1sh+ as etc. (count: 54) (the rest: 90/ total: 144)\n",
    "\n",
    "df2_exclude_error_3 = df2_exclude[(~(df2_exclude['zero_referents'] == \"1ph-\") & ~(df2_exclude['zero_referents'] == \"1ph+\") & ~(df2_exclude['zero_referents'] == \"1sh+\") & ~(df2_exclude['zero_referents'] == \"2sh+\")) & (df2_exclude['subject_by_rule'] == \"1sh+\")]\n",
    "print(df2_exclude_error_3.shape) # (275, 21) 필드값 없음까지!\n",
    "df2_exclude_error_3_1 = df2_exclude_error_3[df2_exclude_error_3['zero_referents'].notnull()]\n",
    "print(df2_exclude_error_3_1.shape) # (54, 21) 필드값 있는 경우만 \n",
    "\n",
    "df2_exclude_error_3_1_group = df2_exclude_error_3_1[\"group\"].value_counts()\n",
    "df2_exclude_error_3_1_zero_referents = df2_exclude_error_3_1.groupby(\"zero_referents\")[\"group\"].value_counts()\n",
    "\n",
    "# print(df2_exclude_error_3_1_group)\n",
    "\n",
    "# df2_exclude_error_3_1_group.to_csv(\"df2_exclude_error_3_1_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_exclude_error_3_1_zero_referents.to_csv(\"df2_exclude_error_3_1_zero_referents_071022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# df2_exclude_error_3_2 = df2_exclude_error_3[(~(df2_exclude_error_3['antecedent'] == \"저\") & ~(df2_exclude['antecedent'] == \"저는\") & ~(df2_exclude['antecedent'] == \"저도\") & ~(df2_exclude['antecedent'] == \"저희가\") & ~(df2_exclude['antecedent'] == \"저희들도\") & ~(df2_exclude['antecedent'] == \"전\") & ~(df2_exclude['antecedent'] == \"제가\")) & (df2_exclude['subject_by_rule'] == \"1sh+\")]\n",
    "#\"저\", \"저는\", \"저도\", \"저희가\", \"저희는\", 저희들도\", \"전\", \"제가\"\n",
    "\n",
    "df2_exclude_error_3_antecedent = df2_exclude[(df2_exclude['subject_by_rule'] == \"1sh+\")]\n",
    "print(df2_exclude_error_3_antecedent.shape) # (5382, 21) 필드값 없음까지!\n",
    "df2_exclude_error_3_antecedent_notnull = df2_exclude_error_3_antecedent[df2_exclude_error_3_antecedent['antecedent'].str.strip().astype(bool)]\n",
    "print(df2_exclude_error_3_antecedent_notnull.shape) # (221, 21) 필드값 있는 경우만!\n",
    "\n",
    "df2_exclude_error_3_2 = df2_exclude_error_3_antecedent_notnull[~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저희는\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저는\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저도\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저희가\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"저희들도\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"전\") & ~(df2_exclude_error_3_antecedent_notnull['antecedent'] == \"제가\")]\n",
    "print(df2_exclude_error_3_2.shape) # (90, 21) 필드값 있는 경우만!\n",
    "\n",
    "df2_exclude_error_3_2_group = df2_exclude_error_3_2[\"group\"].value_counts()\n",
    "df2_exclude_error_3_2_antecedent = df2_exclude_error_3_2.groupby(\"antecedent\")[\"group\"].value_counts()\n",
    "df2_exclude_error_3_2_group_antecedent = df2_exclude_error_3_2.groupby(\"group\")[\"antecedent\"].value_counts()\n",
    "\n",
    "# df2_exclude_error_3_2_group.to_csv(\"df2_exclude_error_3_2_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_exclude_error_3_2_antecedent.to_csv(\"df2_exclude_error_3_2_antecedent_071022.csv\", encoding = \"utf-8-sig\")\n",
    "# df2_exclude_error_3_2_group_antecedent.to_csv(\"df2_exclude_error_3_2_group_antecedent_071022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "\n",
    "#(1ph-, 1ph+, 1sh+)\n",
    "df2_exclude_error_4 = df2_exclude[(df2_exclude['subject_by_rule'] == \"2sh+\") & ((df2_exclude['zero_referents'] == \"1ph-\") | (df2_exclude['zero_referents'] == \"1ph+\") | (df2_exclude['zero_referents'] == \"1sh+\"))]\n",
    "print(df2_exclude_error_4.shape) # (65, 21)\n",
    "\n",
    "df2_exclude_error_4_group = df2_exclude_error_4[\"group\"].value_counts()\n",
    "# df2_exclude_error_4_group.to_csv(\"df2_exclude_error_4_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# zero_referents가 1ph-, 1ph+, 1sh+, 2ph+, 2sh-, 2sh+ 가 아닌 subject_by_rule \"2sh+\"\n",
    "df2_exclude_error_4_1 = df2_exclude[(~(df2_exclude['zero_referents'] == \"2ph+\") & ~(df2_exclude['zero_referents'] == \"2sh-\") & ~(df2_exclude['zero_referents'] == \"1ph-\") & ~(df2_exclude['zero_referents'] == \"1ph+\") & ~(df2_exclude['zero_referents'] == \"1sh+\") & ~(df2_exclude['zero_referents'] == \"2sh+\")) & (df2_exclude['subject_by_rule'] == \"2sh+\")]\n",
    "print(df2_exclude_error_4_1.shape) # 필드값 없는 것 까지 \n",
    "\n",
    "df2_exclude_error_4_1_notnull = df2_exclude_error_4_1[df2_exclude_error_4_1['zero_referents'].notnull()]\n",
    "print(df2_exclude_error_4_1_notnull.shape) # (46, 21)\n",
    "\n",
    "df2_exclude_error_4_1_notnull_group = df2_exclude_error_4_1_notnull[\"group\"].value_counts()\n",
    "# df2_exclude_error_4_1_notnull_group.to_csv(\"df2_exclude_error_4_1_notnull_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df2_exclude_error_4_antecedent = df2_exclude[(df2_exclude['subject_by_rule'] == \"2sh+\")]\n",
    "print(df2_exclude_error_4_antecedent.shape) # (6739, 21) 필드값 없음까지!\n",
    "df2_exclude_error_4_antecedent_notnull = df2_exclude_error_4_antecedent[df2_exclude_error_4_antecedent['antecedent'].str.strip().astype(bool)]\n",
    "print(df2_exclude_error_4_antecedent_notnull.shape) # (62, 21) 필드값 있는 경우만!\n",
    "\n",
    "df2_exclude_error_4_antecedent_notnull_group = df2_exclude_error_4_antecedent_notnull[\"group\"].value_counts()\n",
    "# df2_exclude_error_4_antecedent_notnull_group.to_csv(\"df2_exclude_error_4_antecedent_notnull_group_061022.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 1ph-, 1ph+, 1sh+, 2ph+, 2sh-, 2sh+ 제외, 그리고 필드값 없음 제외\n",
    "df2_exclude_error_5 = df2_exclude[(df2_exclude['subject_by_rule'] == \"2sh+\") & (~(df2_exclude['zero_referents'] == \"1ph-\") & ~(df2_exclude['zero_referents'] == \"1ph+\") & ~(df2_exclude['zero_referents'] == \"1sh+\") & ~(df2_exclude['zero_referents'] == \"2ph+\") & ~(df2_exclude['zero_referents'] == \"2sh-\") & ~(df2_exclude['zero_referents'] == \"2sh+\"))]\n",
    "print(df2_exclude_error_5.shape) # 108, 21) 필드값 없는 것 까지\n",
    "\n",
    "df2_exclude_error_5_1  = df2_exclude_error_5[df2_exclude_error_5['zero_referents'].notnull()]\n",
    "print(df2_exclude_error_5_1.shape) # (46, 21) 필드값 없는 것 까지\n",
    "\n",
    "df2_exclude_error_5_1_zero_referents = df2_exclude_error_5_1.groupby(\"zero_referents\")[\"group\"].value_counts()\n",
    "# df2_exclude_error_5_1_zero_referents.to_csv(\"df2_exclude_error_5_1_zero_referents_081022.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none (1.669) 220223 \n",
    "\n",
    "df3_none_change = pd.read_csv(\"df3_rule_applied_220223.csv\", index_col=0)  # referents의 값이 없거나 값이 틀린 case 수정함 220223 \n",
    "# (15883, 21) richtig\n",
    "# df3_none_change.to_csv(\"df3_none_change_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "df3_none_change_none = df3_none_change[df3_none_change[\"subject_by_rule\"] == \"none\"]\n",
    "df3_none_change_none.shape #(1669, 21) richtig\n",
    "# df3_none_change_none.to_csv(\"df3_none_change_none_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "df3_none_change_none_zero_referents = df3_none_change_none[\"zero_referents\"].value_counts()\n",
    "# df3_none_change_none_zero_referents.to_csv(\"df3_none_change_none_zero_referents_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df3_none_change_none_antecedent = df3_none_change_none[\"antecedent\"].value_counts()\n",
    "# df3_none_change_none_antecedent.to_csv(\"df3_none_change_none_antecedent_220223.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# none data type added 220223\n",
    "\n",
    "df3_none_type_added = pd.read_csv(\"df3_none_type_added_220223.csv\", index_col=0)  # referents의 type을 추가함\n",
    "# (1669, 22) richtig  열 하나가 더 늘어남\n",
    "\n",
    "df3_none_type_added_rule_order = df3_none_type_added.groupby(\"type\")[\"rule_order\"].value_counts()\n",
    "\n",
    "# df3_none_type_added_rule_order.to_csv(\"df3_none_type_added_rule_order_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df3_none_type_added_rule_order_value_count = df3_none_type_added[\"rule_order\"].value_counts()\n",
    "\n",
    "# df3_none_type_added_rule_order_value_count\n",
    "\n",
    "df3_none_type_added_rule_order_type = df3_none_type_added.groupby(\"rule_order\")[\"type\"].value_counts()\n",
    "\n",
    "# df3_none_type_added_rule_order_type.to_csv(\"df3_none_type_added_rule_order_type_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df3_none_type_added_rule_order_213 = df3_none_type_added[df3_none_type_added[\"rule_order\"] == 213]\n",
    "\n",
    "# df3_none_type_added_rule_order_213_verb_syn = df3_none_type_added_rule_order_213[\"verb_syn\"].value_counts()\n",
    "\n",
    "# df3_none_type_added_rule_order_213_verb_syn.to_csv(\"df3_none_type_added_rule_order_213_verb_syn_220223.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2049, 23)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# N/A (2.049) analysis 150323\n",
    "# df2_exclude_061022 에서 NA가 할당된 case만 따로 csv로 만들기, 2.049여야 함\n",
    "\n",
    "#df2_exclude_orig = pd.read_csv(\"df2_exclude_061022.csv\", index_col=0)\n",
    "\n",
    "df2_exclude_orig = pd.read_csv(\"df2_exclude_230322.csv\", index_col=0)\n",
    "\n",
    "# df2_exclude_orig.shape # (15883, 21) rightig\n",
    "\n",
    "df2_NA = df2_exclude_orig[df2_exclude_orig[\"subject_by_rule\"].isnull()]\n",
    "\n",
    "df2_NA.shape # (2049, 21) richtig\n",
    " \n",
    "# df2_NA.to_csv(\"df2_NA_check_260922.csv\", encoding = \"utf-8-sig\") \n",
    "#df2_NA.to_csv(\"df2_NA_check_230322.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-28-f877c4105f33>:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_NA_root['new_rule_root'] = np.select(conditionlist, choicelist, default='Not Specified')\n"
     ]
    }
   ],
   "source": [
    "# N/A analysis 1 (root) 160323\n",
    "\n",
    "\n",
    "# df2_NA_orig = pd.read_csv(\"df2_NA_check_260922.csv\", index_col=0)\n",
    "df2_NA_orig = pd.read_csv(\"df2_NA_check_230322.csv\", index_col=0)\n",
    "\n",
    "# df2_NA_orig.shape # (2049, 17) richtig\n",
    "\n",
    "df2_NA_root = df2_NA_orig[df2_NA_orig[\"verb_syn\"] == \"root\"]\n",
    "\n",
    "# df2_NA_root.shape # (854, 17) richtig\n",
    "\n",
    "# df2_NA_root에 새로운 열 추가 \n",
    "\n",
    "conditionlist = [\n",
    "    (df2_NA_root['zp_mood'] == 'decl'),\n",
    "    (df2_NA_root['zp_mood'] == 'ques')] \n",
    "\n",
    "choicelist = ['1sh+', '2sh+']\n",
    "\n",
    "df2_NA_root['new_rule_root'] = np.select(conditionlist, choicelist, default='Not Specified')\n",
    "\n",
    "# df2_NA_root.shape # (17266, 21)\n",
    "\n",
    "# df2_NA_root.to_csv(\"df2_NA_check_root_new_rule_160323.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-29-b7f0585fb9ef>:34: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_NA_root['prediction'] = np.select(conditionlist, choicelist, default='N/A')\n"
     ]
    }
   ],
   "source": [
    "# N/A analysis 1 right prediction (root new rule 정확도 체크) 160323\n",
    "\n",
    "# df2_NA_root에서 시작하면 됨\n",
    "\n",
    "first_person_list = ['1ph-', '1ph+', '1sh+']\n",
    "first_person_ant_list = ['저는', '제가', '저도', '저희가', '저희는', '전']\n",
    "second_person_list = ['2ph-', '2ph+', '2sh+']\n",
    "\n",
    "\n",
    "df2_NA_root_ich = df2_NA_root[df2_NA_root['new_rule_root'] == '1sh+']\n",
    "\n",
    "# df2_NA_root_ich.shape # (579, 18) richtig\n",
    "\n",
    "df2_NA_root_Sie = df2_NA_root[df2_NA_root['new_rule_root'] == '2sh+']\n",
    "\n",
    "# df2_NA_root_Sie.shape # (275, 18) richtig\n",
    "\n",
    "\n",
    "df2_NA_root_first_person_match = df2_NA_root_ich[df2_NA_root_ich.zero_referents.isin(first_person_list) == True]\n",
    "df2_NA_root_second_person_match = df2_NA_root_Sie[df2_NA_root_Sie.zero_referents.isin(second_person_list) == True]\n",
    "\n",
    "# df2_NA_root_first_person_match.shape # (400, 18) richtig\n",
    "# df2_NA_root_second_person_match.shape # (46, 18) richtig\n",
    "\n",
    "conditionlist = [\n",
    "    (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['zero_referents'].isin(first_person_list) == True),\n",
    "    (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['antecedent'].isin(first_person_ant_list) == True),\n",
    "    (df2_NA_root['new_rule_root'] == '2sh+') & (df2_NA_root['zero_referents'].isin(second_person_list) == True),\n",
    "    (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['zero_referents'].isin(first_person_list) == False),\n",
    "    (df2_NA_root['new_rule_root'] == '2sh+') & (df2_NA_root['zero_referents'].isin(second_person_list) == False)] \n",
    "\n",
    "choicelist = ['richtig_first', 'richtig_first_ant', 'richtig_second', 'wrong_first', 'wrong_second']\n",
    "\n",
    "df2_NA_root['prediction'] = np.select(conditionlist, choicelist, default='N/A')\n",
    "\n",
    "# df2_NA_root.head()\n",
    "\n",
    "# df2_NA_root.to_csv(\"df2_NA_check_root_prediction_160323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df2_NA_root_predic = df2_NA_root['prediction'].value_counts() \n",
    "\n",
    "richtig_list = [\"richtig_first\", \"richtig_first_ant\"]\n",
    "\n",
    "df2_NA_root_predic_first = df2_NA_root[df2_NA_root[\"prediction\"].isin(richtig_list) == True]\n",
    "df2_NA_root_predic_first_value_count = df2_NA_root_predic_first[\"zero_referents\"].value_counts() \n",
    "df2_NA_root_predic_first_ante_value_count = df2_NA_root_predic_first[\"antecedent\"].value_counts() \n",
    "\n",
    "# df2_NA_root_predic_first_value_count.to_csv(\"df2_NA_root_predic_first_value_count_new_160323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# df2_NA_root_predic_first_ante_value_count.to_csv(\"df2_NA_root_predic_first_ant_value_count_new_160323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "df2_NA_root_predic_second = df2_NA_root[df2_NA_root[\"prediction\"] == \"richtig_second\"]\n",
    "df2_NA_root_predic_second_value_count = df2_NA_root_predic_second[\"zero_referents\"].value_counts() \n",
    "\n",
    "# df2_NA_root_predic_second_value_count.to_csv(\"df2_NA_root_predic_second_value_count_160323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# df2_NA_root_predic.to_csv(\"df2_NA_check_root_prediction_STAT_160323.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N/A analysis 1 wrong prediction (root new rule 정확도 체크) 160323\n",
    "\n",
    "df2_NA_root_predic_wrong_first = df2_NA_root[df2_NA_root['prediction'] == 'wrong_first']\n",
    "df2_NA_root_predic_wrong_second = df2_NA_root[df2_NA_root['prediction'] == 'wrong_second']\n",
    "\n",
    "# df2_NA_root_predic_wrong_first.shape # (151, 19) richtig\n",
    "# df2_NA_root_predic_wrong_second.shape # (229, 19) richtig\n",
    "\n",
    "\n",
    "df2_NA_root_predic_wrong_first_referents_value_count = df2_NA_root_predic_wrong_first[\"zero_referents\"].value_counts() \n",
    "df2_NA_root_predic_wrong_first_ant_value_count = df2_NA_root_predic_wrong_first[\"antecedent\"].value_counts() \n",
    "\n",
    "df2_NA_root_predic_wrong_second_referents_value_count = df2_NA_root_predic_wrong_second[\"zero_referents\"].value_counts() \n",
    "df2_NA_root_predic_wrong_second_ant_value_count = df2_NA_root_predic_wrong_second[\"antecedent\"].value_counts() \n",
    "\n",
    "#df2_NA_root_predic_wrong_first_referents_value_count.to_csv(\"wrong_first_referents_value_count_160323.csv\", encoding = \"utf-8-sig\") \n",
    "#df2_NA_root_predic_wrong_first_ant_value_count.to_csv(\"wrong_first_antecedent_value_count_160323.csv\", encoding = \"utf-8-sig\") \n",
    "#df2_NA_root_predic_wrong_second_referents_value_count.to_csv(\"wrong_second_referents_value_count_160323.csv\", encoding = \"utf-8-sig\") \n",
    "#df2_NA_root_predic_wrong_second_ant_value_count.to_csv(\"wrong_second_antecedent_value_count_160323.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-974c902d2d47>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df2_NA_clau[\"new_rule_clau\"] = np.where(df2_NA_clau[\"connective\"] == \"class_A\", \"copy\",\"Not Specified\")\n"
     ]
    }
   ],
   "source": [
    "# N/A analysis 2 (clau) 160323\n",
    "\n",
    "df2_NA_clau = df2_NA_orig[df2_NA_orig[\"verb_syn\"] == \"clau\"]\n",
    "\n",
    "# df2_NA_clau.shape # (1195, 17) richtig\n",
    "\n",
    "# df2_NA_clau에 새로운 열 추가 \n",
    "\n",
    "df2_NA_clau[\"new_rule_clau\"] = np.where(df2_NA_clau[\"connective\"] == \"class_A\", \"copy\",\"Not Specified\")\n",
    "# df2_NA_clau.head()\n",
    "\n",
    "# df2_NA_clau.to_csv(\"df2_NA_check_clau_new_rule_230323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# index에 따라 base_morpheme과 base_morpheme_second를 추가 가능?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N/A analysis 2 (clau new rule 정확도 체크) 290322\n",
    "\n",
    "# 복사한 주어가 같냐 안 같냐\n",
    "\n",
    "# 1. 기존에 clau 테스트 한 파일 불러와서 idx 저장\n",
    "\n",
    "clau_one = pd.read_csv('clau_count_one_add_subject_and_verb_29052022.csv') \n",
    "clau_one_idx = clau_one['idx']\n",
    "clau_one_idx_val = clau_one_idx.values \n",
    "clau_one_idx_val_list = clau_one_idx_val.tolist() \n",
    "\n",
    "\n",
    "# len(clau_one_idx_val_list) # 432 richtig\n",
    "\n",
    "# 2. 현재 clau test 파일 불러와서 connective가 class_A인 케이스의 idx 저장하고 1의 idx가 2의 idx에 다 포함되는지 확인\n",
    "\n",
    "clau_check = pd.read_csv('df2_NA_check_clau_new_rule_230323.csv') \n",
    "clau_class_A = clau_check[clau_check[\"connective\"] == \"class_A\"]\n",
    "\n",
    "# clau_class_A.shape # (740, 25) richtig\n",
    "\n",
    "clau_class_A_idx = clau_class_A['idx']\n",
    "clau_class_A_idx_val = clau_class_A_idx.values \n",
    "clau_class_A_idx_val_list = clau_class_A_idx_val.tolist()\n",
    "\n",
    "# len(clau_class_A_idx_val_list) # 740 richtig\n",
    "\n",
    "# 3. 리스트 값 비교\n",
    "\n",
    "set1 = set(clau_one_idx_val_list)\n",
    "set2 = set(clau_class_A_idx_val_list)\n",
    "\n",
    "set_intersection = set1.intersection(set2)\n",
    "\n",
    "# len(set_intersection) # 370\n",
    "\n",
    "# 432가 아니라 370이군!!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 코드 활용해서 clau_one과 주어 복사 가능한지 시도해보기 290323\n",
    "\n",
    "# df2_exclude 대신 df2_exclude_orig 활용하기\n",
    "# df2_exclude_orig # \"df2_exclude_230322.csv\", index_col=0\n",
    "\n",
    "df4 = df2_exclude_orig[df2_exclude_orig[\"Filter\"] == 'C']\n",
    "# print(df4.shape) # (2049, 23) NA의 수와 같음, richtig\n",
    "\n",
    "df4 = df4.drop(['subject_by_rule','group','rule'], axis=1)\n",
    "# df4 = df4.sort_values(by=['connective', 'verb_syn'])\n",
    "# df4.to_csv(\"df4_290323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# Filter A와 B인 case들과 class_A와의 접점을 찾아야 하는 이유는 꼭 복사될 주어가 정답이 아니더라도\n",
    "# 복사할 수 있는 주어가 있을 수 있다는 것이기 때문에\n",
    "# 정답이 아닌 것은 나중에 정확도 평가할 때 Error analysis를 하면 될 듯\n",
    "\n",
    "df2_FilterA_B = df2_exclude_orig[(df2_exclude_orig[\"Filter\"] == 'A') | (df2_exclude_orig[\"Filter\"] == 'B')] \n",
    "# print(df2_FilterA_B.shape) # (13834, 23) Resolutionsystem의 input 수와 같음, richtig\n",
    "\n",
    "# Filter A와 B인 case와 class_A에서 중복되는 etri_sent있는지 확인하고, 해당 df에 대해서 Filter A와 B의 분포 살펴보기\n",
    "\n",
    "df2_FilterA_B_etri_sent = df2_FilterA_B.etri_sent\n",
    "df2_FilterA_B_etri_sent_val = df2_FilterA_B_etri_sent.values \n",
    "df2_FilterA_B_etri_sent_val_list = df2_FilterA_B_etri_sent_val.tolist() # 13834\n",
    "# print(len(df2_FilterA_B_etri_sent_val_list))\n",
    "df2_FilterA_B_etri_sent_val_list_set = set(df2_FilterA_B_etri_sent_val_list) # 12480\n",
    "# print(len(df2_FilterA_B_etri_sent_val_list_set))\n",
    "\n",
    "# print(\"df2_FilterA_B_etri_sent_val_list:\", len(df2_FilterA_B_etri_sent_val_list))\n",
    "# print(\"df2_FilterA_B_etri_sent_val_list_set:\", len(df2_FilterA_B_etri_sent_val_list_set))\n",
    "\n",
    "df4_class_A = df4[df4.connective == 'class_A'] \n",
    "# print(df4_class_A.shape) # (740, 20) class A의 connective를 지닌 case 740 richtig\n",
    "# df4_class_A.to_csv(\"df4_class_A_290323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df4_class_A_etri_sent = df4_class_A.etri_sent\n",
    "df4_class_A_etri_sent_val = df4_class_A_etri_sent.values \n",
    "df4_class_A_etri_sent_val_list = df4_class_A_etri_sent_val.tolist() # 740\n",
    "df4_class_A_etri_sent_val_list_set = set(df4_class_A_etri_sent_val_list) # 714\n",
    "\n",
    "# print(len(df4_class_A_etri_sent_val_list))\n",
    "# print(len(df4_class_A_etri_sent_val_list_set))\n",
    "\n",
    "intersection_df2_FilterA_B_and_classA = list(set(df2_FilterA_B_etri_sent_val_list) & set(df4_class_A_etri_sent_val_list))\n",
    "# print(intersection_df2_FilterA_B_and_classA)\n",
    "# print(\"intersection_df2_FilterA_B_and_classA:\", len(intersection_df2_FilterA_B_and_classA)) # 575 (740개의 77.70%)\n",
    "\n",
    "# df4_class_A_check의 경우 class A에 속하는 conjunction을 지니고 있지만 resolutionsystem에서 규칙을 적용 받지 못한 etri_sentence에 관한 데이터임\n",
    "\n",
    "df4_class_A_check = df4_class_A[df4_class_A.etri_sent.isin(intersection_df2_FilterA_B_and_classA) == False] # (146, 20)\n",
    "# df4_class_A_check의 경우 복사할 주어가 있는지 또는 이미 존재하는 주어가 있는지 등을 체크해 볼 수 있음  \n",
    "# df4_class_A_check.to_csv(\"df4_class_A_check_290323.csv\", encoding = \"utf-8-sig\") \n",
    "## 중복되는 etri_sent가 있을 수 있음\n",
    "## \"df4_class_A_check_2_comment_23052022.xslx\"에 comment 적어놓음 (230522)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 우선 FilterA,B와 FilterC면서 class A인 케이스 간의 etri_sent의 intersection이였던 case의 clue를 check하면서 \n",
    "## 이미 할당된 subject가 있는 경우 이것을 자동으로 복사할 수 있는지 check하는 단계 290323\n",
    "\n",
    "classA_clue_check = df4_class_A[df4_class_A.etri_sent.isin(intersection_df2_FilterA_B_and_classA) == True] \n",
    "# print(classA_clue_check.shape) #(594, 20) 아마 set으로 count하면 575일듯! 중복된 항목 제거하면 575임\n",
    "# classA_clue_check.to_csv(\"classA_clue_check_290323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# sent_dep 예시: ['modi', 'modi', 'clau', 'modi', 'clau', 'root', 'punc']\n",
    "\n",
    "classA_sent_dep = classA_clue_check.sent_dep\n",
    "classA_sent_dep_val = classA_sent_dep.values \n",
    "classA_sent_dep_val_list = classA_sent_dep_val.tolist() \n",
    "\n",
    "# print(len(classA_sent_dep_val_list)) # 594 richtig\n",
    "\n",
    "classA_etri_sent = classA_clue_check.etri_sent\n",
    "classA_etri_sent_val = classA_etri_sent.values \n",
    "classA_etri_sent_val_list = classA_etri_sent_val.tolist() \n",
    "\n",
    "# print(len(classA_etri_sent_val_list)) # 594 richtig\n",
    "\n",
    "# sent_dep에 있는 clau의 개수를 세서 새로운 열로 추가함 \n",
    "        \n",
    "clau_count = [dep.count('clau') for dep in classA_clue_check['sent_dep']]\n",
    "clau_count_add = classA_clue_check.assign(clau = clau_count) \n",
    "\n",
    "# print(clau_count_add.shape) # (594, 21) richtig\n",
    "\n",
    "# clau_count_add.to_csv(\"clau_count_add_290323.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    464\n",
       "2    212\n",
       "3     52\n",
       "4      9\n",
       "5      3\n",
       "Name: clau, dtype: int64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df4_class_A에서 class A에 속하는 conjunction을 지닌 predicate 740개들 중에 clau의 개수 310323\n",
    "\n",
    "\n",
    "# sent_dep 예시: ['modi', 'modi', 'clau', 'modi', 'clau', 'root', 'punc']\n",
    "\n",
    "df4_classA_sent_dep = df4_class_A.sent_dep\n",
    "df4_classA_sent_dep_val = df4_classA_sent_dep.values \n",
    "df4_classA_sent_dep_val_list = df4_classA_sent_dep_val.tolist() \n",
    "\n",
    "# print(len(df4_classA_sent_dep_val_list)) # 740 richtig\n",
    "\n",
    "df4_clau_count = [dep.count('clau') for dep in df4_class_A['sent_dep']]\n",
    "# print(len(df4_clau_count)) # 740 richtig\n",
    "\n",
    "df4_clau_count_add = df4_class_A.assign(clau = df4_clau_count) \n",
    "\n",
    "# df4_clau_count_add.to_csv(\"df4_clau_count_add_310323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df4_clau_freq = df4_clau_count_add['clau'].value_counts()\n",
    "\n",
    "# df4_clau_freq.to_csv(\"df4_clau_freq_310323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df4_clau_freq\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clau가 하나로 카운트 되는 464개의 predicates에 대해서 몇 개가 etri_sent가 같은지 확인하기 \n",
    "\n",
    "df4_clau_one = df4_clau_count_add[df4_clau_count_add['clau'] == 1]\n",
    "# print(df4_clau_one.shape) # (464, 21) richtig\n",
    "\n",
    "df4_clau_one_etri_sent = df4_clau_one.etri_sent\n",
    "df4_clau_one_etri_sent_val = df4_clau_one_etri_sent.values \n",
    "df4_clau_one_etri_sent_val_list = df4_clau_one_etri_sent_val.tolist() \n",
    "\n",
    "# print(len(df4_clau_one_etri_sent)) # 464 richtig (중복되는 etri_sent가 없음!)\n",
    "\n",
    "df2_FilterA = df2_exclude_orig[df2_exclude_orig[\"Filter\"] == 'A'] \n",
    "\n",
    "# print(df2_FilterA.shape) # (11644, 23) richtig\n",
    "\n",
    "df2_FilterA_etri_sent = df2_FilterA.etri_sent\n",
    "df2_FilterA_etri_sent_val = df2_FilterA_etri_sent.values \n",
    "df2_FilterA_etri_sent_val_list = df2_FilterA_etri_sent.tolist() \n",
    "\n",
    "# print(len(df2_FilterA_etri_sent_val_list)) # 11644 richtig\n",
    "\n",
    "# print(len(set(df4_clau_one_etri_sent_val_list))) # 464 (중복되는 etri_sent가 없음!)\n",
    "# print(len(set(df2_FilterA_etri_sent_val_list))) # 11644 (중복되는 etri_sent가 없음!)\n",
    "\n",
    "# df2_FilterA에서 df4_clau_one_etri_sent_val_list 안에 들어 있는 case를 extract하고 이것의 subject_by_rule을 알아내기\n",
    "\n",
    "df2_FilterA_root = df2_FilterA[df2_FilterA.etri_sent.isin(df4_clau_one_etri_sent_val_list)]\n",
    "\n",
    "# print(df2_FilterA_root.shape) # (365, 23)\n",
    "\n",
    "# clau가 하나로 count되는 464개의 predicates 중 이미 resolutionssystem에 의해 할당된 referents가 있는 case가 365개라는 뜻 (전체의 78,66%)! 310323 \n",
    "# 나머지 99개는 manuell하게 확인해보기 (explicit한 주어가 있는지, 없는 경우에도 S1과 S2의 주어가 같은지)\n",
    "\n",
    "df2_FilterA_root_subject_by_rule = df2_FilterA_root.subject_by_rule\n",
    "df2_FilterA_root_subject_by_rule_val = df2_FilterA_root_subject_by_rule.values \n",
    "df2_FilterA_root_subject_by_rule_val_list = df2_FilterA_root_subject_by_rule_val.tolist() \n",
    "# print(len(df2_FilterA_root_subject_by_rule_val_list)) # 365 richtig\n",
    "\n",
    "df2_FilterA_root_verb = df2_FilterA_root.verb\n",
    "df2_FilterA_root_verb_val = df2_FilterA_root_verb.values \n",
    "df2_FilterA_root_verb_val_list = df2_FilterA_root_verb_val.tolist() \n",
    "# print(len(df2_FilterA_root_verb_val_list)) # 365 richtig\n",
    "\n",
    "df2_FilterA_root_etri_sent = df2_FilterA_root.etri_sent\n",
    "df2_FilterA_root_etri_sent_val = df2_FilterA_root_etri_sent.values \n",
    "df2_FilterA_root_etri_sent_val_list = df2_FilterA_root_etri_sent_val.tolist() \n",
    "# print(len(df2_FilterA_root_etri_sent_val_list)) # 365 richtig\n",
    "\n",
    "filterA_subject_by_rule_dict = dict(zip(df2_FilterA_root_etri_sent_val_list, df2_FilterA_root_subject_by_rule_val_list)) \n",
    "# print(len(filterA_subject_by_rule_dict)) # 365 richtig \n",
    "\n",
    "filterA_verb_dict = dict(zip(df2_FilterA_root_etri_sent_val_list, df2_FilterA_root_verb_val_list)) \n",
    "# print(len(filterA_verb_dict)) # 365 richtig \n",
    "\n",
    "# df4_clau_one에 filterA에 있던 정보인 subject_by_rule과 verb copy하기 310323\n",
    "\n",
    "filter_A_subject_by_rule_keys = list(filterA_subject_by_rule_dict.keys())\n",
    "\n",
    "filter_A_copied_subject_by_rule = [] \n",
    "\n",
    "for sent in df4_clau_one['etri_sent']:\n",
    "    if sent in filter_A_subject_by_rule_keys:\n",
    "        filter_A_copied_subject_by_rule.append(filterA_subject_by_rule_dict[sent])\n",
    "    \n",
    "# print(len(filter_A_copied_subject_by_rule)) # 365 richtig\n",
    "\n",
    "filter_A_verb_dict_keys = list(filterA_verb_dict.keys())\n",
    "\n",
    "filter_A_copied_verb = []\n",
    "\n",
    "for sent in df4_clau_one['etri_sent']:\n",
    "    if sent in filter_A_verb_dict_keys:\n",
    "        filter_A_copied_verb.append(filterA_verb_dict[sent])\n",
    "\n",
    "# print(len(filter_A_copied_verb)) # 365 richtig\n",
    "\n",
    "# df4_clau_one에서 etri_sent가 df2_FilterA_root_etri_sent_val_list인 것만 df로 우선 만들어야 함 (464 -> 365)\n",
    "\n",
    "df4_clau_one_365 = df4_clau_one[df4_clau_one.etri_sent.isin(df2_FilterA_root_etri_sent_val_list)]\n",
    "# print(df4_clau_one_365.shape) # (365, 21) richtig\n",
    "\n",
    "\n",
    "# 열이 늘어나야 함! 21 - 22 - 23             \n",
    "clau_one_365_add_subject_by_rule = df4_clau_one_365.assign(copied_subject_by_rule = filter_A_copied_subject_by_rule) \n",
    "clau_one_365_add_subject_and_verb = clau_one_365_add_subject_by_rule.assign(copied_verb = filter_A_copied_verb) \n",
    "\n",
    "# print(clau_one_365_add_subject_by_rule.shape) # (365, 22) richtig\n",
    "# print(clau_one_365_add_subject_and_verb.shape) # (365, 23) richtig\n",
    "\n",
    "# clau_one_365_add_subject_and_verb.to_csv(\"clau_one_365_add_subject_and_verb_310323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# df4_clau_one.to_csv(\"df4_clau_one_020423.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 23)\n"
     ]
    }
   ],
   "source": [
    "# 365개 대상으로 copied_subject_by_rule와 zero_referents 비교 020423\n",
    "\n",
    "# 이 중 copied_subject_by_rule가 none인 20개 case도 manuell하게 주어가 같은지 확인해야 함\n",
    "\n",
    "\n",
    "# 1. copied_subject_by_rule '1ph-' / zero_referents '1ph-'\n",
    "\n",
    "clau_one_365_add_subject_and_verb_wir = clau_one_365_add_subject_and_verb[clau_one_365_add_subject_and_verb['copied_subject_by_rule'] == '1ph-']\n",
    "# print(clau_one_365_add_subject_and_verb_wir.shape) # (4, 23) richtig\n",
    "\n",
    "clau_one_365_add_subject_and_verb_wir_match = clau_one_365_add_subject_and_verb_wir[clau_one_365_add_subject_and_verb_wir['zero_referents'] == '1ph-']\n",
    "# print(clau_one_365_add_subject_and_verb_wir_match.shape) # (4, 23) richtig\n",
    "\n",
    "# '1ph-'(wir)가 copy된 case는 총 4으로 이 중 4개 (100%)가 주어를 공유하는 것으로 분석됨\n",
    "\n",
    "# 2. copied_subject_by_rule '1sh+' / zero_referents '1ph+' / 1sh+\n",
    "\n",
    "clau_one_365_add_subject_and_verb_ich = clau_one_365_add_subject_and_verb[clau_one_365_add_subject_and_verb['copied_subject_by_rule'] == '1sh+']\n",
    "# print(clau_one_365_add_subject_and_verb_ich.shape) # (200, 23) richtig\n",
    "\n",
    "zero_referents_first_person = ['1ph+', '1sh+']\n",
    "\n",
    "clau_one_365_first_person_match = clau_one_365_add_subject_and_verb_ich[clau_one_365_add_subject_and_verb_ich.zero_referents.isin(zero_referents_first_person) == True]\n",
    "\n",
    "# print(clau_one_365_first_person_match.shape) # (188, 23) richtig\n",
    "\n",
    "# right prediction: 200개 중 188개 match (94%)\n",
    "\n",
    "clau_one_365_first_person_not_match = clau_one_365_add_subject_and_verb_ich[clau_one_365_add_subject_and_verb_ich.zero_referents.isin(zero_referents_first_person) == False]\n",
    "\n",
    "# print(clau_one_365_first_person_not_match.shape) # (12, 23) 200 - 188 = 12 richtig\n",
    "\n",
    "# wrong prediction: 200개 중 12 (6%)\n",
    "\n",
    "# '1sh+'(ich)가 copy된 case는 총 200으로 이 중 188개 (94%)가 주어를 공유하는 것으로 분석됐고, 12개 (6%)가 주어를 공유하지 않는 것으로 나타남\n",
    "# clau_one_365_first_person_match와 clau_one_365_first_person_not_match를 각각 추가 열로 넣을 수 있다면 이걸 통해서 예문, conjunction 다 뽑을 수 있음 020423\n",
    "\n",
    "\n",
    "# 3. copied_subject_by_rule '2sh+' / zero_referents '2sh+' \n",
    "\n",
    "clau_one_365_add_subject_and_verb_Sie = clau_one_365_add_subject_and_verb[clau_one_365_add_subject_and_verb['copied_subject_by_rule'] == '2sh+']\n",
    "# print(clau_one_365_add_subject_and_verb_Sie.shape) # (141, 23) richtig\n",
    "\n",
    "clau_one_365_add_subject_and_verb_Sie_match = clau_one_365_add_subject_and_verb_Sie[clau_one_365_add_subject_and_verb_Sie['zero_referents'] == '2sh+']\n",
    "# print(clau_one_365_add_subject_and_verb_Sie_match.shape) # (132, 23) richtig\n",
    "\n",
    "# right prediction: 141개 중 132개 match (93,62%)\n",
    "\n",
    "clau_one_365_add_subject_and_verb_Sie_not_match = clau_one_365_add_subject_and_verb_Sie[clau_one_365_add_subject_and_verb_Sie['zero_referents'] != '2sh+']\n",
    "print(clau_one_365_add_subject_and_verb_Sie_not_match.shape) # (9, 23) 141 - 132 richtig\n",
    "\n",
    "# wrong prediction: 141개 중 9개 (6,38%)\n",
    "\n",
    "# '1sh+'(Sie)가 copy된 case는 총 141으로 이 중 132개 (93,62%)가 주어를 공유하는 것으로 분석됐고, 9개 (6,38%)가 주어를 공유하지 않는 것으로 나타남\n",
    "# clau_one_365_first_person_match와 clau_one_365_first_person_not_match를 각각 추가 열로 넣을 수 있다면 이걸 통해서 예문, conjunction 다 뽑을 수 있음 020423\n",
    "\n",
    "\n",
    "# 4. copied_subject_by_rule 'none' \n",
    "\n",
    "clau_one_365_add_subject_and_verb_none = clau_one_365_add_subject_and_verb[clau_one_365_add_subject_and_verb['copied_subject_by_rule'] == 'none']\n",
    "# print(clau_one_365_add_subject_and_verb_none.shape) # (20, 23) richtig\n",
    "\n",
    "# 이 중 copied_subject_by_rule가 none인 20개 case도 manuell하게 주어가 같은지 확인해야 함\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# df2_NA_root_first_person_match = df2_NA_root_ich[df2_NA_root_ich.zero_referents.isin(first_person_list) == True]\n",
    "# df2_NA_root_second_person_match = df2_NA_root_Sie[df2_NA_root_Sie.zero_referents.isin(second_person_list) == True]\n",
    "\n",
    "# # df2_NA_root_first_person_match.shape # (400, 18) richtig\n",
    "# # df2_NA_root_second_person_match.shape # (46, 18) richtig\n",
    "\n",
    "# conditionlist = [\n",
    "#     (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['zero_referents'].isin(first_person_list) == True),\n",
    "#     (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['antecedent'].isin(first_person_ant_list) == True),\n",
    "#     (df2_NA_root['new_rule_root'] == '2sh+') & (df2_NA_root['zero_referents'].isin(second_person_list) == True),\n",
    "#     (df2_NA_root['new_rule_root'] == '1sh+') & (df2_NA_root['zero_referents'].isin(first_person_list) == False),\n",
    "#     (df2_NA_root['new_rule_root'] == '2sh+') & (df2_NA_root['zero_referents'].isin(second_person_list) == False)] \n",
    "\n",
    "# choicelist = ['richtig_first', 'richtig_first_ant', 'richtig_second', 'wrong_first', 'wrong_second']\n",
    "\n",
    "# df2_NA_root['prediction'] = np.select(conditionlist, choicelist, default='N/A')\n",
    "\n",
    "# # df2_NA_root.head()\n",
    "\n",
    "# # df2_NA_root.to_csv(\"df2_NA_check_root_prediction_160323.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clau가 1개인 464개 중 복사할 주어가 있는 364개가 아닌 나머지 99개, S2에 왜 복사할 주어가 없는지 확인해 보기\n",
    "\n",
    "df4_clau_one_99 = df4_clau_one[df4_clau_one.etri_sent.isin(df2_FilterA_root_etri_sent_val_list) == False]\n",
    "\n",
    "# print(df4_clau_one_99.shape) # (99, 21) richtig\n",
    "\n",
    "# df4_clau_one_99.to_csv(\"df4_clau_one_99_020423.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Null이 있다고 annotated된 전체 17,266개의 predicate (df2)을 대상으로 \n",
    "# S1의 sentence ID (df4_clau_one_etri_sent_val_list)와 같은 value를 지니는 root인 predicate이 몇 개나 있는지 확인 020423\n",
    "# 이들은 S2의 Subjekt가 explcit하지 않는 경우임\n",
    "\n",
    "# print(df2.shape) # (17266, 23)\n",
    "# print(len(df4_clau_one_etri_sent_val_list)) # 464\n",
    "\n",
    "# 기존 list는 int였는데, df2의 etri_sent가 str이여서, 기존 list의 요소들을 int에서 str으로 바꿔줌\n",
    "\n",
    "df4_clau_one_etri_sent_val_list_str = list(map(str, df4_clau_one_etri_sent_val_list))\n",
    "# print(len(df4_clau_one_etri_sent_val_list_str)) # 464 richtig\n",
    "\n",
    "df2_root = df2[df2[\"verb_syn\"] == 'root'] \n",
    "# print(df2_root.shape) # (12783, 23)\n",
    "\n",
    "df2_root_clau_one = df2_root[df2_root.etri_sent.isin(df4_clau_one_etri_sent_val_list_str)]\n",
    "# print(df2_root_clau_one.shape) # (431, 23) 464 중 431은 92,89%\n",
    "\n",
    "# df2_root_clau_one.to_csv(\"df2_root_clau_one_020423.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df2_root_clau_one_etri_sent = df2_root_clau_one.etri_sent\n",
    "df2_root_clau_one_etri_sent_val = df2_root_clau_one_etri_sent.values \n",
    "df2_root_clau_one_etri_sent_val_list = df2_root_clau_one_etri_sent_val.tolist() \n",
    "\n",
    "# print(len(df2_root_clau_one_etri_sent_val_list)) # 431 ricthig, type: str\n",
    "\n",
    "# print(len(set(df4_clau_one_etri_sent_val_list_str))) # 464 richtig\n",
    "# print(len(set(df2_root_clau_one_etri_sent_val_list))) # 431 richtig\n",
    "\n",
    "complement = list(set(df4_clau_one_etri_sent_val_list_str) - set(df2_root_clau_one_etri_sent_val_list)) # type: str\n",
    "# print(len(complement)) # 33 richtig\n",
    "\n",
    "# 464에서 431 뺀 33 케이스가 explicit한 Subjekt가 있을 확률이 높음 \n",
    "# 따로 df로 만들어서 확인 \n",
    "\n",
    "# df2_root_explicit_sbj_check = df2_root[df2_root.etri_sent.isin(complement)]\n",
    "# print(df2_root_explicit_sbj_check.shape)\n",
    "\n",
    "df2_explicit_sbj_check = df2[df2.etri_sent.isin(complement)]\n",
    "# print(df2_explicit_sbj_check.shape) # (35, 23) 왜 33 아니지?? df 만들어서 확\n",
    "\n",
    "df2_explicit_sbj_check_sent_val = df2_explicit_sbj_check['etri_sent'].value_counts()\n",
    "# print(df2_explicit_sbj_check_sent_val)\n",
    "\n",
    "# 15292    2\n",
    "# 18468    2\n",
    "# 이 두 개 확인해보기 (etri_sent가 두 개 있는 케이스)\n",
    "\n",
    "# 35개 중에서도 'sent_dep'에 subj 있는 것만 체크하면 18개 030423\n",
    "# 실제로 16개임 (subj_yes: 1) / 18468의 경우 root의 주어가 실제로 있는 것이 아님\n",
    "# 16개 중 4개만 S2의 explicit한 주어를 공유함\n",
    "\n",
    "# 나머지 19개 (subj_yes: 0) 15292와 18468 포함 \n",
    "# 실제로 17개로 쳐야 함\n",
    "# 따라서 전체는 16 + 17 = 33 richtig\n",
    "# 17개 중에는 9개만 주어를 공유함 \n",
    "\n",
    "# print(len(set(complement))) # 33\n",
    "\n",
    "# df2_explicit_sbj_check.to_csv(\"df2_explicit_sbj_check_020423.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S2의 Subjekt가 explcit하지 않는 경우, 만약 이들이 Resolutionsystem의 input이라면 규칙에 의해 할당한 referent를 복사하는 게 가능할 듯\n",
    "# 이것이 몇 퍼센트가 되는지 확인해야 함\n",
    "\n",
    "# input이 아닌 경우 manuell하게 S1과 S2의 주어가 같은지 확인해 봐야 할 듯\n",
    "\n",
    "# 431개는 S1과 S2 둘 다 Nullsubjekt가 있을 확률이 높으므로, 둘의 zero_referents, antecedent를 비교해봐야 함\n",
    "\n",
    "# for element in df2_root_clau_one_etri_sent_val_list:\n",
    "#     print(type(element)) # list의 요소 type 'str'\n",
    "\n",
    "# print(df4_clau_one.dtypes) # etri_sent type 'int64'\n",
    "\n",
    "df2_root_clau_one_etri_sent_val_list_int = [int (i) for i in df2_root_clau_one_etri_sent_val_list] \n",
    "# print(len(df2_root_clau_one_etri_sent_val_list_int)) # 431 richtig\n",
    "\n",
    "# df4_clau_one에서 df2_root_clau_one_etri_sent_val_list에 속하는 case의 df 우선 만들어 놓기, sollte 431 / 030423\n",
    "# df4_clau_one_431\n",
    "\n",
    "df4_clau_one_431 = df4_clau_one[df4_clau_one.etri_sent.isin(df2_root_clau_one_etri_sent_val_list_int)]\n",
    "# print(df4_clau_one_431.shape) # (431, 21) richtig\n",
    "\n",
    "# df2_root_clau_one에서 root의 verb, 주어, zero_referents, antencedents 추가하기 (기존 코드 활용) 030423\n",
    "# 근데 subject_by_rule 아니고, zero_referents랑 antecedents 불러와서 이 것을 비교할 것임\n",
    "\n",
    "# print(df2_root_clau_one.shape) # (431, 23)\n",
    "\n",
    "df2_root_clau_one_zero_referents = df2_root_clau_one.zero_referents\n",
    "df2_root_clau_one_zero_referents_val = df2_root_clau_one_zero_referents.values \n",
    "df2_root_clau_one_zero_referents_val_list = df2_root_clau_one_zero_referents_val.tolist() \n",
    "# print(len(df2_root_clau_one_zero_referents_val_list)) # None 포함되어 있음 / 431\n",
    "\n",
    "df2_root_clau_one_antecedent = df2_root_clau_one.antecedent\n",
    "df2_root_clau_one_antecedent_val = df2_root_clau_one_antecedent.values \n",
    "df2_root_clau_one_antecedent_val_list = df2_root_clau_one_antecedent_val.tolist() \n",
    "# pint(len(df2_root_clau_one_antecedent_val_list)) # None 포함되어 있음 / 431\n",
    "\n",
    "df2_root_clau_one_verb = df2_root_clau_one.verb\n",
    "df2_root_clau_one_verb_val = df2_root_clau_one_verb.values \n",
    "df2_root_clau_one_verb_val_list = df2_root_clau_one_verb_val.tolist() \n",
    "# print(len(df2_root_clau_one_verb_val_list)) # 431\n",
    "\n",
    "df2_root_clau_one_zero_referents_dict = dict(zip(df2_root_clau_one_etri_sent_val_list_int, df2_root_clau_one_zero_referents_val_list)) \n",
    "# print(len(df2_root_clau_one_zero_referents_dict)) # 431 richtig \n",
    "\n",
    "df2_root_clau_one_antecedent_dict = dict(zip(df2_root_clau_one_etri_sent_val_list_int, df2_root_clau_one_antecedent_val_list)) \n",
    "# print(len(df2_root_clau_one_antecedent_dict)) # 431 richtig \n",
    "\n",
    "df2_root_clau_one_verb_dict = dict(zip(df2_root_clau_one_etri_sent_val_list_int, df2_root_clau_one_verb_val_list)) \n",
    "# print(len(df2_root_clau_one_verb_dict)) # 431 richtig \n",
    "\n",
    "\n",
    "# df4_clau_one_431에 root에 있던 정보인 zero_referents, antecedent, verb copy하기 030423\n",
    "\n",
    "df2_root_clau_one_zero_referents_keys = list(df2_root_clau_one_zero_referents_dict.keys())\n",
    "\n",
    "copied_zero_referents = [] \n",
    "\n",
    "for sent in df4_clau_one_431['etri_sent']:\n",
    "    if sent in df2_root_clau_one_zero_referents_keys:\n",
    "        copied_zero_referents.append(df2_root_clau_one_zero_referents_dict[sent])\n",
    "    \n",
    "# print(len(copied_zero_referents)) # 431 richtig\n",
    "\n",
    "df2_root_clau_one_antecedent_keys = list(df2_root_clau_one_antecedent_dict.keys())\n",
    "\n",
    "copied_antecedent = [] \n",
    "\n",
    "for sent in df4_clau_one_431['etri_sent']:\n",
    "    if sent in df2_root_clau_one_antecedent_keys:\n",
    "        copied_antecedent.append(df2_root_clau_one_antecedent_dict[sent])\n",
    "    \n",
    "# print(len(copied_antecedent)) # 431 richtig\n",
    "\n",
    "df2_root_clau_one_verb_keys = list(df2_root_clau_one_verb_dict.keys())\n",
    "\n",
    "copied_verb = [] \n",
    "\n",
    "for sent in df4_clau_one_431['etri_sent']:\n",
    "    if sent in df2_root_clau_one_verb_keys:\n",
    "        copied_verb.append(df2_root_clau_one_verb_dict[sent])\n",
    "    \n",
    "# print(len(copied_verb)) # 431 richtig\n",
    "\n",
    "\n",
    "# df4_clau_one_431의 열이 늘어나야 함! (431, 21) - 22 - 23 - 24           \n",
    "df4_clau_one_431_zero_referents = df4_clau_one_431.assign(copied_zero_referents = copied_zero_referents) \n",
    "df4_clau_one_431_antecedent = df4_clau_one_431_zero_referents.assign(copied_antecedent = copied_antecedent) \n",
    "df4_clau_one_431_verb = df4_clau_one_431_antecedent.assign(copied_verb = copied_verb) \n",
    "\n",
    "# print(df4_clau_one_431_zero_referents.shape) # (431, 22) richtig\n",
    "# print(df4_clau_one_431_antecedent.shape) # (431, 23) richtig\n",
    "# print(df4_clau_one_431_verb.shape) # (431, 24) richtig\n",
    "\n",
    "\n",
    "# df4_clau_one_431_verb.to_csv(\"df4_clau_one_431_copied_done_030423.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 첫 번째로 할 것은 우선 zero_referents가 simple하게 같은 case가 아마 대다수, 이걸 알아내고, 나머지 제외한 거 어떻게 implement할 건지 확인할 수 있음 030423\n",
    "\n",
    "# case 1: zero_referents == copied_zero_referents\n",
    "\n",
    "df4_clau_one_431_verb['match'] = df4_clau_one_431_verb['etri_sent'][(df4_clau_one_431_verb['zero_referents'] == df4_clau_one_431_verb['copied_zero_referents'])]\n",
    "    \n",
    "\n",
    "# df4_clau_one_431_verb.to_csv(\"df4_clau_one_431_string_match_check_030423.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 431개 중 412개 zero_referents string match 됨 (95,59%)\n",
    "\n",
    "# 나머지 19개 maneull check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etri_sent</th>\n",
       "      <th>sent</th>\n",
       "      <th>verb</th>\n",
       "      <th>share_yes</th>\n",
       "      <th>applied_rule</th>\n",
       "      <th>subject_by_rule</th>\n",
       "      <th>zero_referents</th>\n",
       "      <th>antecedent</th>\n",
       "      <th>group</th>\n",
       "      <th>rule_order</th>\n",
       "      <th>rule</th>\n",
       "      <th>connective</th>\n",
       "      <th>base_morpheme</th>\n",
       "      <th>base_morpheme_second</th>\n",
       "      <th>connective_true</th>\n",
       "      <th>zp_hono</th>\n",
       "      <th>zp_mood</th>\n",
       "      <th>sent_dep</th>\n",
       "      <th>subj_yes</th>\n",
       "      <th>zp_markable_ID</th>\n",
       "      <th>zero_type</th>\n",
       "      <th>tense_zp</th>\n",
       "      <th>verb_syn</th>\n",
       "      <th>verb_head</th>\n",
       "      <th>Filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16250</th>\n",
       "      <td>6198</td>\n",
       "      <td>아래층에서 담배를 펴서 창문으로 연기가 들어옵니다 .</td>\n",
       "      <td>펴서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>사람</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>펴</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'modi', 'subj', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_4760</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15871</th>\n",
       "      <td>5480</td>\n",
       "      <td>가끔 운송 시 내부 충격을 받아서 이런 일이 발생하기도합니다 .</td>\n",
       "      <td>받아서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>음반을</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>받</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'modi', 'dobj', 'clau...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5176</td>\n",
       "      <td>inter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2094</th>\n",
       "      <td>10605</td>\n",
       "      <td>어제 너무 많이 먹어서 소화가 안됩니다 .</td>\n",
       "      <td>먹어서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>먹</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'clau', 'subj', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_4910</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1657</th>\n",
       "      <td>11391</td>\n",
       "      <td>지금 요리를 다 치워서 확인이 안되는데 .</td>\n",
       "      <td>치워서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>치우</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'modi', 'clau', 'subj', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5475</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>790</th>\n",
       "      <td>10170</td>\n",
       "      <td>술을 마셔서그런지 기분이 좋군요 .</td>\n",
       "      <td>마셔서그런지</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>마시</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'subj', 'root', 'punc']</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5266</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6523</th>\n",
       "      <td>17891</td>\n",
       "      <td>네, 아침부터 팔려서 지금 딱 세 개 상품이 남았어요 .</td>\n",
       "      <td>팔려서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>쥬얼리</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>팔리</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'modi', 'modi', 'modi...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_4664</td>\n",
       "      <td>inter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>9</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13332</th>\n",
       "      <td>29532</td>\n",
       "      <td>등산 중에 발을 다쳐서 친구가 못 움직이고있습니다 .</td>\n",
       "      <td>다쳐서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>친구</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>다치</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'dobj', 'clau', 'subj', 'modi...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5110</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6333</th>\n",
       "      <td>18468</td>\n",
       "      <td>잠시 저쪽에 가서 약을 발라도 괜찮을까요 ?</td>\n",
       "      <td>가서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>가</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'dobj', 'subj', 'root...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5100</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>261</td>\n",
       "      <td>방해해서 죄송합니다 .</td>\n",
       "      <td>방해해서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>방해하</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5136</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3483</th>\n",
       "      <td>12909</td>\n",
       "      <td>친절하신 분 만나서 기분좋네요 .</td>\n",
       "      <td>만나서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>만나</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5082</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4400</th>\n",
       "      <td>15292</td>\n",
       "      <td>두 블록에서 오른쪽으로 꺾어서 쭉 가면된다는 말씀 이죠 ?</td>\n",
       "      <td>꺾어서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>꺾</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'clau', 'modi', 'modi...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5343</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8771</th>\n",
       "      <td>21772</td>\n",
       "      <td>보관과정에서 얼음에 싸여서 그런 겁니다 겁니다 .</td>\n",
       "      <td>싸여서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>전복들이</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>싸이</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'modi', 'scom', 'root...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5449</td>\n",
       "      <td>inter</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11243</th>\n",
       "      <td>25808</td>\n",
       "      <td>소란을 떨어 죄송합니다 .</td>\n",
       "      <td>떨어</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>떨</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5001</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11810</th>\n",
       "      <td>26634</td>\n",
       "      <td>정확히 못 가르쳐줘서 미안해요 .</td>\n",
       "      <td>가르쳐줘서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>가르쳐주</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5042</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15745</th>\n",
       "      <td>5214</td>\n",
       "      <td>짧은 시간에 여러 곳을 둘러볼수있어서 좋은 상품 입니다 .</td>\n",
       "      <td>둘러볼수있어서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>둘러보</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'dobj', 'clau', 'modi...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_4977</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>8</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4851</th>\n",
       "      <td>14813</td>\n",
       "      <td>알려주어서 고마워요 .</td>\n",
       "      <td>알려주어서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>알리</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5061</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>2</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6516</th>\n",
       "      <td>18080</td>\n",
       "      <td>네, 도와줘서 감사해요 .</td>\n",
       "      <td>도와줘서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>도와주</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_4781</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7493</th>\n",
       "      <td>18820</td>\n",
       "      <td>네, 밖을 볼수있어서 다행이군요 .</td>\n",
       "      <td>볼수있어서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>보</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_4984</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11648</th>\n",
       "      <td>26101</td>\n",
       "      <td>네, 잃어버리지않아서 다행이에요 .</td>\n",
       "      <td>잃어버리지않아서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>잃어버리</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5198</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13700</th>\n",
       "      <td>29453</td>\n",
       "      <td>오래전부터 불교를 국교로 지정해서 그렇습니다 .</td>\n",
       "      <td>지정해서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>사람</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>지정하</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'modi', 'clau', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5057</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13712</th>\n",
       "      <td>29465</td>\n",
       "      <td>한국의 역사와 멋을 외국분에게 알릴수있어서 저도 좋은 기회 기회였던 거 같습니다 .</td>\n",
       "      <td>알릴수있어서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>알리</td>\n",
       "      <td>(아/어)서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'dobj', 'modi', 'clau', 'scom...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5065</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>11</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6301</th>\n",
       "      <td>18678</td>\n",
       "      <td>아니요, 잠시후에 바나나를 팔러 아이들이 올겁니다 .</td>\n",
       "      <td>팔러</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>아이들</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>팔</td>\n",
       "      <td>(으)러</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'dobj', 'clau', 'subj', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5284</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15482</th>\n",
       "      <td>5397</td>\n",
       "      <td>경복궁 내부를 관람하려면 표가 필요한 거 군요 .</td>\n",
       "      <td>관람하려면</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>사람들</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>관람하</td>\n",
       "      <td>(으)려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'subj', 'modi', 'scom...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5115</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2650</th>\n",
       "      <td>12470</td>\n",
       "      <td>한국으로 통화하려면 이게 젤 싼거죠 ?</td>\n",
       "      <td>통화하려면</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>통화하</td>\n",
       "      <td>(으)려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'subj', 'sub2', 'root', 'punc']</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5529</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8564</th>\n",
       "      <td>22015</td>\n",
       "      <td>그런데 주문진으로 가려면 이 방향이 맞나요 ?</td>\n",
       "      <td>가려면</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>가</td>\n",
       "      <td>(으)려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'modi', 'subj', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5592</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9820</th>\n",
       "      <td>24210</td>\n",
       "      <td>유스호스텔을 이용하려면 협회회원 카드가 필요합니다 .</td>\n",
       "      <td>이용하려면</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>이용하</td>\n",
       "      <td>(으)려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'modi', 'subj', 'root', 'punc']</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5287</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4610</th>\n",
       "      <td>15440</td>\n",
       "      <td>객실에서 인터넷을 사용하려면 어떻게 해야하나요 ?</td>\n",
       "      <td>사용하려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>사용하</td>\n",
       "      <td>(으)려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'modi', 'root', 'punc']</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5436</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7310</th>\n",
       "      <td>19647</td>\n",
       "      <td>그건 잘 모르겠고, 창가 쪽 입니다 .</td>\n",
       "      <td>모르겠고,</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>모르</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['subj', 'modi', 'clau', 'modi', 'scom', 'root...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5550</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5547</th>\n",
       "      <td>16250</td>\n",
       "      <td>그 시간엔 다 나가고 집에 아무도 없을텐데요 .</td>\n",
       "      <td>나가고</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>사람들</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>나가</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'clau', 'modi', 'subj...</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_5087</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>7</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16516</th>\n",
       "      <td>6305</td>\n",
       "      <td>지난주에 예약했고 여권은 여기 있습니다 .</td>\n",
       "      <td>예약했고</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>예약하</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'subj', 'modi', 'root', 'punc']</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_4834</td>\n",
       "      <td>extra</td>\n",
       "      <td>past</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13931</th>\n",
       "      <td>3376</td>\n",
       "      <td>아, 일본에 있는 딸집에서 요양을하고 옵니다 .</td>\n",
       "      <td>요양을하고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>요양을하</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'modi', 'modi', 'clau', 'root...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5029</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>276</td>\n",
       "      <td>잘 모르겠고, 엘리베이터 옆에 있는 화장실 인데요 .</td>\n",
       "      <td>모르겠고,</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>모르</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'modi', 'modi', 'modi', 'scom...</td>\n",
       "      <td>0</td>\n",
       "      <td>markable_5140</td>\n",
       "      <td>extra</td>\n",
       "      <td>pres</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10156</th>\n",
       "      <td>23371</td>\n",
       "      <td>신청하고나서 일정을 제가 선택할수있나요 ?</td>\n",
       "      <td>신청하고나서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>신청하</td>\n",
       "      <td>고나서</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['clau', 'dobj', 'subj', 'root', 'punc']</td>\n",
       "      <td>1</td>\n",
       "      <td>markable_4821</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       etri_sent                                            sent      verb  \\\n",
       "16250       6198                   아래층에서 담배를 펴서 창문으로 연기가 들어옵니다 .        펴서   \n",
       "15871       5480             가끔 운송 시 내부 충격을 받아서 이런 일이 발생하기도합니다 .       받아서   \n",
       "2094       10605                         어제 너무 많이 먹어서 소화가 안됩니다 .       먹어서   \n",
       "1657       11391                         지금 요리를 다 치워서 확인이 안되는데 .       치워서   \n",
       "790        10170                             술을 마셔서그런지 기분이 좋군요 .    마셔서그런지   \n",
       "6523       17891                 네, 아침부터 팔려서 지금 딱 세 개 상품이 남았어요 .       팔려서   \n",
       "13332      29532                   등산 중에 발을 다쳐서 친구가 못 움직이고있습니다 .       다쳐서   \n",
       "6333       18468                        잠시 저쪽에 가서 약을 발라도 괜찮을까요 ?        가서   \n",
       "21           261                                    방해해서 죄송합니다 .      방해해서   \n",
       "3483       12909                              친절하신 분 만나서 기분좋네요 .       만나서   \n",
       "4400       15292                두 블록에서 오른쪽으로 꺾어서 쭉 가면된다는 말씀 이죠 ?       꺾어서   \n",
       "8771       21772                     보관과정에서 얼음에 싸여서 그런 겁니다 겁니다 .       싸여서   \n",
       "11243      25808                                  소란을 떨어 죄송합니다 .        떨어   \n",
       "11810      26634                              정확히 못 가르쳐줘서 미안해요 .     가르쳐줘서   \n",
       "15745       5214                짧은 시간에 여러 곳을 둘러볼수있어서 좋은 상품 입니다 .   둘러볼수있어서   \n",
       "4851       14813                                    알려주어서 고마워요 .     알려주어서   \n",
       "6516       18080                                  네, 도와줘서 감사해요 .      도와줘서   \n",
       "7493       18820                             네, 밖을 볼수있어서 다행이군요 .     볼수있어서   \n",
       "11648      26101                             네, 잃어버리지않아서 다행이에요 .  잃어버리지않아서   \n",
       "13700      29453                      오래전부터 불교를 국교로 지정해서 그렇습니다 .      지정해서   \n",
       "13712      29465  한국의 역사와 멋을 외국분에게 알릴수있어서 저도 좋은 기회 기회였던 거 같습니다 .    알릴수있어서   \n",
       "6301       18678                   아니요, 잠시후에 바나나를 팔러 아이들이 올겁니다 .        팔러   \n",
       "15482       5397                     경복궁 내부를 관람하려면 표가 필요한 거 군요 .     관람하려면   \n",
       "2650       12470                           한국으로 통화하려면 이게 젤 싼거죠 ?     통화하려면   \n",
       "8564       22015                       그런데 주문진으로 가려면 이 방향이 맞나요 ?       가려면   \n",
       "9820       24210                   유스호스텔을 이용하려면 협회회원 카드가 필요합니다 .     이용하려면   \n",
       "4610       15440                     객실에서 인터넷을 사용하려면 어떻게 해야하나요 ?     사용하려면   \n",
       "7310       19647                           그건 잘 모르겠고, 창가 쪽 입니다 .     모르겠고,   \n",
       "5547       16250                      그 시간엔 다 나가고 집에 아무도 없을텐데요 .       나가고   \n",
       "16516       6305                         지난주에 예약했고 여권은 여기 있습니다 .      예약했고   \n",
       "13931       3376                      아, 일본에 있는 딸집에서 요양을하고 옵니다 .     요양을하고   \n",
       "56           276                   잘 모르겠고, 엘리베이터 옆에 있는 화장실 인데요 .     모르겠고,   \n",
       "10156      23371                         신청하고나서 일정을 제가 선택할수있나요 ?    신청하고나서   \n",
       "\n",
       "       share_yes  applied_rule  subject_by_rule zero_referents antecedent  \\\n",
       "16250          0           NaN              NaN             사람        NaN   \n",
       "15871          0           NaN              NaN            NaN        음반을   \n",
       "2094           0           NaN              NaN           1sh+        NaN   \n",
       "1657           0           NaN              NaN           1sh+        NaN   \n",
       "790            0           NaN              NaN           1sh+        NaN   \n",
       "6523           1           NaN              NaN            NaN        쥬얼리   \n",
       "13332          1           NaN              NaN             친구        NaN   \n",
       "6333           1           NaN              NaN           2sh+        NaN   \n",
       "21             1           NaN              NaN           1sh+        NaN   \n",
       "3483           1           NaN              NaN           1sh+        NaN   \n",
       "4400           1           NaN              NaN           1sh+        NaN   \n",
       "8771           1           NaN              NaN            NaN       전복들이   \n",
       "11243          1           NaN              NaN           1sh+        NaN   \n",
       "11810          1           NaN              NaN           1sh+        NaN   \n",
       "15745          0           NaN              NaN           2sh+        NaN   \n",
       "4851           0           NaN              NaN           2sh+        NaN   \n",
       "6516           0           NaN              NaN           2sh+        NaN   \n",
       "7493           0           NaN              NaN           2sh+        NaN   \n",
       "11648          0           NaN              NaN           1sh+        NaN   \n",
       "13700          0           NaN              NaN             사람        NaN   \n",
       "13712          0           NaN              NaN           1sh+        NaN   \n",
       "6301           1           NaN              NaN            아이들        NaN   \n",
       "15482          0           NaN              NaN            사람들        NaN   \n",
       "2650           0           NaN              NaN           1sh+        NaN   \n",
       "8564           0           NaN              NaN           1sh+        NaN   \n",
       "9820           0           NaN              NaN           2sh+        NaN   \n",
       "4610           1           NaN              NaN           1sh+        NaN   \n",
       "7310           0           NaN              NaN           1sh+        NaN   \n",
       "5547           0           NaN              NaN            사람들        NaN   \n",
       "16516          0           NaN              NaN           1sh+        NaN   \n",
       "13931          1           NaN              NaN           1sh+        NaN   \n",
       "56             0           NaN              NaN           1sh+        NaN   \n",
       "10156          1           NaN              NaN           1sh+        NaN   \n",
       "\n",
       "       group  rule_order  rule connective base_morpheme base_morpheme_second  \\\n",
       "16250    NaN         NaN   NaN    class_A             펴               (아/어)서   \n",
       "15871    NaN         NaN   NaN    class_A             받               (아/어)서   \n",
       "2094     NaN         NaN   NaN    class_A             먹               (아/어)서   \n",
       "1657     NaN         NaN   NaN    class_A            치우               (아/어)서   \n",
       "790      NaN         NaN   NaN    class_A            마시               (아/어)서   \n",
       "6523     NaN         NaN   NaN    class_A            팔리               (아/어)서   \n",
       "13332    NaN         NaN   NaN    class_A            다치               (아/어)서   \n",
       "6333     NaN         NaN   NaN    class_A             가               (아/어)서   \n",
       "21       NaN         NaN   NaN    class_A           방해하               (아/어)서   \n",
       "3483     NaN         NaN   NaN    class_A            만나               (아/어)서   \n",
       "4400     NaN         NaN   NaN    class_A             꺾               (아/어)서   \n",
       "8771     NaN         NaN   NaN    class_A            싸이               (아/어)서   \n",
       "11243    NaN         NaN   NaN    class_A             떨               (아/어)서   \n",
       "11810    NaN         NaN   NaN    class_A          가르쳐주               (아/어)서   \n",
       "15745    NaN         NaN   NaN    class_A           둘러보               (아/어)서   \n",
       "4851     NaN         NaN   NaN    class_A            알리               (아/어)서   \n",
       "6516     NaN         NaN   NaN    class_A           도와주               (아/어)서   \n",
       "7493     NaN         NaN   NaN    class_A             보               (아/어)서   \n",
       "11648    NaN         NaN   NaN    class_A          잃어버리               (아/어)서   \n",
       "13700    NaN         NaN   NaN    class_A           지정하               (아/어)서   \n",
       "13712    NaN         NaN   NaN    class_A            알리               (아/어)서   \n",
       "6301     NaN         NaN   NaN    class_A             팔                 (으)러   \n",
       "15482    NaN         NaN   NaN    class_A           관람하                (으)려면   \n",
       "2650     NaN         NaN   NaN    class_A           통화하                (으)려면   \n",
       "8564     NaN         NaN   NaN    class_A             가                (으)려면   \n",
       "9820     NaN         NaN   NaN    class_A           이용하                (으)려면   \n",
       "4610     NaN         NaN   NaN    class_A           사용하                (으)려면   \n",
       "7310     NaN         NaN   NaN    class_A            모르                    고   \n",
       "5547     NaN         NaN   NaN    class_A            나가                    고   \n",
       "16516    NaN         NaN   NaN    class_A           예약하                    고   \n",
       "13931    NaN         NaN   NaN    class_A          요양을하                    고   \n",
       "56       NaN         NaN   NaN    class_A            모르                    고   \n",
       "10156    NaN         NaN   NaN    class_A           신청하                  고나서   \n",
       "\n",
       "       connective_true  zp_hono  zp_mood  \\\n",
       "16250                1      NaN      NaN   \n",
       "15871                1      NaN      NaN   \n",
       "2094                 1      NaN      NaN   \n",
       "1657                 1      NaN      NaN   \n",
       "790                  0      NaN      NaN   \n",
       "6523                 1      NaN      NaN   \n",
       "13332                1      NaN      NaN   \n",
       "6333                 1      NaN      NaN   \n",
       "21                   1      NaN      NaN   \n",
       "3483                 1      NaN      NaN   \n",
       "4400                 1      NaN      NaN   \n",
       "8771                 1      NaN      NaN   \n",
       "11243                1      NaN      NaN   \n",
       "11810                1      NaN      NaN   \n",
       "15745                1      NaN      NaN   \n",
       "4851                 1      NaN      NaN   \n",
       "6516                 1      NaN      NaN   \n",
       "7493                 1      NaN      NaN   \n",
       "11648                1      NaN      NaN   \n",
       "13700                1      NaN      NaN   \n",
       "13712                1      NaN      NaN   \n",
       "6301                 1      NaN      NaN   \n",
       "15482                1      NaN      NaN   \n",
       "2650                 1      NaN      NaN   \n",
       "8564                 1      NaN      NaN   \n",
       "9820                 1      NaN      NaN   \n",
       "4610                 1      NaN      NaN   \n",
       "7310                 1      NaN      NaN   \n",
       "5547                 1      NaN      NaN   \n",
       "16516                1      NaN      NaN   \n",
       "13931                1      NaN      NaN   \n",
       "56                   1      NaN      NaN   \n",
       "10156                0      NaN      NaN   \n",
       "\n",
       "                                                sent_dep  subj_yes  \\\n",
       "16250  ['modi', 'dobj', 'clau', 'modi', 'subj', 'root...         1   \n",
       "15871  ['modi', 'modi', 'modi', 'modi', 'dobj', 'clau...         1   \n",
       "2094   ['modi', 'modi', 'modi', 'clau', 'subj', 'root...         1   \n",
       "1657   ['modi', 'dobj', 'modi', 'clau', 'subj', 'root...         1   \n",
       "790             ['dobj', 'clau', 'subj', 'root', 'punc']         1   \n",
       "6523   ['modi', 'modi', 'clau', 'modi', 'modi', 'modi...         1   \n",
       "13332  ['modi', 'modi', 'dobj', 'clau', 'subj', 'modi...         1   \n",
       "6333   ['modi', 'modi', 'clau', 'dobj', 'subj', 'root...         0   \n",
       "21                              ['clau', 'root', 'punc']         0   \n",
       "3483            ['modi', 'dobj', 'clau', 'root', 'punc']         0   \n",
       "4400   ['modi', 'modi', 'modi', 'clau', 'modi', 'modi...         0   \n",
       "8771   ['modi', 'modi', 'clau', 'modi', 'scom', 'root...         0   \n",
       "11243                   ['dobj', 'clau', 'root', 'punc']         0   \n",
       "11810           ['modi', 'modi', 'clau', 'root', 'punc']         0   \n",
       "15745  ['modi', 'modi', 'modi', 'dobj', 'clau', 'modi...         0   \n",
       "4851                            ['clau', 'root', 'punc']         0   \n",
       "6516                    ['modi', 'clau', 'root', 'punc']         0   \n",
       "7493            ['modi', 'dobj', 'clau', 'root', 'punc']         0   \n",
       "11648                   ['modi', 'clau', 'root', 'punc']         0   \n",
       "13700   ['modi', 'dobj', 'modi', 'clau', 'root', 'punc']         0   \n",
       "13712  ['modi', 'modi', 'dobj', 'modi', 'clau', 'scom...         0   \n",
       "6301   ['modi', 'modi', 'dobj', 'clau', 'subj', 'root...         1   \n",
       "15482  ['modi', 'dobj', 'clau', 'subj', 'modi', 'scom...         1   \n",
       "2650    ['modi', 'clau', 'subj', 'sub2', 'root', 'punc']         1   \n",
       "8564   ['modi', 'modi', 'clau', 'modi', 'subj', 'root...         1   \n",
       "9820    ['dobj', 'clau', 'modi', 'subj', 'root', 'punc']         1   \n",
       "4610    ['modi', 'dobj', 'clau', 'modi', 'root', 'punc']         0   \n",
       "7310   ['subj', 'modi', 'clau', 'modi', 'scom', 'root...         1   \n",
       "5547   ['modi', 'modi', 'modi', 'clau', 'modi', 'subj...         1   \n",
       "16516   ['modi', 'clau', 'subj', 'modi', 'root', 'punc']         1   \n",
       "13931  ['modi', 'modi', 'modi', 'modi', 'clau', 'root...         0   \n",
       "56     ['modi', 'clau', 'modi', 'modi', 'modi', 'scom...         0   \n",
       "10156           ['clau', 'dobj', 'subj', 'root', 'punc']         1   \n",
       "\n",
       "      zp_markable_ID zero_type tense_zp verb_syn  verb_head Filter  \n",
       "16250  markable_4760     extra      NaN     clau          6      C  \n",
       "15871  markable_5176     inter      NaN     clau          9      C  \n",
       "2094   markable_4910     extra      NaN     clau          6      C  \n",
       "1657   markable_5475     extra      NaN     clau          6      C  \n",
       "790    markable_5266     extra      NaN     clau          4      C  \n",
       "6523   markable_4664     inter      NaN     clau          9      C  \n",
       "13332  markable_5110     extra      NaN     clau          7      C  \n",
       "6333   markable_5100     extra      NaN     clau          6      C  \n",
       "21     markable_5136     extra      NaN     clau          2      C  \n",
       "3483   markable_5082     extra      NaN     clau          4      C  \n",
       "4400   markable_5343     extra      NaN     clau          6      C  \n",
       "8771   markable_5449     inter      NaN     clau          6      C  \n",
       "11243  markable_5001     extra      NaN     clau          3      C  \n",
       "11810  markable_5042     extra      NaN     clau          4      C  \n",
       "15745  markable_4977     extra      NaN     clau          8      C  \n",
       "4851   markable_5061     extra      NaN     clau          2      C  \n",
       "6516   markable_4781     extra      NaN     clau          3      C  \n",
       "7493   markable_4984     extra      NaN     clau          4      C  \n",
       "11648  markable_5198     extra      NaN     clau          3      C  \n",
       "13700  markable_5057     extra      NaN     clau          5      C  \n",
       "13712  markable_5065     extra      NaN     clau         11      C  \n",
       "6301   markable_5284     extra      NaN     clau          6      C  \n",
       "15482  markable_5115     extra      NaN     clau          5      C  \n",
       "2650   markable_5529     extra      NaN     clau          5      C  \n",
       "8564   markable_5592     extra      NaN     clau          6      C  \n",
       "9820   markable_5287     extra      NaN     clau          5      C  \n",
       "4610   markable_5436     extra      NaN     clau          5      C  \n",
       "7310   markable_5550     extra     pres     clau          6      C  \n",
       "5547   markable_5087     extra      NaN     clau          7      C  \n",
       "16516  markable_4834     extra     past     clau          5      C  \n",
       "13931  markable_5029     extra      NaN     clau          6      C  \n",
       "56     markable_5140     extra     pres     clau          5      C  \n",
       "10156  markable_4821     extra      NaN     clau          4      C  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conjunction 관련 implement 050423\n",
    "\n",
    "# 1. 우선 두 개의 csv 불러오기, 왜냐하면 manuell하게 추가된 열들이 존재하기 때문\n",
    "# 1.1. \"df4_clau_one_431_string_match_check_030423.csv\" # 33개\n",
    "# 1.2. \"df2_explicit_sbj_check_020423.csv\" # 431개\n",
    "\n",
    "data_1 = pd.read_csv(\"df4_clau_one_431_string_match_check_030423.csv\", index_col=0)\n",
    "# print(data_1.shape) # (431, 26)\n",
    "\n",
    "data_2 = pd.read_csv(\"df2_explicit_sbj_check_020423.csv\", index_col=0)\n",
    "# print(data_2.shape) # (33, 25) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 1.2에 Fall 1(16개, 'subj_yes': 1)과  Fall 2(17개, 'subj_yes': 0)가 섞여 있음\n",
    "\n",
    "data_2_Fall_1 = data_2[data_2['subj_yes'] == 1]\n",
    "data_2_Fall_2 = data_2[data_2['subj_yes'] == 0]\n",
    "\n",
    "# print(data_2_Fall_1.shape) # (16, 25) richtig\n",
    "# print(data_2_Fall_2.shape) # (17, 25) richtig\n",
    "\n",
    "# Fall 1 (data_2)에서 'share_yes'를 기준으로 'base_morpheme_second'의 value_counts\n",
    "\n",
    "Fall_1_share_yes = data_2_Fall_1[data_2_Fall_1['share_yes'] == 1]\n",
    "# print(Fall_1_share_yes.shape) # (4, 25) richtig\n",
    "\n",
    "Fall_1_share_no = data_2_Fall_1[data_2_Fall_1['share_yes'] == 0]\n",
    "# print(Fall_1_share_no.shape) # (12, 25) richtig\n",
    "\n",
    "\n",
    "# Fall 2 (data_2) 에서 'share_yes'를 기준으로 'base_morpheme_second'의 value_counts\n",
    "\n",
    "data_2_Fall_2_share_yes = data_2_Fall_2[data_2_Fall_2['share_yes'] == 1]\n",
    "# print(data_2_Fall_2_share_yes.shape) # (9, 25) # richtig\n",
    "\n",
    "data_2_Fall_2_share_no = data_2_Fall_2[data_2_Fall_2['share_yes'] == 0]\n",
    "# print(data_2_Fall_2_share_no.shape) # (8, 25) richtig\n",
    "\n",
    "# data_2 전체: 4 + 12 + 9 + 8 = 33 richtig\n",
    "\n",
    "# Fall 2 (data_1) 에서 'share_yes'를 기준으로 'base_morpheme_second'의 value_counts\n",
    "\n",
    "data_1_Fall_2_share_yes = data_1[data_1['share_yes'] == 1]\n",
    "# print(data_1_Fall_2_share_yes.shape) # (423, 26) # richtig\n",
    "\n",
    "data_1_Fall_2_share_no = data_1[data_1['share_yes'] == 0]\n",
    "# print(data_1_Fall_2_share_no.shape) # (8, 26) # richtig\n",
    "\n",
    "# data_1 전체: 423 + 8 = 431 richtig\n",
    "\n",
    "# Fall 1와 Fall 2 (data_1 + data_2)에서 'share_yes'를 기준으로 'base_morpheme_second'의 value_counts (meeting 끝나고 산책하고 여기 다시 시작 050423)\n",
    "\n",
    "# 'base_morpheme_second'가 맞는지 확인해서 아닌 케이스 수정해야함\n",
    "# 같이 묶을 수 있는 케이스도 묶어야 함 \n",
    "\n",
    "\n",
    "# 전체 1. (data_2, Fall_1)\n",
    "Fall_1_connective = data_2_Fall_1['base_morpheme_second'].value_counts()\n",
    "# print(Fall_1_connective) # 전체 16 richtig\n",
    "\n",
    "Fall_1_share_yes_connective = Fall_1_share_yes['base_morpheme_second'].value_counts() \n",
    "# print(Fall_1_share_yes_connective)\n",
    "\n",
    "Fall_1_share_no_connective = Fall_1_share_no['base_morpheme_second'].value_counts() \n",
    "# print(Fall_1_share_no_connective)\n",
    "\n",
    "# Fall_1_connective.to_csv(\"Fall_1_connective_050423.csv\", encoding = \"utf-8-sig\") \n",
    "# Fall_1_share_yes_connective.to_csv(\"Fall_1_share_yes_connective_050423.csv\", encoding = \"utf-8-sig\") \n",
    "# Fall_1_share_no_connective.to_csv(\"Fall_1_share_no_connective_050423.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "# 전체 2. (data_2, Fall_2)\n",
    "data_2_Fall_2_connective = data_2_Fall_2['base_morpheme_second'].value_counts()\n",
    "# print(data_2_Fall_2_connective) # 전체 17 richtig\n",
    "\n",
    "data_2_Fall_2_share_yes_connective = data_2_Fall_2_share_yes['base_morpheme_second'].value_counts() \n",
    "# print(data_2_Fall_2_share_yes_connective)\n",
    "\n",
    "data_2_Fall_2_share_no_connective = data_2_Fall_2_share_no['base_morpheme_second'].value_counts() \n",
    "# print(data_2_Fall_2_share_no_connective)\n",
    "\n",
    "# data_2_Fall_2_connective.to_csv(\"data_2_Fall_2_connective.csv\", encoding = \"utf-8-sig\") \n",
    "# data_2_Fall_2_share_yes_connective.to_csv(\"data_2_Fall_2_share_yes_connective_050423.csv\", encoding = \"utf-8-sig\") \n",
    "# data_2_Fall_2_share_no_connective.to_csv(\"data_2_Fall_2_share_no_connective.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 전체 3. (data_1, Fall_2)\n",
    "data_1_Fall_2_connective = data_1['base_morpheme_second'].value_counts()\n",
    "# print(data_1_Fall_2_connective) \n",
    "\n",
    "data_1_Fall_2_share_yes_connective = data_1_Fall_2_share_yes['base_morpheme_second'].value_counts() \n",
    "# print(data_1_Fall_2_share_yes_connective)\n",
    "\n",
    "data_1_Fall_2_share_no_connective = data_1_Fall_2_share_no['base_morpheme_second'].value_counts() \n",
    "# print(data_1_Fall_2_share_no_connective)\n",
    "\n",
    "# data_1_Fall_2_connective.to_csv(\"data_1_Fall_2_connective.csv\", encoding = \"utf-8-sig\") \n",
    "# data_1_Fall_2_share_yes_connective.to_csv(\"data_1_Fall_2_share_yes_connective_050423.csv\", encoding = \"utf-8-sig\") \n",
    "# data_1_Fall_2_share_no_connective.to_csv(\"data_1_Fall_2_share_no_connective.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Fall_1_connective = pd.DataFrame(data_2_Fall_1_freq)\n",
    "# Fall_1_connective = data_2_Fall_1_freq.rename_axis('connective').reset_index(name='counts')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>etri_sent</th>\n",
       "      <th>sent</th>\n",
       "      <th>verb</th>\n",
       "      <th>applied_rule</th>\n",
       "      <th>zero_referents</th>\n",
       "      <th>antecedent</th>\n",
       "      <th>rule_order</th>\n",
       "      <th>connective</th>\n",
       "      <th>base_morpheme</th>\n",
       "      <th>base_morpheme_second</th>\n",
       "      <th>connective_true</th>\n",
       "      <th>zp_hono</th>\n",
       "      <th>zp_mood</th>\n",
       "      <th>sent_dep</th>\n",
       "      <th>zp_markable_ID</th>\n",
       "      <th>zero_type</th>\n",
       "      <th>tense_zp</th>\n",
       "      <th>verb_syn</th>\n",
       "      <th>verb_head</th>\n",
       "      <th>Filter</th>\n",
       "      <th>clau</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>136</td>\n",
       "      <td>네, 세탁실에 가져다놓으시면 내일아침까지 세탁해서 드릴거예요 .</td>\n",
       "      <td>세탁해서</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1ph+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>세탁하</td>\n",
       "      <td>아서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'clau', 'modi', 'clau', 'root...</td>\n",
       "      <td>markable_5026</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105</th>\n",
       "      <td>434</td>\n",
       "      <td>주차장입구의 오른쪽은 C 구역 이니 그쪽에 가서 찾아보세요 .</td>\n",
       "      <td>가서</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>가</td>\n",
       "      <td>서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'modi', 'scom', 'clau', 'modi...</td>\n",
       "      <td>markable_5278</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>8</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>452</td>\n",
       "      <td>차안에 열쇠를 두고 내렸는데 어떻게 하죠 ?</td>\n",
       "      <td>두고</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>두</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'clau', 'modi', 'root...</td>\n",
       "      <td>markable_5287</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>164</td>\n",
       "      <td>저희 축제의 전통의상을 입고 촬영할수있어요 .</td>\n",
       "      <td>입고</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>입</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'modi', 'dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>markable_5046</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>730</td>\n",
       "      <td>다시 먹어보고 맞지않으면 다시 부르겠습니다 .</td>\n",
       "      <td>먹어보고</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>먹어보</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'clau', 'clau', 'modi', 'root', 'punc']</td>\n",
       "      <td>markable_5458</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17103</th>\n",
       "      <td>8258</td>\n",
       "      <td>그럼 튀김을 넣어서 주세요 .</td>\n",
       "      <td>넣어서</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>넣</td>\n",
       "      <td>어서</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['modi', 'dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>markable_4972</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17145</th>\n",
       "      <td>8195</td>\n",
       "      <td>사진을 현상하러 오셨나요 ?</td>\n",
       "      <td>현상하러</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>현상하</td>\n",
       "      <td>러</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'root', 'punc']</td>\n",
       "      <td>markable_4928</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>3</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17157</th>\n",
       "      <td>8214</td>\n",
       "      <td>여길 다 둘러보고 다른 곳으로 가야겠네요 .</td>\n",
       "      <td>둘러보고</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>둘러보</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'modi', 'clau', 'modi', 'modi', 'root...</td>\n",
       "      <td>markable_4939</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>6</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17217</th>\n",
       "      <td>8836</td>\n",
       "      <td>에스컬레이터를 타고 2 층으로 가시면 오른쪽에 있습니다 .</td>\n",
       "      <td>타고</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>타</td>\n",
       "      <td>고</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'modi', 'modi', 'clau', 'modi...</td>\n",
       "      <td>markable_5291</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>5</td>\n",
       "      <td>C</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17263</th>\n",
       "      <td>8460</td>\n",
       "      <td>유료채널을 보려면 어떻게 해야합니까 ?</td>\n",
       "      <td>보려면</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1sh+</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>class_A</td>\n",
       "      <td>보</td>\n",
       "      <td>려면</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>['dobj', 'clau', 'modi', 'root', 'punc']</td>\n",
       "      <td>markable_5084</td>\n",
       "      <td>extra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>clau</td>\n",
       "      <td>4</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>594 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       etri_sent                                 sent  verb applied_rule  \\\n",
       "66           136  네, 세탁실에 가져다놓으시면 내일아침까지 세탁해서 드릴거예요 .  세탁해서          NaN   \n",
       "105          434   주차장입구의 오른쪽은 C 구역 이니 그쪽에 가서 찾아보세요 .    가서          NaN   \n",
       "144          452             차안에 열쇠를 두고 내렸는데 어떻게 하죠 ?    두고          NaN   \n",
       "145          164            저희 축제의 전통의상을 입고 촬영할수있어요 .    입고          NaN   \n",
       "179          730            다시 먹어보고 맞지않으면 다시 부르겠습니다 .  먹어보고          NaN   \n",
       "...          ...                                  ...   ...          ...   \n",
       "17103       8258                     그럼 튀김을 넣어서 주세요 .   넣어서          NaN   \n",
       "17145       8195                      사진을 현상하러 오셨나요 ?  현상하러          NaN   \n",
       "17157       8214             여길 다 둘러보고 다른 곳으로 가야겠네요 .  둘러보고          NaN   \n",
       "17217       8836     에스컬레이터를 타고 2 층으로 가시면 오른쪽에 있습니다 .    타고          NaN   \n",
       "17263       8460                유료채널을 보려면 어떻게 해야합니까 ?   보려면          NaN   \n",
       "\n",
       "      zero_referents antecedent  rule_order connective base_morpheme  \\\n",
       "66              1ph+        NaN         NaN    class_A           세탁하   \n",
       "105             2sh+        NaN         NaN    class_A             가   \n",
       "144             1sh+        NaN         NaN    class_A             두   \n",
       "145             2sh+        NaN         NaN    class_A             입   \n",
       "179             1sh+        NaN         NaN    class_A           먹어보   \n",
       "...              ...        ...         ...        ...           ...   \n",
       "17103           2sh+        NaN         NaN    class_A             넣   \n",
       "17145           2sh+        NaN         NaN    class_A           현상하   \n",
       "17157           1sh+        NaN         NaN    class_A           둘러보   \n",
       "17217           2sh+        NaN         NaN    class_A             타   \n",
       "17263           1sh+        NaN         NaN    class_A             보   \n",
       "\n",
       "      base_morpheme_second  connective_true zp_hono zp_mood  \\\n",
       "66                      아서                1     NaN     NaN   \n",
       "105                      서                1     NaN     NaN   \n",
       "144                      고                1     NaN     NaN   \n",
       "145                      고                1     NaN     NaN   \n",
       "179                      고                1     NaN     NaN   \n",
       "...                    ...              ...     ...     ...   \n",
       "17103                   어서                1     NaN     NaN   \n",
       "17145                    러                1     NaN     NaN   \n",
       "17157                    고                1     NaN     NaN   \n",
       "17217                    고                1     NaN     NaN   \n",
       "17263                   려면                1     NaN     NaN   \n",
       "\n",
       "                                                sent_dep zp_markable_ID  \\\n",
       "66     ['modi', 'modi', 'clau', 'modi', 'clau', 'root...  markable_5026   \n",
       "105    ['modi', 'dobj', 'modi', 'scom', 'clau', 'modi...  markable_5278   \n",
       "144    ['modi', 'dobj', 'clau', 'clau', 'modi', 'root...  markable_5287   \n",
       "145     ['modi', 'modi', 'dobj', 'clau', 'root', 'punc']  markable_5046   \n",
       "179     ['modi', 'clau', 'clau', 'modi', 'root', 'punc']  markable_5458   \n",
       "...                                                  ...            ...   \n",
       "17103           ['modi', 'dobj', 'clau', 'root', 'punc']  markable_4972   \n",
       "17145                   ['dobj', 'clau', 'root', 'punc']  markable_4928   \n",
       "17157  ['dobj', 'modi', 'clau', 'modi', 'modi', 'root...  markable_4939   \n",
       "17217  ['dobj', 'clau', 'modi', 'modi', 'clau', 'modi...  markable_5291   \n",
       "17263           ['dobj', 'clau', 'modi', 'root', 'punc']  markable_5084   \n",
       "\n",
       "      zero_type tense_zp verb_syn  verb_head Filter  clau  \n",
       "66        extra      NaN     clau          6      C     2  \n",
       "105       extra      NaN     clau          8      C     2  \n",
       "144       extra      NaN     clau          4      C     2  \n",
       "145       extra      NaN     clau          5      C     1  \n",
       "179       extra      NaN     clau          3      C     2  \n",
       "...         ...      ...      ...        ...    ...   ...  \n",
       "17103     extra      NaN     clau          4      C     1  \n",
       "17145     extra      NaN     clau          3      C     1  \n",
       "17157     extra      NaN     clau          6      C     1  \n",
       "17217     extra      NaN     clau          5      C     2  \n",
       "17263     extra      NaN     clau          4      C     1  \n",
       "\n",
       "[594 rows x 21 columns]"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 1. classA_clue_check에서 우선 해당 문장의 clau가 몇 개인지 살펴보고 clau가 하나인 경우 root의 주어 복사 가능할 것 같음 290323\n",
    "## 우선 이 case가 몇 개인지 확인해보기\n",
    "\n",
    "clau_freq = clau_count_add.groupby(['clau']).count()  \n",
    "\n",
    "# 전체 594 중\n",
    "\n",
    "# clau  etri_sent\n",
    "#  1: 365\n",
    "#  2: 173     \n",
    "#  3: 44\n",
    "#  4: 9\n",
    "#  5: 3\n",
    "\n",
    "clau_freq_new = clau_count_add['clau'].value_counts()  # hmm...\n",
    "\n",
    "# clau_freq_new\n",
    "\n",
    "clau_count_add\n",
    "\n",
    "\n",
    "# clau_freq\n",
    "\n",
    "# clau_freq.to_csv(\"clau_count_add_groupby_290323.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "557\n"
     ]
    }
   ],
   "source": [
    "## 2. clau가 1개로 count되는 365개의 case에 대해서 (594개의 61,45%)우선 resolutionsystem에서 이미 복원된 주어를 어떻게 복사할지 구현하기 290323\n",
    "## 우선 etir_sent가 중복되지 않는다면 일이 더 쉬워지므로 etri_sent 하나만 있는 case와 clau가 1인 case의 intersection을 살펴보기! 290323\n",
    "\n",
    "clau_count_add_counter = {}  \n",
    "for value in classA_etri_sent_val_list:\n",
    "    try: clau_count_add_counter[value] += 1\n",
    "    except: clau_count_add_counter[value] = 1\n",
    "        \n",
    "# print(len(clau_count_add_counter)) # 575! 594개 중 중복된 항목 제거하면 575 richtig\n",
    "\n",
    "classA_etri_sent_over_two = [] # 18 (etri_sent가 중복되는 classA)\n",
    "classA_etri_sent_one = [] # 557 (etri_sent가 하나만 있는 classA)\n",
    "\n",
    "for key, value in clau_count_add_counter.items():\n",
    "    if value >= 2:\n",
    "        classA_etri_sent_over_two.append(key)\n",
    "    if value == 1:\n",
    "        classA_etri_sent_one.append(key)\n",
    "        \n",
    "# print(len(classA_etri_sent_over_two)) # 18\n",
    "# print(len(classA_etri_sent_one)) # 557\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(594, 22)"
      ]
     },
     "execution_count": 233,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. clau가 1인 etri_sent # (365, 22)\n",
    "\n",
    "clau_count_one = clau_count_add[clau_count_add.clau == 1] \n",
    "# print(clau_count_one.shape) # (365, 22) richtig\n",
    "\n",
    "clau_one_etri_sent = clau_count_one.etri_sent\n",
    "clau_one_etri_sent_val = clau_one_etri_sent.values \n",
    "clau_one_etri_sent_val_list = clau_one_etri_sent_val.tolist() \n",
    "\n",
    "# print(len(clau_one_etri_sent_val_list)) # 365\n",
    "\n",
    "intersection_etri_sent_clau_one = list(set(classA_etri_sent_one) & set(clau_one_etri_sent_val_list)) \n",
    "\n",
    "# print(len(intersection_etri_sent_clau_one)) # 365\n",
    "\n",
    "simple_case = clau_count_add.etri_sent.isin(intersection_etri_sent_clau_one)\n",
    "clau_count_add = clau_count_add.assign(simple = simple_case) # (683, 17)\n",
    "\n",
    "# print(clau_count_add.shape) # (594, 22)\n",
    "\n",
    "# 2. clau가 2 이상인 etri_sent # (229, 22)\n",
    "\n",
    "clau_count_not_one = clau_count_add[clau_count_add.clau != 1] \n",
    "\n",
    "# print(clau_count_not_one.shape) # (229, 22) 594 - 365 = 229 richtig\n",
    "\n",
    "\n",
    "# 실제로 규칙이 제안하는 referent가 맞지만 extra가 아니라서 직접적으로 정답이 비교 안되는 case (case3)\n",
    "# df3_not_case1_not_singplural.xlsx에서 referent_correct sheet \n",
    "\n",
    "referent_correct = pd.read_excel('case2A_B_C_11052022.xlsx', sheet_name = 'referent_correct') \n",
    "referent_correct_idx = referent_correct['idx']\n",
    "referent_correct_idx_val = referent_correct_idx.values \n",
    "referent_correct_idx_val_list = referent_correct_idx_val.tolist() # 124 --> case3 list 요소의 type float (list 값이 nan으로 나옴)\n",
    "\n",
    "# print(len(referent_correct_idx_val_list)) # 1241인칭 right prediction\n",
    "\n",
    "df2_exclude_case3 = df2_exclude_orig[df2_exclude_orig.index.isin(referent_correct_idx_val_list) == True]\n",
    "# print(df2_exclude_case3.shape) # (120, 23)\n",
    "\n",
    "# clau_count_add.to_csv(\"clau_count_add_2_290323.csv\", encoding = \"utf-8-sig\") # 290323\n",
    "# simple 열 추가됨: clau 1이면 True, clau 2 이상이면 False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. intersection_etri_sent_clau_one를 지닌 case를 df2_FilterA_B에서 찾아서 subject_by_rule 정보 가져오기!!\n",
    "\n",
    "df2_FilterA_B_intersection_etri_sent = df2_FilterA_B[df2_FilterA_B.etri_sent.isin(intersection_etri_sent_clau_one) == True] \n",
    "# print(df2_FilterA_B_intersection_etri_sent.shape) # (367, 23)\n",
    "\n",
    "df2_FilterA_B_intersection_subject_by_rule = df2_FilterA_B_intersection_etri_sent.subject_by_rule\n",
    "# df2_FilterA_B_intersection_subject_by_rule은 subject_by_rule이 index와 함께 있는 자료 e.g 31 2sh+ (이 경우 etri_sent는 255임)\n",
    "\n",
    "clau_count_one_intersection_etri_sent = clau_count_one[clau_count_one.etri_sent.isin(intersection_etri_sent_clau_one) == True] \n",
    "# print(clau_count_one_intersection_etri_sent.shape) # (365, 22)\n",
    "\n",
    "# df2_FilterA_B_intersection_etri_sent.to_csv(\"df2_FilterA_B_intersection_etri_sent_290323.csv\", encoding = \"utf-8-sig\") \n",
    "# clau_count_one_intersection_etri_sent.to_csv(\"clau_count_one__intersection_etri_sent_290323.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "## 이 두 개가 차이가 나는 etri_sent 찾기 \n",
    "\n",
    "df2_FilterA_B_intersection_etri_sent = df2_FilterA_B_intersection_etri_sent.etri_sent\n",
    "df2_FilterA_B_intersection_etri_sent_val = df2_FilterA_B_intersection_etri_sent.values \n",
    "df2_FilterA_B_intersection_etri_sent_val_list = df2_FilterA_B_intersection_etri_sent_val.tolist()\n",
    "\n",
    "# print(len(df2_FilterA_B_intersection_etri_sent_val_list)) # 367\n",
    "\n",
    "clau_count_one_intersection_etri_sent = clau_count_one_intersection_etri_sent.etri_sent\n",
    "clau_count_one_intersection_etri_sent_val = clau_count_one_intersection_etri_sent.values \n",
    "clau_count_one_intersection_etri_sent_val_list = clau_count_one_intersection_etri_sent_val.tolist()\n",
    "\n",
    "# print(len(clau_count_one_intersection_etri_sent_val_list)) # 365\n",
    "\n",
    "# df2_FilterA_B_intersection_etri_sent_val_list 이 리스트에서 중복되는 값이 있는지 찾기)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "counter = Counter(df2_FilterA_B_intersection_etri_sent_val_list)\n",
    "\n",
    "counter_dict = dict(counter)\n",
    "\n",
    "# for key, value in counter_dict.items():\n",
    "#     if value == 2:\n",
    "#         print(key)\n",
    "\n",
    "# 1. \"df2_FilterA_B_intersection_etri_sent_290323.csv\"에서는\n",
    "\n",
    "# 13661 보통 1인분 *주문하시고* 밥을 주문해서 *드시거든요*.\n",
    "# 27222 일단 방송으로 보내고 경비원들 *풀어서* *찾아보겠습니다*.\n",
    "\n",
    "# 2. \"clau_count_one__intersection_etri_sent_290323.csv\"에서는\n",
    "\n",
    "# 13661 보통 1인분 주문하시고 밥을 *주문해서* 드시거든요. (index: 3564)\n",
    "# 27222 일단 방송으로 *보내고* 경비원들 풀어서 찾아보겠습니다. (index: 12272)\n",
    "\n",
    "# 이 두 개를 제외해야 하는 거 아닌가? clau가 1이 아님\n",
    "\n",
    "# print(len(set(df2_FilterA_B_intersection_etri_sent_val_list))) # 365\n",
    "# print(len(set(clau_count_one_intersection_etri_sent_val_list))) # 365\n",
    "\n",
    "etri_sent_duplicate_set = {13661, 27222}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{164: '촬영할수있어요',\n",
       " 365: '드릴게요',\n",
       " 737: '환승하셔야합니다',\n",
       " 744: '지워주세요',\n",
       " 975: '앉으면되나요',\n",
       " 252: '열어드리겠습니다',\n",
       " 1056: '올라가봐야겠어요',\n",
       " 868: '오면됩니까',\n",
       " 476: '처리해드릴게요',\n",
       " 352: '연락드리겠습니다',\n",
       " 1030: '구경할수있습니다',\n",
       " 640: '타야하나요',\n",
       " 97: '가야하나요',\n",
       " 9: '가겠습니다',\n",
       " 207: '갈게요',\n",
       " 461: '연락해드릴게요',\n",
       " 334: '계산해도되나요',\n",
       " 10153: '먹고싶어요',\n",
       " 9757: '기다리시겠어요',\n",
       " 9975: '기다려주세요',\n",
       " 9782: '기다리세요',\n",
       " 10174: '가봐야겠네요',\n",
       " 9799: '말씀해드릴게요',\n",
       " 9917: '계산해주세요',\n",
       " 10130: '이용해야겠네요',\n",
       " 10223: '가져다드리겠습니다',\n",
       " 10393: '신기해요',\n",
       " 10237: '기다리세요',\n",
       " 10395: '생각해볼게요',\n",
       " 9743: '기다리시겠어요',\n",
       " 9395: '검사를해주어야합니다',\n",
       " 9494: '가야하나요',\n",
       " 9439: '타야하나요',\n",
       " 9445: '가야하나요',\n",
       " 9467: '가야하나요',\n",
       " 10056: '해야하죠',\n",
       " 10058: '구입해야겠네요',\n",
       " 10071: '기다려주세요',\n",
       " 10893: '담으시면됩니다',\n",
       " 11079: '수리해드리도록하겠습니다',\n",
       " 11354: '올라가보도록하겠습니다',\n",
       " 11003: '들어갈수있겠어요',\n",
       " 10867: '검색할수있습니다',\n",
       " 11469: '가려고해요',\n",
       " 11128: '가야해요',\n",
       " 10446: '제출해주세요',\n",
       " 11292: '주세요',\n",
       " 11460: '결정할게요',\n",
       " 10708: '시간되세요',\n",
       " 10502: '주나요',\n",
       " 10832: '가야하나요',\n",
       " 11657: '모르겠습니다',\n",
       " 11658: '여쭤보세요',\n",
       " 11931: '이동하실수있습니다',\n",
       " 11927: '이동하십시오',\n",
       " 11943: '들어있어야되나요',\n",
       " 12394: '갈게요',\n",
       " 11777: '가야하나요',\n",
       " 11805: '들어오시면안됩니다',\n",
       " 11808: '들어가도될까요',\n",
       " 11924: '이동해야하는지요',\n",
       " 12442: '선택해주시면됩니다',\n",
       " 11597: '해야하죠',\n",
       " 11629: '설명해드릴게요',\n",
       " 11552: '구입하시겠어요',\n",
       " 11878: '결제하셔야해요',\n",
       " 12098: '배정해주셨으면합니다',\n",
       " 11885: '당기시면됩니다',\n",
       " 11743: '찾아보겠습니다',\n",
       " 13058: '오겠습니다',\n",
       " 13552: '누르면됩니다',\n",
       " 13557: '걸어두었어요',\n",
       " 13373: '드릴게요',\n",
       " 13376: '갈게요',\n",
       " 13380: '내리셔야합니다',\n",
       " 12790: '먹을뻔했거든요',\n",
       " 13001: '타세요',\n",
       " 13490: '찍을게요',\n",
       " 13510: '타려고하는데요',\n",
       " 13331: '타실건가요',\n",
       " 13139: '알아보세요',\n",
       " 12738: '결정하세요',\n",
       " 12737: '결정해야겠네요',\n",
       " 12665: '앉으시면됩니다',\n",
       " 12661: '가려고요',\n",
       " 12704: '여행하려고하거든요',\n",
       " 12638: '올게요',\n",
       " 12659: '오게되었어요',\n",
       " 13293: '타세요',\n",
       " 12905: '가보고싶어서요',\n",
       " 14038: '맛있었습니다',\n",
       " 14230: '타세요',\n",
       " 14064: '갈수있습니까',\n",
       " 13675: '먹을게요',\n",
       " 14380: '갈수가없네요',\n",
       " 14329: '사용하실수있습니다',\n",
       " 14516: '신고를하셔야합니다',\n",
       " 14337: '꽂으시면됩니다',\n",
       " 14154: '하려고해요',\n",
       " 14461: '할게요',\n",
       " 13771: '당기시면됩니다',\n",
       " 14091: '서있으면되는겁니까',\n",
       " 13889: '보세요',\n",
       " 14912: '다녔거든요',\n",
       " 15184: '끓이겠습니다',\n",
       " 15071: '가야하나요',\n",
       " 15457: '가져가주세요',\n",
       " 15092: '나가야하지요',\n",
       " 15475: '두세요',\n",
       " 15314: '신어보십시오',\n",
       " 15496: '오겠습니다',\n",
       " 15575: '넣으세요',\n",
       " 15046: '오겠습니다',\n",
       " 15605: '오겠습니다',\n",
       " 15704: '올라가세요',\n",
       " 15651: '출발합니다',\n",
       " 15217: '가겠습니다',\n",
       " 14812: '가면됩니다',\n",
       " 16740: '출발합니다',\n",
       " 16562: '확인해보도록하겠습니다',\n",
       " 16862: '와주시면됩니다',\n",
       " 16886: '오십시오',\n",
       " 16885: '가면됩니까',\n",
       " 16509: '가시면됩니다',\n",
       " 16708: '가야합니까',\n",
       " 16815: '결정하겠습니다',\n",
       " 16816: '와주십시오',\n",
       " 16824: '해야합니까',\n",
       " 16456: '넣어주십시오',\n",
       " 16471: '타야합니까',\n",
       " 15856: '올게요',\n",
       " 16077: '해야하죠',\n",
       " 16085: '잠가버렸어요',\n",
       " 16097: '넘어졌어요',\n",
       " 15939: '들어갈수없습니다',\n",
       " 16450: '탑승하셔야합니다',\n",
       " 17385: '쓰고있습니다',\n",
       " 17552: '하고싶은데요',\n",
       " 17802: '들지않았는데요',\n",
       " 17739: '착용하셔야돼요',\n",
       " 17199: '먹을수있겠군요',\n",
       " 17015: '살수있죠',\n",
       " 17021: '타야하죠',\n",
       " 17108: '드릴게요',\n",
       " 16977: '주문해야하나요',\n",
       " 16989: '갈게요',\n",
       " 17304: '먹을수도있나요',\n",
       " 17069: '내려야하나요',\n",
       " 18556: '가야할거같은데요',\n",
       " 18679: '주면되나요',\n",
       " 18308: '타면됩니다',\n",
       " 18314: '기다려야해요',\n",
       " 18268: '맡겨주세요',\n",
       " 18565: '기억하지못하겠습니다',\n",
       " 18568: '가보시겠습니까',\n",
       " 18445: '가면되나요',\n",
       " 19053: '갑시다',\n",
       " 18879: '갈까요',\n",
       " 18887: '먹어볼까요',\n",
       " 19109: '가죠',\n",
       " 18909: '보시겠어요',\n",
       " 19125: '가야하나요',\n",
       " 19172: '내린거같아요',\n",
       " 19374: '상냥하지않은것같아요',\n",
       " 19539: '가겠습니다',\n",
       " 19043: '가죠',\n",
       " 19042: '힘드네요',\n",
       " 19044: '갈까요',\n",
       " 19483: '오겠습니다',\n",
       " 19501: '보여주면되나요',\n",
       " 18785: '가져가도되나요',\n",
       " 19146: '가져오겠습니다',\n",
       " 19145: '가져다주세요',\n",
       " 20250: '오겠습니다',\n",
       " 20376: '넘어갈예정입니다',\n",
       " 21013: '드시면됩니다',\n",
       " 20947: '가야하나요',\n",
       " 20735: '가겠습니다',\n",
       " 20525: '오겠습니다',\n",
       " 20957: '가야하나요',\n",
       " 20974: '남았나요',\n",
       " 20914: '해야합니까',\n",
       " 20089: '타시면돼요',\n",
       " 21840: '기다려주시겠어요',\n",
       " 21670: '주시면됩니다',\n",
       " 21324: '가져다드리겠습니다',\n",
       " 21154: '드리겠습니다',\n",
       " 21528: '가야겠네요',\n",
       " 21692: '확인해보겠습니다',\n",
       " 21168: '기다려주십시오',\n",
       " 21793: '오신건가요',\n",
       " 21448: '돌아가면됩니다',\n",
       " 21636: '내리셔야할겁니다',\n",
       " 21317: '불러드리겠습니다',\n",
       " 22126: '준비하도록하겠습니다',\n",
       " 21560: '들어가시면됩니다',\n",
       " 21726: '할수있습니다',\n",
       " 21732: '요리한거같습니다',\n",
       " 21759: '다시해드리겠습니다',\n",
       " 21863: '쉴수있겠네요',\n",
       " 21869: '오셨나요',\n",
       " 21036: '타시면됩니다',\n",
       " 21068: '받아야하나요',\n",
       " 21261: '가면됩니다',\n",
       " 21259: '가면됩니다',\n",
       " 21258: '가야합니까',\n",
       " 21557: '연습해보면안되나요',\n",
       " 21208: '주문하시면됩니다',\n",
       " 22575: '타시면될겁니다',\n",
       " 22178: '처방해드리겠습니다',\n",
       " 22212: '들어가시면됩니다',\n",
       " 22308: '타세요',\n",
       " 22709: '확인해보겠습니다',\n",
       " 22829: '먹어야하나요',\n",
       " 22661: '받으셔야합니다',\n",
       " 22953: '가져가세요',\n",
       " 22414: '태우면되지않나요',\n",
       " 22246: '먹을수있나요',\n",
       " 22441: '가세요',\n",
       " 23690: '찾아가시면됩니다',\n",
       " 24199: '주시겠어요',\n",
       " 24306: '올게요',\n",
       " 24092: '내주세요',\n",
       " 23655: '빌려와야겠네요',\n",
       " 23318: '이용하실수있습니다',\n",
       " 23633: '주십시오',\n",
       " 23647: '드실건가요',\n",
       " 23572: '가도될까요',\n",
       " 23391: '내야하나요',\n",
       " 23749: '내렸어요',\n",
       " 25113: '내야하나요',\n",
       " 25363: '타면되나요',\n",
       " 25057: '전화드렸습니다',\n",
       " 24741: '지나치신것같네요',\n",
       " 24455: '돕니다',\n",
       " 26547: '들어가셔야해요',\n",
       " 26548: '타야하나요',\n",
       " 25837: '가야하나요',\n",
       " 25848: '가야겠네요',\n",
       " 26054: '가져다드리겠습니다',\n",
       " 26299: '대기하겠습니다',\n",
       " 26352: '가시면됩니다',\n",
       " 26537: '들어가면되고요',\n",
       " 26529: '받을게요',\n",
       " 26448: '들어갈수있나요',\n",
       " 26272: '오십시오',\n",
       " 26451: '맡길게요',\n",
       " 26478: '발라보세요',\n",
       " 26557: '이용하셔야해요',\n",
       " 25771: '연락드리도록하겠습니다',\n",
       " 25722: '나오신겁니까',\n",
       " 25751: '내야합니까',\n",
       " 26254: '쉬십시오',\n",
       " 25926: '나오겠습니다',\n",
       " 26788: '탈수있나요',\n",
       " 26796: '찾아가면되는거죠',\n",
       " 27193: '사려구요',\n",
       " 26623: '가려고요',\n",
       " 27215: '없는거예요',\n",
       " 26630: '가야지요',\n",
       " 26640: '갈아타야하나요',\n",
       " 27732: '하시겠어요',\n",
       " 27537: '가야하나요',\n",
       " 26953: '통과해주세요',\n",
       " 26959: '타실수없습니다',\n",
       " 27168: '앉으세요',\n",
       " 27074: '보여주세요',\n",
       " 27799: '갈수있습니다',\n",
       " 27584: '가면되나요',\n",
       " 27628: '드세요',\n",
       " 26682: '청하세요',\n",
       " 28308: '고르시면됩니다',\n",
       " 28138: '타실수있습니다',\n",
       " 28154: '보내드리겠습니다',\n",
       " 28156: '전화드리겠습니다',\n",
       " 28698: '사도록할게요',\n",
       " 28526: '가리겠습니까',\n",
       " 28611: '앉아계세요',\n",
       " 28273: '드릴게요',\n",
       " 28650: '없어요',\n",
       " 28445: '가십시오',\n",
       " 28856: '가야겠네요',\n",
       " 28961: '걸어야겠네요',\n",
       " 28596: '반납하셔야해요',\n",
       " 28053: '가져다주셔야해요',\n",
       " 27865: '찾아보세요',\n",
       " 28027: '보내겠습니다',\n",
       " 28259: '알려주도록해요',\n",
       " 28546: '갖다주세요',\n",
       " 28245: '해야하는거죠',\n",
       " 29565: '물어보세요',\n",
       " 29586: '버틸수있을것같습니다',\n",
       " 29440: '가야합니까',\n",
       " 29722: '가야하나요',\n",
       " 29561: '내린것같습니다',\n",
       " 29019: '해야하나요',\n",
       " 29207: '다칠뻔했어요',\n",
       " 29507: '가야합니까',\n",
       " 3527: '안내해드리겠습니다',\n",
       " 3919: '해야하는거죠',\n",
       " 3373: '사야겠습니다',\n",
       " 3553: '해야하나요',\n",
       " 3406: '가세요',\n",
       " 3566: '내린것같아요',\n",
       " 3895: '교환해드리겠습니다',\n",
       " 3288: '복사를해두지않았습니다',\n",
       " 3142: '이동하려고요',\n",
       " 3337: '오셨나요',\n",
       " 3344: '오겠습니다',\n",
       " 3278: '가세요',\n",
       " 4930: '타세요',\n",
       " 4573: '찾았다고하네요',\n",
       " 4516: '주세요',\n",
       " 4040: '기다리고있습니다',\n",
       " 4043: '듣지못했습니다',\n",
       " 4053: '들으면되는걸요',\n",
       " 4149: '오셨습니까',\n",
       " 4157: '하세요',\n",
       " 4165: '오셨습니까',\n",
       " 4256: '불면됩니다',\n",
       " 4503: '보여주세요',\n",
       " 4515: '고르세요',\n",
       " 4363: '오셨습니까',\n",
       " 4357: '물어보세요',\n",
       " 4205: '취할수가없습니다',\n",
       " 4064: '기다려야되네요',\n",
       " 4345: '뽑아야겠네요',\n",
       " 4353: '가면되는군요',\n",
       " 5577: '찍죠',\n",
       " 5283: '사오겠습니다',\n",
       " 5470: '정지시켜야겠어요',\n",
       " 5544: '내리고싶은데요',\n",
       " 5416: '관람해주세요',\n",
       " 6050: '제작해주실수있나요',\n",
       " 5523: '오면될까요',\n",
       " 5998: '오셨나요',\n",
       " 5068: '올리려고합니다',\n",
       " 5067: '간직하시려는건가요',\n",
       " 5072: '소개할게요',\n",
       " 5146: '찾으면됩니다',\n",
       " 5110: '와야겠군요',\n",
       " 5504: '가십시오',\n",
       " 6739: '넣어주세요',\n",
       " 6743: '탑승하실수없습니다',\n",
       " 6752: '더내야하나요',\n",
       " 6575: '가겠습니다',\n",
       " 6935: '주문하겠습니다',\n",
       " 6578: '갖다드리겠습니다',\n",
       " 6606: '내려드릴게요',\n",
       " 6859: '도와드리겠습니다',\n",
       " 6730: '요리해드릴까요',\n",
       " 6562: '돌려야하나요',\n",
       " 6228: '도와드리겠습니다',\n",
       " 6260: '모아볼게요',\n",
       " 6801: '기다려주세요',\n",
       " 8343: '움직여야합니까',\n",
       " 8381: '잘압니다',\n",
       " 9147: '기다려주세요',\n",
       " 8261: '갈게요',\n",
       " 8258: '주세요',\n",
       " 8195: '오셨나요',\n",
       " 8214: '가야겠네요',\n",
       " 8460: '해야합니까'}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# etri_sent_duplicate을 제외하고는 진짜 simple하게 복사할 수 있는 듯 (dict 사용도 가능 아니면 직접적으로 df 합치기도 가능할 듯) 300323\n",
    "\n",
    "etri_sent_remove_list = [i for i in df2_FilterA_B_intersection_etri_sent_val_list if i not in etri_sent_duplicate_set] # str이 아니라 integer인듯 300323\n",
    "# print(len(etri_sent_remove_list)) # 363 rightig (367 - 4) 중복되는 두 개의 etri_sent에 속하는 문장은 총 4개 \n",
    "\n",
    "# 양쪽에서 우선 etri_sent_duplicate만 제외하고 subject_by_rule dict로 가지고 있기! (e.g. {etri_sent: subject_by_rule})\n",
    "\n",
    "# df2_FilterA_B에서 etri_sent_remove_list를 지닌 subject_by_rule을 dict로 만들어보기\n",
    "\n",
    "df2_FilterA_B_etri_sent = df2_FilterA_B[df2_FilterA_B.etri_sent.isin(etri_sent_remove_list)]\n",
    "# print(df2_FilterA_B_etri_sent.shape) # (363, 23) richtig\n",
    "\n",
    "df2_FilterA_B_subject_by_rule = df2_FilterA_B_etri_sent.subject_by_rule\n",
    "df2_FilterA_B_subject_by_rule_val = df2_FilterA_B_subject_by_rule.values \n",
    "df2_FilterA_B_subject_by_rule_val_list = df2_FilterA_B_subject_by_rule_val.tolist() \n",
    "# print(len(df2_FilterA_B_subject_by_rule_val_list)) # 363 richtig\n",
    "\n",
    "df2_FilterA_B_verb = df2_FilterA_B_etri_sent.verb\n",
    "df2_FilterA_B_verb_val = df2_FilterA_B_verb.values \n",
    "df2_FilterA_B_verb_val_list = df2_FilterA_B_verb_val.tolist() \n",
    "# print(len(df2_FilterA_B_verb_val_list)) # 363 richtig\n",
    "\n",
    "subject_by_rule_dict = dict(zip(etri_sent_remove_list, df2_FilterA_B_subject_by_rule_val_list)) \n",
    "# print(len(subject_by_rule_dict)) # 363 richtig (원래의 코드와 달리 dictinary update하지 않음) 300323\n",
    "\n",
    "verb_dict = dict(zip(etri_sent_remove_list, df2_FilterA_B_verb_val_list)) \n",
    "# print(len(verb_dict)) # 363 richtig (원래의 코드와 달리 dictinary update하지 않음) 300323\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이제 clau_count_one (365, 22)에 copied subject_by_rule 추가해주기!\n",
    "# 여기에서도 우선 총 2개의 문장 제외 돼야 하는 거 아닌가? etri_sent가 13661, 27222인 케이스들\n",
    "\n",
    "# clau_count_one 자체에서도 2문장 제외시켜야 함 \n",
    "\n",
    "etri_sent_duplicate_list = [13661, 27222]\n",
    "\n",
    "clau_count_one_new = clau_count_one[clau_count_one.etri_sent.isin(etri_sent_duplicate_list) == False]\n",
    "#print(clau_count_one.shape) # (365, 22) richtig\n",
    "#print(clau_count_one_new.shape) # (363, 22) richtig\n",
    "\n",
    "etri_sent_remove_list2 = [i for i in clau_count_one_intersection_etri_sent_val_list if i not in etri_sent_duplicate_set] \n",
    "# print(len(etri_sent_remove_list2)) # 363 richtig (365 - 2)\n",
    "\n",
    "clau_one_etri_sent = clau_count_one_new[clau_count_one_new.etri_sent.isin(etri_sent_remove_list2)]\n",
    "# print(clau_one_etri_sent.shape) # (363, 22) richtig\n",
    "\n",
    "subject_by_rule_keys = list(subject_by_rule_dict.keys())\n",
    "\n",
    "copied_subject_by_rule = [] \n",
    "\n",
    "for sent in clau_count_one_new['etri_sent']:\n",
    "    if sent in subject_by_rule_keys:\n",
    "        copied_subject_by_rule.append(subject_by_rule_dict[sent])\n",
    "    \n",
    "# print(len(copied_subject_by_rule)) # 363\n",
    "\n",
    "copied_verb = []\n",
    "\n",
    "for sent in clau_count_one_new['etri_sent']:\n",
    "    if sent in subject_by_rule_keys:\n",
    "        copied_verb.append(verb_dict[sent])\n",
    "\n",
    "# print(len(copied_verb)) # 363\n",
    "\n",
    "# 열이 늘어나야 함! 22 - 23 - 24             \n",
    "clau_count_one_add_subject_by_rule = clau_count_one_new.assign(copied_subject_by_rule = copied_subject_by_rule) \n",
    "clau_count_one_add_subject_and_verb = clau_count_one_add_subject_by_rule.assign(copied_verb = copied_verb) # (432, 19)\n",
    "\n",
    "# print(clau_count_one_add_subject_by_rule.shape) # (363, 23)\n",
    "# print(clau_count_one_add_subject_and_verb.shape) # (363, 24) richtig\n",
    "\n",
    "\n",
    "# clau_count_one_add_subject_and_verb.to_csv(\"clau_count_one_add_subject_and_verb_300323.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 나중에 copied_subject_by_rule과 zero_referents 비교할 때 string으로 되어 있는 antecedent도 있고, sing/plural도 고려해야 함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error case 1: 규칙 groupA가 잘 적용되지 않아서 copied subject_by_rule가 틀린 경우 (규칙 group A를 더 최적화 해야함 - 완료)\n",
    "# error case 2: 규칙 groupA가 제시하는 copied subject_by_rule이  틀린 경우 \n",
    "# error case 3: class A에 속하지만 subject sharing property가 적용될 수 없는 connective (대부분 너무 용법이 많아서)\n",
    "# error case 4: sing <-> plural이 애매한 경우 (사실은 정답이 될 수 있음)\n",
    " ## e.g.\n",
    " # 6977\t18887\n",
    " # 7230\t19044\n",
    " # 16498\t6124\n",
    " # 12976\t28027\n",
    " # 16251\t6393\n",
    " ## e.g.15471\t5720\t어제밤에 뉴왁 공항에 내렸고 오늘은 나이아가라폭포를 봤습니다 .\t내렸고\t봤습니다\t1sh+\t\t{우리는}\n",
    "# error case 5: 직접적으로 연결된 절 아님 \n",
    " ## e.g. 3923\t14684\t넘어져서 무릎을 다쳤는데요, 약을 좀 주십시오 .\t넘어져서\t주십시오\n",
    " ## e.g. 7134\t19390\t매콤하고 새콤합니다 . 한국사람 입에도 맞을거같아요 .\t매콤하고\t맞을거같아요\n",
    "# error case 6: zero_referents 외의 경험자가 주어가 될 수 있는 경우\n",
    " ## e.g. 10639\t24940\t생각보다 덥지도않고 즐거웠어요 .\t덥지도않고\t즐거웠어요\t1sh+\t날씨\n",
    " ## e.g. 7862\t20132\t죄송한데요, 좀 작은거같아서 불편하네요 .\t작은거같아서\t불편하네요\t1sh+\t사이즈\n",
    "# error case 7: antecedent 외의 경험자가 주어가 될 수 있는 경우\n",
    " ## e.g. 1632\t11029\t편하고 좋습니다 .\t편하고\t좋습니다\t1sh+\t\t{치마가}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 1: group value_counts (ranking 1-3)\n",
    "want_statement_Fall_one = df2_case1[df2_case1[\"group\"] == \"want statement\"]\n",
    "want_statement_Fall_one_value_count = want_statement_Fall_one[\"rule\"].value_counts() \n",
    "# want_statement_Fall_one_value_count.to_csv(\"want_statement_Fall_one_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "mood_derivable_Fall_one = df2_case1[df2_case1[\"group\"] == \"mood derivable\"]\n",
    "mood_derivable_Fall_one_value_count = mood_derivable_Fall_one[\"rule\"].value_counts() \n",
    "# mood_derivable_Fall_one_value_count.to_csv(\"mood_derivable_Fall_one_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "clau_Fall_one = df2_case1[df2_case1[\"group\"] == \"clau\"]\n",
    "clau_Fall_one_value_count = clau_Fall_one[\"rule\"].value_counts() \n",
    "# clau_Fall_one_value_count.to_csv(\"clau_Fall_one_value_count_260922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 2: Person match (ranking 1-3)\n",
    "# Fall 2.1 first person match\n",
    "politeness_Fall_two = df2_first_person_match[df2_first_person_match[\"group\"] == \"politeness\"]\n",
    "politeness_Fall_two_value_count = politeness_Fall_two[\"rule\"].value_counts() \n",
    "# politeness_Fall_two_value_count.to_csv(\"politeness_Fall_two_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "want_statement_Fall_two = df2_first_person_match[df2_first_person_match[\"group\"] == \"want statement\"]\n",
    "want_statement_Fall_two_value_count = want_statement_Fall_two[\"rule\"].value_counts() \n",
    "# want_statement_Fall_two_value_count.to_csv(\"want_statement_Fall_two_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "clau_Fall_two = df2_first_person_match[df2_first_person_match[\"group\"] == \"clau\"]\n",
    "clau_Fall_two_value_count = clau_Fall_two[\"rule\"].value_counts() \n",
    "# clau_Fall_two_value_count.to_csv(\"clau_Fall_two_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# # Fall 2.2 second person match\n",
    "mood_derivable_Fall_two_two = df2_second_person_match[df2_second_person_match[\"group\"] == \"mood derivable\"]\n",
    "mood_derivable_Fall_two_two_value_count = mood_derivable_Fall_two_two[\"rule\"].value_counts() \n",
    "# mood_derivable_Fall_two_two_value_count.to_csv(\"mood_derivable_Fall_two_two_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "clau_Fall_two_two = df2_second_person_match[df2_second_person_match[\"group\"] == \"clau\"]\n",
    "clau_Fall_two_two_value_count = clau_Fall_two_two[\"rule\"].value_counts() \n",
    "# clau_Fall_two_two_value_count.to_csv(\"clau_Fall_two_two_value_count.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "possibility_Fall_two_two = df2_second_person_match[df2_second_person_match[\"group\"] == \"possibility\"]\n",
    "possibility_Fall_two_two_value_count = possibility_Fall_two_two[\"rule\"].value_counts() \n",
    "# possibility_Fall_two_two_value_count.to_csv(\"possibility_Fall_two_two_value_count_260922.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fall 3 antecedent match\n",
    "want_statement_Fall_three = df2_case3_antecedent_match[df2_case3_antecedent_match[\"group\"] == \"want statement\"]\n",
    "want_statement_Fall_three_value_count = want_statement_Fall_three[\"rule\"].value_counts() \n",
    "# want_statement_Fall_three_value_count.to_csv(\"want_statement_Fall_three_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "experience_Fall_three = df2_case3_antecedent_match[df2_case3_antecedent_match[\"group\"] == \"experience\"]\n",
    "experience_Fall_three_value_count = experience_Fall_three[\"rule\"].value_counts() \n",
    "# experience_Fall_three_value_count.to_csv(\"experience_Fall_three_value_count_260922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "clau_Fall_three = df2_case3_antecedent_match[df2_case3_antecedent_match[\"group\"] == \"clau\"]\n",
    "clau_Fall_three_value_count = clau_Fall_three[\"rule\"].value_counts() \n",
    "# clau_Fall_three_value_count.to_csv(\"clau_Fall_three_value_count_260922.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3718, 21)\n",
      "3718\n",
      "15508\n",
      "(375, 21)\n"
     ]
    }
   ],
   "source": [
    "# 그냥 df2_exclude에서 제거해보기\n",
    "\n",
    "df2_NA_none = df2_exclude[(df2_exclude.subject_by_rule == \"N/A\") | (df2_exclude.subject_by_rule == \"none\") ]\n",
    "print(df2_NA_none.shape) #(3718, 21)\n",
    "\n",
    "df2_NA_none_idx = df2_NA_none.index.tolist()\n",
    "print(len(df2_NA_none_idx)) # 3718\n",
    "\n",
    "df2_not_error_idx_all = df2_NA_none_idx + referent_match_all_idx\n",
    "print(len(df2_not_error_idx_all))\n",
    "\n",
    "df2_possible_error = df2_exclude.drop(df2_not_error_idx_all, axis = 0)\n",
    "print(df2_possible_error.shape) # (375, 21) yeah!!!!\n",
    "\n",
    "# df2_possible_error.to_csv(\"df2_possible_error_180922.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n",
      "(376, 8)\n",
      "376\n",
      "375\n",
      "(376, 22)\n"
     ]
    }
   ],
   "source": [
    "# df2_possible_error와 error_category 지닌 데이터의 idx 비교하기\n",
    "\n",
    "df2_possible_error_idx = df2_possible_error.index.tolist()\n",
    "print(len(df2_possible_error_idx)) # 375 richtig\n",
    "\n",
    "df2_total_error_with_category = pd.read_csv(\"df2_total_error_with_category_110922.csv\", index_col=0)\n",
    "print(df2_total_error_with_category.shape) # (376, 8)\n",
    "\n",
    "df2_total_error_with_category_idx = df2_total_error_with_category.index.tolist()\n",
    "print(len(df2_total_error_with_category_idx)) # 376\n",
    "\n",
    "df2_error_category = df2_total_error_with_category['category']\n",
    "# print(df2_error_category)\n",
    "\n",
    "intersection = list(set(df2_possible_error_idx) & set(df2_total_error_with_category_idx))\n",
    "print(len(intersection)) # 375 \n",
    "\n",
    "# difference 하나 찾기!\n",
    "\n",
    "difference = list(set(df2_total_error_with_category_idx) - set(df2_possible_error_idx))\n",
    "difference # [5699]\n",
    "\n",
    "# df2_possible_error에 category 추가하기 index를 중심으로\n",
    "\n",
    "df2_exclude_add_category = pd.concat([df2_possible_error, df2_error_category], axis=1)\n",
    "\n",
    "print(df2_exclude_add_category.shape) # (376, 22)\n",
    "\n",
    "# \"df2_exclude_add_category_180922.csv\" # 375개인 것 (5699 index 가진 case 삭제) manuell 만듦\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "375\n"
     ]
    }
   ],
   "source": [
    "# \"df2_exclude_add_category_180922.csv\" 불러와서 error category frequent 만들기\n",
    "\n",
    "error_total = pd.read_csv('df2_exclude_add_category_180922.csv')\n",
    "\n",
    "error_total_freq = error_total['category'].value_counts() \n",
    "print(error_total['group'].size) # 375 richtig\n",
    " \n",
    "# error_total_freq.to_csv(\"error_total_freq_180922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 22)\n",
      "(314, 22)\n",
      "322\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "# 2. df2_exclude(15883)에서 error_category 두 개로 나눔, 실제 error인것과 precision과 coverage에 다시 추가되어야 하는 것\n",
    "\n",
    "#exclude_add\n",
    "\n",
    "file_name = 'error_category_636_082022.csv'#여기를 원래 열의 수로 바꿔야 할 듯 그리고 category열을 추가 (성공! 130922)\n",
    "error_category = pd.read_csv(file_name, index_col=0) # index_col=0을 하면서 \"Unnamed: 0\" 없어짐\n",
    "# print(error_category.shape) # (636, 8)\n",
    "\n",
    "error_category_column = error_category['category']\n",
    "# print(error_category_column)\n",
    "\n",
    "error_category_index = error_category.index.tolist()\n",
    "# print(len(error_category_index)) # 636\n",
    "\n",
    "df2_exclude_add_column = df2_exclude[df2_exclude.index.isin(error_category_index) == True]\n",
    "# print(df2_exclude_add_column.shape) # (636, 21)\n",
    "\n",
    "df2_exclude_add_category = pd.concat([df2_exclude_add_column,error_category_column], axis=1)\n",
    "# print(df2_exclude_add_category.shape) # (636, 22)\n",
    "\n",
    "# dataframe에서는 object가 string임\n",
    "# error_category = error_category.applymap(str) # excel의 모든 열의 값을 string으로 바꿈\n",
    "# error_category.dtypes\n",
    "\n",
    "# 1. df2_error_add의 index 저장\n",
    "# error category가 \"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\"\n",
    "\n",
    "not_error_df = df2_exclude_add_category[df2_exclude_add_category.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == True]\n",
    "print(not_error_df.shape) # (322, 22)\n",
    "\n",
    "error_df = df2_exclude_add_category[df2_exclude_add_category.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == False]\n",
    "print(error_df.shape) # (314, 22)\n",
    "\n",
    "not_error_idx = not_error_df.index.tolist()\n",
    "print(len(not_error_idx)) # 322\n",
    "error_idx = error_df.index.tolist()\n",
    "print(len(error_idx)) #314\n",
    "\n",
    "# error_df.to_csv(\"error1_df.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15569, 21)\n",
      "(314, 21)\n",
      "314\n",
      "(314, 22)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15569, 21)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. df2_exclude(15883)에서 우선 error인 것만 제외하고 \n",
    "\n",
    "df2_exclude_error = df2_exclude[df2_exclude.index.isin(error_idx) == False]\n",
    "excluded_error = df2_exclude[df2_exclude.index.isin(error_idx) == True]\n",
    "print(df2_exclude_error.shape) # (15569, 21) \"15883-15569=314\" finally richtig \n",
    "print(excluded_error.shape) # (314, 21) \n",
    "\n",
    "excluded_error_idx = excluded_error.index.tolist()\n",
    "print(len(excluded_error_idx)) # 314\n",
    "# df2_exclude_error.to_csv(\"df2_exclude_error_030922.csv\", encoding = \"utf-8-sig\") # 여기의 index를 다시 찾아서 error_df에서 해당 error들만으로 다시 df 만들어야 함 => excluded_error_df\n",
    "\n",
    "excluded_error_df = error_df[error_df.index.isin(excluded_error_idx) == True]\n",
    "print(excluded_error_df.shape) # (314, 8)\n",
    "# excluded_error_df.to_csv(\"excluded_error_df_050922.csv\", encoding = \"utf-8-sig\") # \"excluded_error_df.csv\" 이걸로 error_analysis 하면 됨\n",
    "\n",
    "df2_exclude_error.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(322, 21)\n",
      "(15891, 22)\n",
      "(15891, 23)\n"
     ]
    }
   ],
   "source": [
    "# 3. not_error인 경우 df2_exclude_error에 새로운 열을 추가해서 original과 from_not_error 구분해주고 final 파일 만들어서 이것으로 precision 등 계산하면 됨\n",
    "\n",
    "# df2_exclude_error (15569, 21)에 not_error_df (322개) 추가 \n",
    "\n",
    "df2_exclude_not_error_df = df2_exclude[df2_exclude.index.isin(not_error_idx) == True]\n",
    "print(df2_exclude_not_error_df.shape) #(322, 21)\n",
    "\n",
    "df2_total = [df2_exclude_error, not_error_df]\n",
    "df2_not_error_added_df = pd.concat(df2_total)\n",
    "\n",
    "print(df2_not_error_added_df.shape) # (15891, 22) 15569 + 322 = 15891 richtig\n",
    "\n",
    "from_error_TF = []\n",
    "\n",
    "for idx in df2_not_error_added_df.index:\n",
    "    if idx in not_error_idx:\n",
    "        from_error_TF.append('from_error')\n",
    "    else: \n",
    "        from_error_TF.append('original') \n",
    "        \n",
    "df2_exclude_add_from_error = df2_not_error_added_df.assign(from_error_TF = from_error_TF) \n",
    "\n",
    "print(df2_exclude_add_from_error.shape) # (15891, 23)\n",
    "\n",
    "# df2_exclude_add_from_error\n",
    "\n",
    "from_error_TF_count = df2_exclude_add_from_error.groupby(['from_error_TF']).count() \n",
    "# print(from_error_TF_count) # from error: 322 richtig\n",
    "\n",
    "\n",
    "# df2_exclude_add_from_error.to_csv(\"df2_final_group1_df.csv\", encoding = \"utf-8-sig\") # \"df2_final_group1_df\" 이거 규칙별 final check 하고 three columns 만들어서 precision 계산"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11422, 23)\n"
     ]
    }
   ],
   "source": [
    "# 1. \"df2_exclude_add_from_error\" (15891, 23) 를 가지고 총 세 개의 케이스로 맞는 케이스를 분리해서 precision, recall 등을 계산하고\n",
    "# case 1: subject_by_rule과 zero_referent가 string match\n",
    "# cas3 2: singular <-> plural, 인칭만 맞는 케이스 (이건 실제로 context가 필요한 케이스이므로 맞는 케이스로 넣음)\n",
    "# case 3: subject_by_rule과 antecedent가 같은 경우 (antecedent에 다양한 케이스가 존재할 수 있음)\n",
    "\n",
    "# 나중에 규칙이 적용된 전체 데이터셋 크기는 error까지 다 합친 것으로 해야함!!!!\n",
    "# 1. simple string match (case1)\n",
    "\n",
    "df2_final_group1_case1 = df2_exclude_add_from_error[df2_exclude_add_from_error.subject_by_rule == df2_exclude_add_from_error.zero_referents] \n",
    "print(df2_final_group1_case1.shape) # (11422, 23)\n",
    "\n",
    "# df2_final_group1_case1.to_csv(\"df2_final_group1_case1.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.  singular <-> plural, person match (case2) * case1을 제외하고 찾기\n",
    "### case2A_B_C_02052022_xlsx에서 sheet0(\"singplural\")에 있는 idx\n",
    "\n",
    "df2_final_group1_not_case1 = df2_exclude_add_from_error[df2_exclude_add_from_error.subject_by_rule != df2_exclude_add_from_error.zero_referents] \n",
    "# print(df2_final_group1_not_case1.shape) # (4469, 23) 15891 - 11422 = 4469 richtig\n",
    "\n",
    "df2_final_group1_not_case1_idx = df2_final_group1_not_case1.index.tolist()\n",
    "# print(len(df2_final_group1_not_case1_idx)) # 4469\n",
    "\n",
    "df2_final_group1_case2_first_person = df2_final_group1_not_case1[df2_final_group1_not_case1['subject_by_rule'].str.contains('1')] \n",
    "df2_final_group1_case2_second_person = df2_final_group1_not_case1[df2_final_group1_not_case1['subject_by_rule'].str.contains('2')] \n",
    "\n",
    "# df2_final_group1_case2_first_person.to_csv(\"df2_final_group1_case2_first_person.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_final_group1_case2_second_person.to_csv(\"df2_final_group1_case2_second_person.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# print(df2_final_group1_case2_first_person.shape) #(659, 23) richtig\n",
    "# print(df2_final_group1_case2_second_person.shape) #(93, 23) richtig\n",
    "\n",
    "df2_final_group1_case2_first_person_match = df2_final_group1_case2_first_person[(df2_final_group1_case2_first_person['zero_referents'] == '1ph-') | (df2_final_group1_case2_first_person['zero_referents'] == '1ph+')] \n",
    "df2_final_group1_case2_second_person_match = df2_final_group1_case2_second_person[(df2_final_group1_case2_second_person['zero_referents'] == '2ph+') | (df2_final_group1_case2_second_person['zero_referents'] == '2sh-')]\n",
    "\n",
    "# print(df2_final_group1_case2_first_person_match.shape) # (381, 23) richtig\n",
    "# print(df2_final_group1_case2_second_person_match.shape) # (67, 23) richtig\n",
    "\n",
    "df2_final_group1_case2_first_person_match_idx = df2_final_group1_case2_first_person_match.index.tolist()\n",
    "df2_final_group1_case2_second_person_match_idx = df2_final_group1_case2_second_person_match.index.tolist()\n",
    "\n",
    "# print(len(df2_final_group1_case2_first_person_match_idx)) # 381\n",
    "# print(len(df2_final_group1_case2_second_person_match_idx)) # 67\n",
    "\n",
    "# df2_final_group1_case2_first_person_match.to_csv(\"df2_final_group1_case2_first_person_match.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_final_group1_case2_second_person_match.to_csv(\"df2_final_group1_case2_second_person_match.csv\", encoding = \"utf-8-sig\") \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. subject_by_rule과 antecedent 같은 경우 (case3) \n",
    "# df2_final_group1_not_case1에서 df2_final_group1_case2_first_person_match와 df2_final_group1_case2_second_person_match를 제외한 것에서 시작\n",
    "\n",
    "df2_final_group1_case2_first_second_person_match_idx = df2_final_group1_case2_first_person_match_idx + df2_final_group1_case2_second_person_match_idx\n",
    "# print(len(df2_final_group1_case2_first_second_person_match_idx)) # 448 (381+67 = 448 richtig)\n",
    "\n",
    "df2_final_group1_case3_check = df2_final_group1_not_case1[df2_final_group1_not_case1.index.isin(df2_final_group1_case2_first_second_person_match_idx) == False]\n",
    "\n",
    "# print(df2_final_group1_case3_check.shape) # (4021, 23) 4469-448 = 4021 richtig\n",
    "# df2_final_group1_case3_check.to_csv(\"df2_final_group1_case3_check.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "# df2_final_group1_case3_check에서 3개의 subcases # total 233:\n",
    "# 3.1 subject_by_rule \"1sh+\" & antecedent \"{저}\", \"{저는}\", \"{저도}\", \"{저희가}\", \"{저희들도}\", \"{전}\", \"{제가}\" (check the dtypes if string match not works) # 232\n",
    "# 3.2 subject_by_rule \"1ph-\" & antecedent \"{우리}\" # 1\n",
    "# 3.3 subject_by_rule \"2sh+\" # no match\n",
    "\n",
    "# right_antecedent인 case의 idx를 하나의 csv파일로 만듦 (이걸 다시 만들어야 함 110922)\n",
    " \n",
    "antecedent_correct = pd.read_excel('right_antecedent_idx_050922.xlsx') \n",
    "antecedent_correct_idx = antecedent_correct['idx']\n",
    "antecedent_correct_idx_val = antecedent_correct_idx.values \n",
    "antecedent_correct_idx_val_list = antecedent_correct_idx_val.tolist() # 233 richtig\n",
    "\n",
    "# print(len(antecedent_correct_idx_val_list))\n",
    "\n",
    "df2_final_group1_case3 = df2_final_group1_case3_check[df2_final_group1_case3_check.index.isin(antecedent_correct_idx_val_list) == True]\n",
    "df2_final_group1_error = df2_final_group1_case3_check[df2_final_group1_case3_check.index.isin(antecedent_correct_idx_val_list) == False]\n",
    "\n",
    "# print(df2_final_group1_case3.shape) # (233, 23)\n",
    "# print(df2_final_group1_error.shape) # (3788, 23) 4021-233 = 3788 richtig\n",
    "\n",
    "# df2_final_group1_case3.to_csv(\"df2_final_group1_case3.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_final_group1_error.to_csv(\"df2_final_group1_error.csv\", encoding = \"utf-8-sig\") # 이거랑, 위의 error 합쳐야함, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df2_final_group1_error에서 N/A랑 none 제외시켜야 함\n",
    "\n",
    "# 1. df2_final_group1_error(3788 cases)에서 subject_by_rule이 N/A인 case 분리 => group2 규칙 적용 대상 (총 2049 cases) \n",
    "df2_final_group1_to_group2 = df2_final_group1_error[df2_final_group1_error.subject_by_rule == \"N/A\"]\n",
    "# print(df2_final_group1_to_group2.shape) # (2049, 23) richtig (전체 df2_final_group1_df에서도 같은 숫자)\n",
    "\n",
    "# 2. df2_final_group1_error 에서 subject_by_rule이 none인 case 분리 => 규칙 적용이지만 실제 대화 참여자랑 관계없는 case, 이것에 대한 분석도 필요함\n",
    "df2_final_group1_non_discoure_participate_rule = df2_final_group1_error[df2_final_group1_error.subject_by_rule == \"none\"]\n",
    "# print(df2_final_group1_non_discoure_participate_rule.shape) # (1668, 23) richtig (전체 df2_final_group1_df에서도 같은 숫자)\n",
    "\n",
    "error_check_idx1 = df2_final_group1_to_group2.index.tolist()\n",
    "error_check_idx2 = df2_final_group1_non_discoure_participate_rule.index.tolist()\n",
    "\n",
    "# print(len(error_check_idx1)) # 2049\n",
    "# print(len(error_check_idx2)) # 1668\n",
    "\n",
    "error_check_idx_total = error_check_idx1 + error_check_idx2\n",
    "# print(len(error_check_idx_total)) # 3717 (2049+1668 = 3717 richtig)\n",
    "\n",
    "# 3. 실제 error일 확률이 높은 case 찾아서 error catetory 추가하고, exluded_error랑 합쳐서, 이 데이터에 대해서 error analysis하면 됨\n",
    "df2_final_group1_error_check = df2_final_group1_error[df2_final_group1_error.index.isin(error_check_idx_total) == False]\n",
    "# print(df2_final_group1_error_check.shape) # (71, 23) richtig 3788 -3717 = 71 richtig\n",
    "\n",
    "# df2_final_group1_error_check.to_csv(\"df2_final_group1_error_check_110922.csv\", encoding = \"utf-8-sig\") # 110922 다시 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 23)\n",
      "(371, 23)\n"
     ]
    }
   ],
   "source": [
    "# df2_final_group1_error_check에서 다시 df2_exclude_add_from_error에 추가돼야 하는 case 찾기\n",
    "\n",
    "not_error_df2 = df2_final_group1_error_check[df2_final_group1_error_check.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == True]\n",
    "# print(not_error_df2.shape) # (9, 23)\n",
    "\n",
    "error_df2 = df2_final_group1_error_check[df2_final_group1_error_check.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == False]\n",
    "# print(error_df2.shape) # (62, 23) 71 -9 = 62 richtig\n",
    "\n",
    "# # 1. not_error_df2를 제외한 error_df2 (62, 23) 완성 (이전의 error_df (314, 8)과 합쳐서 error_analysis하기 )\n",
    "\n",
    "# error_df2.to_csv(\"error_df2_110922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "df2_total_error_list = [error_df2, error_df]\n",
    "df2_total_error = pd.concat(df2_total_error_list)\n",
    "# print(df2_total_error.shape) # (376, 23) 62 + 314 = 376 richtig\n",
    "\n",
    "# df2_total_error.to_csv(\"df2_total_error_110922.csv\", encoding = \"utf-8-sig\") \n",
    "# 이걸 다시 카테고리가 추가된 csv 파일로 manuell하게 만들기 \"df2_total_error_with_category_110922.csv\"\n",
    "# 여기에 또 not error 포함되어 있음 (5 cases) \"df2_final_group1_df.csv\"에는 이미 포함되어 있는지 확인해 보기 # index 13은 없는 것 같음\n",
    "\n",
    "df2_total_error_with_category = pd.read_csv(\"df2_total_error_with_category_110922.csv\", index_col=0)\n",
    "# print(df2_total_error_with_category.shape) # (376, 8)\n",
    "# 여기에서 category column만 가져와서 df2_total_error에 추가해서 df2_total_error_add_category 새로 만들기\n",
    "\n",
    "df2_total_error_category = df2_total_error_with_category['category']\n",
    "# print(df2_total_error_category.shape) (376,)\n",
    "\n",
    "# df2_total_error.to_csv(\"df2_total_error_130922.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_total_error_category.to_csv(\"df2_total_error_category_130922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "# 1. df2_total_error에서 category 열을 제외 -> df2_total_error_drop_category\n",
    "# 2. df2_total_error_drop_category와 df2_total_error_category를 axis=1로 concat하기\n",
    "\n",
    "df2_total_error_drop_category = df2_total_error.drop(['category'], axis=1)\n",
    "# print(df2_total_error_drop_category.shape) # (376, 22)\n",
    "\n",
    "df2_total_error_add_category = pd.concat([df2_total_error_drop_category, df2_total_error_category], axis=1)\n",
    "# print(df2_total_error_add_category.shape) (376, 23)\n",
    "\n",
    "# df2_total_error_add_category.to_csv(\"df2_total_error_add_category_130922.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "not_error_df3 = df2_total_error_add_category[df2_total_error_add_category.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == True]\n",
    "print(not_error_df3.shape) # (5, 23)\n",
    "# print(not_error_df3.index) # [9, 13, 16, 17, 47]\n",
    "\n",
    "error_df3 = df2_total_error_add_category[df2_total_error_add_category.category.isin([\"check for antecedent (right case)\", \"not error (ambiguity between sing and plur)\", \"not error (ambiguity between sugg and impe)\", \"check for antecedent (right case)\", \"not error (wrong zero referent)\"]) == False]\n",
    "print(error_df3.shape) # (371, 23) 376-5 = 371 richtig\n",
    "\n",
    "# error_df3.to_csv(\"df2_total_error_final_110922.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# error_total_group1_df를 category count해보기 \n",
    "error_category_freq = error_df3.groupby(['category']).count() \n",
    "\n",
    "# error_category_freq\n",
    "\n",
    "# \"Fehlerkategorie_110922.xlsx\" in \"Schreiben_neu\" folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15891\n",
      "9\n",
      "5\n",
      "14\n",
      "[]\n",
      "(15891, 23)\n"
     ]
    }
   ],
   "source": [
    "# 2. df2_exclude_add_from_error (15891, 23)에는 이미 not_error_df2의 index를 가진 문장들이 포함되어 있음\n",
    "\n",
    "# 확실하게 하기 위해서 not_error_df2와 not_error_df3을 추가해보기 (intersection도 활용해도 좋음)\n",
    "# \"df2_final_group1_df.csv\"를 precision, recall 구하는 것에 사용하면 됨\n",
    "\n",
    "df2_exclude_add_from_error_idx = df2_exclude_add_from_error.index.tolist()\n",
    "print(len(df2_exclude_add_from_error_idx)) # 15891\n",
    "\n",
    "not_error_df2_idx = not_error_df2.index.tolist()\n",
    "print(len(not_error_df2_idx)) # 9\n",
    "\n",
    "not_error_df3_idx = not_error_df3.index.tolist()\n",
    "print(len(not_error_df3_idx)) # 5\n",
    "\n",
    "not_error_df2_df3_idx = not_error_df2_idx + not_error_df3_idx\n",
    "print(len(not_error_df2_df3_idx)) # 14\n",
    "\n",
    "D = list(set(not_error_df2_df3_idx) - set(df2_exclude_add_from_error_idx))\n",
    "print(D) # []\n",
    "\n",
    "# print(df2_exclude_add_from_error.shape) # (15891, 23)\n",
    "\n",
    "df2_exclude_add_from_error = df2_total_group1_for_analysis\n",
    "\n",
    "df2_total_group1_for_analysis.to_csv(\"df2_total_group1_for_analysis_test1.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "print(df2_total_group1_for_analysis.shape) # (15891, 23)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2049\n",
      "1668\n"
     ]
    }
   ],
   "source": [
    "# df2_total_group1_for_analysis (15891, 23) precision과 recall 계산하기\n",
    "\n",
    "# df2_total_group1_for_analysis 에서 우선 N/A랑 none 따로 dataframe으로 만들어 놓기\n",
    "\n",
    "# 1. df2_total_group1_for_analysis에서(15892 cases)에서 subject_by_rule이 N/A인 case 분리 => group2 규칙 적용 대상 (총 2049 cases) \n",
    "df2_group1_to_group2 = df2_total_group1_for_analysis[df2_total_group1_for_analysis.subject_by_rule == \"N/A\"]\n",
    "df2_NA_idx = df2_group1_to_group2.index.tolist()\n",
    "print(len(df2_NA_idx)) # 2049\n",
    "\n",
    "# print(df2_group1_to_group2.shape) # (2049, 23) richtig \n",
    "\n",
    "# 2. df2_total_group1_for_analysis에서 subject_by_rule이 none인 case 분리 => 규칙 적용이지만 실제 대화 참여자랑 관계없는 case,이것에 대한 분석도 필요함\n",
    "df2_non_discourse_participant = df2_total_group1_for_analysis[df2_total_group1_for_analysis.subject_by_rule == \"none\"]\n",
    "df2_non_discourse_participant_idx = df2_non_discourse_participant.index.tolist()\n",
    "print(len(df2_non_discourse_participant_idx)) # 1668\n",
    "\n",
    "# print(df2_non_discourse_participate.shape) # (1668, 23) richtig \n",
    "\n",
    "# df2_group1_to_group2.to_csv(\"df2_group1_to_group2_110922.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_non_discourse_participant.to_csv(\"df2_non_discourse_participate_110922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11422\n",
      "(11422, 23)\n"
     ]
    }
   ],
   "source": [
    "# 나중에 규칙이 적용된 전체 데이터셋 크기는 error까지 다 합친 것으로 해야함!!!!\n",
    "# 1. simple string match (case1)\n",
    "\n",
    "df2_case1 = df2_total_group1_for_analysis[df2_total_group1_for_analysis.subject_by_rule == df2_total_group1_for_analysis.zero_referents] \n",
    "df2_case1_idx = df2_case1.index.tolist()\n",
    "print(len(df2_case1_idx)) # 11422\n",
    "\n",
    "print(df2_case1.shape) # (11422, 23)\n",
    "\n",
    "# df2_case1.to_csv(\"df2_case1_110922.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4469, 23)\n",
      "(659, 23)\n",
      "(93, 23)\n",
      "(381, 23)\n",
      "(67, 23)\n"
     ]
    }
   ],
   "source": [
    "# 2.  singular <-> plural, person match (case2) * case1을 제외하고 찾기\n",
    "\n",
    "df2_not_case1 = df2_total_group1_for_analysis[df2_total_group1_for_analysis.subject_by_rule != df2_total_group1_for_analysis.zero_referents] \n",
    "print(df2_not_case1.shape) # (4470, 23) 15892 - 11422 = 4470 richtig\n",
    "\n",
    "df2_not_case1_idx = df2_not_case1.index.tolist()\n",
    "# print(len(df2_not_case1_idx)) # 4470\n",
    "\n",
    "df2_first_person = df2_not_case1[df2_not_case1['subject_by_rule'].str.contains('1')] \n",
    "df2_second_person = df2_not_case1[df2_not_case1['subject_by_rule'].str.contains('2')] \n",
    "\n",
    "print(df2_first_person.shape) #(659, 23) \n",
    "print(df2_second_person.shape) #(94, 23) \n",
    "\n",
    "df2_first_person_match = df2_first_person[(df2_first_person['zero_referents'] == '1ph-') | (df2_first_person['zero_referents'] == '1ph+')] \n",
    "df2_second_person_match = df2_second_person[(df2_second_person['zero_referents'] == '2ph+') | (df2_second_person['zero_referents'] == '2sh-')]\n",
    "\n",
    "print(df2_first_person_match.shape) # (381, 23) \n",
    "print(df2_second_person_match.shape) # (67, 23) \n",
    "\n",
    "df2_first_person_match_idx = df2_first_person_match.index.tolist()\n",
    "df2_second_person_match_idx = df2_second_person_match.index.tolist()\n",
    "\n",
    "# print(len(df2_first_person_match_idx)) # 381\n",
    "# print(len(df2_second_person_match_idx)) # 67\n",
    "\n",
    "# df2_first_person_match.to_csv(\"df2_first_person_match.csv\", encoding = \"utf-8-sig\") \n",
    "# df2_second_person_match.to_csv(\"df2_second_person_match.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(233, 23)\n"
     ]
    }
   ],
   "source": [
    "# 3. subject_by_rule과 antecedent 같은 경우 (case3) \n",
    "# df2_not_case1 에서 df2_first_person_match 와 df2_second_person_match 를 제외한 것에서 시작\n",
    "\n",
    "df2_first_second_person_match_idx = df2_first_person_match_idx + df2_second_person_match_idx\n",
    "# print(len(df2_first_second_person_match_idx)) # 448 (381+67 = 448 richtig)\n",
    "\n",
    "df2_case3_check = df2_not_case1[df2_not_case1.index.isin(df2_first_second_person_match_idx) == False]\n",
    "\n",
    "# print(df2_case3_check.shape) # (4022, 23) 4470-448 = 4022 richtig\n",
    "# df2_case3_check.to_csv(\"df2_case3_check.csv\", encoding = \"utf-8-sig\") \n",
    "\n",
    "\n",
    "# df2_case3_check 에서 3개의 subcases # total 233:\n",
    "# 3.1 subject_by_rule \"1sh+\" & antecedent \"{저}\", \"{저는}\", \"{저도}\", \"{저희가}\", \"{저희들도}\", \"{전}\", \"{제가}\" (check the dtypes if string match not works) # 232\n",
    "# 3.2 subject_by_rule \"1ph-\" & antecedent \"{우리}\" # 1\n",
    "# 3.3 subject_by_rule \"2sh+\" # no match \n",
    "\n",
    "# right_antecedent인 case의 idx를 하나의 csv파일로 만듦 (위의 코드에도 존재함)\n",
    "\n",
    "df2_case3 = df2_case3_check[df2_case3_check.index.isin(antecedent_correct_idx_val_list) == True]\n",
    "df2_case3_idx = df2_case3.index.tolist()\n",
    "# print(len(df2_case3_idx)) # 233\n",
    "\n",
    "# df2_error_final_check = df2_case3_check[df2_case3_check.index.isin(antecedent_correct_idx_val_list) == False]\n",
    "\n",
    "print(df2_case3.shape) # (233, 23)\n",
    "# print(df2_error_final_check.shape) # (3789, 23) 4022-233 = 3789 richtig\n",
    "\n",
    "# df2_case3.to_csv(\"df2_case3.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15891\n",
      "15820\n",
      "62\n",
      "(71, 23)\n"
     ]
    }
   ],
   "source": [
    "# df2_total_group1_for_analysis 에다가 case_class 열 추가하기!\n",
    "# 1.\tto_group2\n",
    "# 2.\tnot_discourse_participant\n",
    "# 3.\tcase1\n",
    "# 4.\tcase2\n",
    "# 5.\tcase3\n",
    "# 6.\tN/A (전체는 15892인데 72개가 무엇일까?) # 또 다른 방법은 1-5까지의 모든 인덱스를 합치고 다시 차집합 확인하기 \n",
    "\n",
    "df2_total_group1_for_analysis_idx = df2_total_group1_for_analysis.index.tolist()\n",
    "print(len(df2_total_group1_for_analysis_idx)) # 15892\n",
    "\n",
    "index_total_idx_list = df2_NA_idx+ df2_non_discourse_participant_idx+ df2_case1_idx + df2_first_second_person_match_idx + df2_case3_idx\n",
    "print(len(index_total_idx_list)) # 15820\n",
    "\n",
    "Difference = list(set(df2_total_group1_for_analysis_idx) - set(index_total_idx_list))\n",
    "print(len(Difference)) # 62\n",
    "\n",
    "df2_difference = df2_total_group1_for_analysis[df2_total_group1_for_analysis.index.isin(Difference) == True]\n",
    "print(df2_difference.shape) # (71, 23)\n",
    "\n",
    "# 이 71개가 지금 뭔지 모르겠고 그리고 왜 difference 62와 숫자 차이가 있는지 모르겠지만, 우선 case 1-3까지 rule 카테고리 등 분석, 우선 표로 만들기\n",
    "\n",
    "# df2_difference.to_csv(\"df2_difference.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11422\n"
     ]
    }
   ],
   "source": [
    "rule_freq_case1 = df2_case1['group'].value_counts() \n",
    "print(df2_case1['group'].size) # 11422 richtig\n",
    " \n",
    "# rule_freq_case1.to_csv(\"rule_freq_case1.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "381\n",
      "67\n",
      "233\n"
     ]
    }
   ],
   "source": [
    "rule_freq_first_person_match = df2_first_person_match['group'].value_counts() \n",
    "print(df2_first_person_match['group'].size) # 381 richtig\n",
    "\n",
    "rule_freq_second_person_match = df2_second_person_match['group'].value_counts() \n",
    "print(df2_second_person_match['group'].size) # 67 richtig\n",
    "\n",
    "rule_freq_case3 = df2_case3['group'].value_counts() \n",
    "print(df2_case3['group'].size) # 233 richtig\n",
    "\n",
    "\n",
    "# rule_freq_first_person_match.to_csv(\"rule_freq_first_person_match.csv\", encoding = \"utf-8-sig\") \n",
    "# rule_freq_second_person_match.to_csv(\"rule_freq_second_person_match.csv\", encoding = \"utf-8-sig\") \n",
    "# rule_freq_case3.to_csv(\"rule_freq_case3.csv\", encoding = \"utf-8-sig\") \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "want verbal suffix                            34\n",
      "cognition predicate                           12\n",
      "clau_neunde                                   11\n",
      "want periphrastic construction                11\n",
      "possibility periphrastic construction/plus    10\n",
      "suggestory verbal suffix                       9\n",
      "permission periphrastic construction           8\n",
      "obligation periphrastic construction           8\n",
      "progress aspect                                5\n",
      "sensation predicate                            5\n",
      "feeling predicate                              4\n",
      "guess periphrastic construction                4\n",
      "politeness auxiliary verb                      3\n",
      "politeness predicate                           3\n",
      "impossibility periphrastic construction        3\n",
      "aspect auxiliary verb                          1\n",
      "clau_cognition                                 1\n",
      "passive aspect                                 1\n",
      "sugg                                           1\n",
      "Name: rule, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rule_freq_case3 = df2_case3['rule'].value_counts() \n",
    "# print(rule_freq_case3)\n",
    "\n",
    "# rule_freq_case3.to_csv(\"rule_freq_case3.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progress aspect                               30\n",
      "hono total                                    14\n",
      "want periphrastic construction                 7\n",
      "want verbal suffix                             2\n",
      "politeness auxiliary verb                      2\n",
      "suggestory verbal suffix                       2\n",
      "possibility periphrastic construction/plus     1\n",
      "obligation periphrastic construction           1\n",
      "clau_neunde                                    1\n",
      "sugg                                           1\n",
      "performative predicate                         1\n",
      "Name: rule, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "rule_freq_difference = df2_difference['rule'].value_counts() \n",
    "# print(rule_freq_difference)\n",
    "\n",
    "# rule_freq_difference.to_csv(\"rule_freq_difference.csv\", encoding = \"utf-8-sig\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "impe was applied,                          2434\n",
      "                                           2049\n",
      "2.19.2 exclude_predicate1 was applied,     1495\n",
      "2.2 want was applied,                      1000\n",
      "3.1.2 suggestory was applied,               581\n",
      "                                           ... \n",
      "8.3 want was applied,                         1\n",
      "2.20 want was applied,                        1\n",
      "7.3 judgement was applied,                    1\n",
      "clau(euni_variant1) was applied,              1\n",
      "0.3 clau(cognition3) was applied,             1\n",
      "Name: applied_rule, Length: 173, dtype: int64\n",
      "(15247, 22)\n"
     ]
    }
   ],
   "source": [
    "rule_freq = df2_exclude_error['applied_rule'].value_counts() \n",
    "#rule_order = df2_exclude_error['rule_order'].value_counts() \n",
    "rule_unique = df2_exclude_error['applied_rule'].unique()\n",
    "#print(rule_unique)\n",
    "print(rule_freq)\n",
    "# rule_freq.to_excel('rule1_freq_with_order.xlsx', encoding = \"utf-8-sig\")\n",
    "# 이미 기존 \"df2_exclude_error\"에 rule_order는 추가되어 있는 상태이므로 이 데이터셋에 rule_freq를 추가하는 것이 하나의 방법일 듯\n",
    "\n",
    "rule_freq_dict = rule_freq.to_dict()\n",
    "\n",
    "#df2_exclude_error_rule_freq = df2_exclude_error.assign(rule_count = rule_freq) \n",
    "# df2_exclude_error_rule_freq.shape # (15247, 23)\n",
    "#df2_exclude_error_rule_freq.to_excel('df2_exclude_error_rule_freq.xlsx', encoding = \"utf-8-sig\")\n",
    "\n",
    "rule_freq_dict_keys = list(rule_freq_dict.keys()) #8개 / J_root는 총 23개 - list로 무사히 바꿈 (dict_kyes의 속성을 지니지 않음)\n",
    "# print(len(rule_freq_dict_keys)) #173\n",
    "\n",
    "rule_count = []\n",
    "\n",
    "for s in df2_exclude_error['applied_rule']:\n",
    "    if s in rule_freq_dict_keys:\n",
    "        rule_count.append(rule_freq_dict[s])\n",
    "    else: \n",
    "        rule_count.append('NAN') \n",
    "        \n",
    "df2_exclude_error_rule_freq = df2_exclude_error.assign(rule_count = rule_count)\n",
    "print(df2_exclude_error_rule_freq.shape) # (15247, 22)\n",
    "\n",
    "#df2_exclude_error_rule_freq.to_excel('df2_exclude_error_rule_freq.xlsx', encoding = \"utf-8-sig\")\n",
    "#df2_exclude_error.shape\n",
    "\n",
    "\n",
    "df2_three_columns = df2_exclude_error_rule_freq[['rule_order','applied_rule','rule_count']]\n",
    "#df2_three_columns.to_excel('df2_three_columns.xlsx', encoding = \"utf-8-sig\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
